Abstract,Author Keywords
"As a typical application of machine learning in the mobile field of security and privacy protection, the main characteristics of arts and crafts materials are to advocate the practicality and functionality of design, emphasizing the unity of practicality and aesthetics, and practicality is the first. The so-called practicality refers to whether the designed product meets the specific functional requirements or aesthetic requirements, and whether it is convenient, comfortable and safe to use. Based on computer vision, this paper studies the material and aesthetic characteristics of arts and crafts works, and the research shows that this method has achieved the best performance. More specifically, the maximum expected value of this method is 0.69, which is much higher than the corresponding values of 0.54 and 0.50 in references [1] and [2] respectively. The experimental results show that our method is specially designed for the distributed learning of process aesthetics, and it is still a very effective tool for the aesthetic classification task of process aesthetic feature images. Based on computer vision, this paper studies the creative style and aesthetic characteristics of arts and crafts materials, and makes a comprehensive investigation and beneficial exploration on their development, aesthetic characteristics and creative style.",Computer vision; industrial art; work materials; process aesthetic characteristics; machine learning for secure; privacy-preserving mobile
"Anomaly analysis is an important component of any surveillance system. In recent years, it has drawn the attention of the computer vision and machine learning communities. In this article, our overarching goal is thus to provide a coherent and systematic review of state-of-the-art techniques and a comprehensive review of the research works in anomaly analysis. We will provide a broad vision of computational models, datasets, metrics, extensive experiments, and what anomaly analysis can do in images and videos. Intensively covering nearly 200 publications, we review (i) anomaly related surveys, (ii) taxonomy for anomaly problems, (iii) the computational models, (iv) the benchmark datasets for studying abnormalities in images and videos, and (v) the performance of state-of-the-art methods in this research problem. In addition, we provide insightful discussions and pave the way for future work.",Anomalies; anomaly analysis; anomaly detection; anomaly prediction; deep learning
"Structural damage detection techniques are gaining widespread attention in construction engineering and management. However, the scarcity of structural damage samples and the cross-task transferability of existing knowledge currently limit this technique in practical applications. Therefore, this paper proposes a novel framework for structural damage detection with large scope of cross-task learning capability that incorporates Bayesian estimation and variational inference into the deep learning backbones and Bayesian weight function into the outer loop process of metalearning. Experimental results demonstrate the superiority of this method for both structural damage image classification and structural damage semantic segmentation. Compared with existing frameworks, the proposed method can alleviate the negative influence of domain bias and reduce computation time and costs due to sample labeling. This paper also discusses how the proposed framework can be used to train a model of the structural damage detection framework in extreme cases. The framework and findings presented in this paper have important theoretical and practical contributions to the literature on vision-based structural damage detection.",Probabilistic neural networks; Metalearning; Structural damage detection; Intelligent construction
"Single target tracking based on computer vision helps to collect, analyse and exploit target information. The SwinTrack algorithm has received widespread attention as one of the twin network algorithms with the best trade-off between tracking accuracy and speed, but it also suffers from the insufficient fusion of deep and shallow features leading to loss of shallow information and insufficient use of temporal information leading to inconsistency between target and template. Semantic information and detailed information are combined and multiple convolutional forms are introduced to propose a multi-level feature fusion strategy to effectively fuse features in space. Besides, based on the idea of feedback, a dynamic template branching approach is also designed to fuse temporal features and enhance the representation of target features. The effectiveness of this method was verified on the OTB100 and GOT10K datasets.",computer vision; feature extraction; image processing; object tracking
"Aiming at obtaining structural information and 3D motion of dynamic scenes, scene flow estimation has been an interest of research in computer vision and computer graphics for a long time. It is also a fundamental task for various applications such as autonomous driving. Compared to previous methods that utilize image representations, many recent researches build upon the power of deep analysis and focus on point clouds representation to conduct 3D flow estimation. This paper comprehensively reviews the pioneering literature in scene flow estimation based on point clouds. Meanwhile, it delves into detail in learning paradigms and presents insightful comparisons between the state-of-the-art methods using deep learning for scene flow estimation. Furthermore, this paper investigates various higher-level scene understanding tasks, including object tracking, motion segmentation, etc. and concludes with an overview of foreseeable research trends for scene flow estimation.",3D scene flow; literature survey
"Automatic translation from signed to spoken languages is an interdisciplinary research domain on the intersection of computer vision, machine translation (MT), and linguistics. While the domain is growing in terms of popularity-the majority of scientific papers on sign language (SL) translation have been published in the past five years-research in this domain is performed mostly by computer scientists in isolation. This article presents an extensive and cross-domain overview of the work on SL translation. We first give a high level introduction to SL linguistics and MT to illustrate the requirements of automatic SL translation. Then, we present a systematic literature review of the state of the art in the domain. Finally, we outline important challenges for future research. We find that significant advances have been made on the shoulders of spoken language MT research. However, current approaches often lack linguistic motivation or are not adapted to the different characteristics of SLs. We explore challenges related to the representation of SL data, the collection of datasets and the evaluation of SL translation models. We advocate for interdisciplinary research and for grounding future research in linguistic analysis of SLs. Furthermore, the inclusion of deaf and hearing end users of SL translation applications in use case identification, data collection, and evaluation, is of utmost importance in the creation of useful SL translation models.",Sign language; Computer vision; Machine translation; Deep learning; Literature review
"In recent years, object detection in computer vision has developed rapidly. However, crowded pedestrian detection in object detection remains a challenging problem, especially in one-stage detectors where improved solutions are rare. In this paper, we propose a novel crowded pedestrian detection method called YOLO-CPD which works better than other one-stage models in crowded environments. Our method primarily enhances the ability of the one-stage detector to detect multiple overlapping objects in a single area. The core of our approach is to use boxes difference to adjust the IoU value of the Non-Maximum Suppression (NMS) and to improve the Intersection over Union regression loss (IoU Loss), with an Optimised Score Module (OPSC). Compared to the baseline, YOLO-CPD can improve the Average Precision (AP) by a 5.04% increase, Recall by a 2.17% increase and the log-average Miss Rate (MR-2) by a 5.12% reduction on the CrowdHuman dataset. In addition, YOLO-CPD also achieved good results in the WiderPerson dataset, demonstrating the strong generalisation capability of our proposed method.",YOLOX; Crowd pedestrian detection; Object detection; Computer vision
"Repair technologies have been considered as sustainable approaches due to their capability to restore value in a damaged component and bring it to like-new condition. However, in contrast to a manufacturing process benefiting from an automated environment, the automation level for repair and remanufacturing processes remains low. With the aim of moving the repair industry towards autonomy, this study proposes a novel repair framework. The developed methodology presents a vision-based Robotic Laser Cladding Repair Cell (RLCRC) that has two features: (a) an intelligent inspection system that uses a deep learning model to automatically detect the damaged region in an image; (b) employing computer vision-based calibration and 3D scanning techniques to precisely identify the geometries of damaged area. The repair of fixed bends is selected as the case study. The results obtained validate the efficacy of the proposed framework, enabling automatic damage detection and damaged volume extraction for worn fixed bends. Following the suggested framework, a time reduction of more than 63% is reported.",Repair systems; Computer vision; Autonomous manufacturing; Deep learning
"With the good performance of deep learning in the field of computer vision (CV), the convolutional neural network (CNN) architectures have become main backbones of image recognition tasks. With the widespread use of mobile devices, neural network models based on platforms with low computing power are gradually being paid attention. However, due to the limitation of computing power, deep learning algorithms are usually not available on mobile devices. This paper proposes a lightweight convolutional neural network TripleNet, which can operate easily on Raspberry Pi. Adopted from the concept of block connections in ThreshNet, the newly proposed network model compresses and accelerates the network model, reduces the amount of parameters of the network, and shortens the inference time of each image while ensuring the accuracy. Our proposed TripleNet and other State-of-the-Art (SOTA) neural networks perform image classification experiments with the CIFAR-10 and SVHN datasets on Raspberry Pi. The experimental results show that, compared with GhostNet, MobileNet, ThreshNet, EfficientNet, and HarDNet, the inference time of TripleNet per image is shortened by 15%, 16%, 17%, 24%, and 30%, respectively.",Edge computing platform; Image classification; Convolutional neural network; Model acceleration; Model compression; Raspberry Pi
"Quality control inspection has become one key task in the industrial manufacturing field. Due to technological advances and the fast development of Industry 4.0 concepts, factories are demanding high precision and accuracy of autonomous systems for inspections of industrial parts. Vision-based measurement (VBM) implements image-based evaluations by using cameras as main sensors and customized software to perform measurements. On the other hand, the new paradigm of production brought by Industry 4.0 allows integration at all levels at the physical manufacturing plant, ranging from the shop floor to the office, resulting in better operational efficiency. This article introduces in detail an architecture that enables the integration of VBM systems for the accomplishment of autonomous quality control inspections. It discusses aspects related to computer vision techniques, from the setup and calibration procedures to the main aspects related to the customized algorithms for measurement based on image processing in 2D and 3D. An explanation for estimating the uncertainty associated with measurements taken by the VBM system according to the standard specifications is provided, as well as a well-organized overview of current research efforts and design challenges in developing VBM systems for autonomous inspection, is presented. Finally, it establishes a manner to integrate VBM systems into the heterogeneous manufacturing systems encountered in factories around the world.",Vision-based measurement (VBM); Industry 4; 0; Quality control inspection; Algorithms for measurement; Uncertainty analysis
"While road obstacle detection techniques have become increasingly effective, they typically ignore the fact that, in practice, the apparent size of the obstacles decreases as their distance to the vehicle increases. In this letter, we account for this by computing a scale map encoding the apparent size of a hypothetical object at every image location. We then leverage this perspective map to (i) generate training data by injecting onto the road synthetic objects whose size corresponds to the perspective foreshortening; and (ii) incorporate perspective information in the decoding part of the detection network to guide the obstacle detector. Our results on standard benchmarks show that, together, these two strategies significantly boost the obstacle detection performance, allowing our approach to consistently outperform state-of-the-art methods in terms of instance-level obstacle detection.",Roads; Cameras; Training data; Feature extraction; Training; Optical imaging; Computer architecture; Computer vision for transportation; data sets for robotic vision; deep learning for visual perception; object detection; segmentation and categorization
"The transportation system undergoes severe impacts due to potholes and the presence of stray animals on the roads resulting in accidents and fatal injuries. The utilization of intelligent transportation systems would reduce accidents and impart safety to the overall transportation network. This research aims to impart transportation safety through a real-time alert warning system for avoiding accidents due to potholes and the presence of stray animals. The study incorporates real-time detection of transportation entities like vehicles, animals, and pedestrians through a YOLO v3 computer vision algorithm processed on the GPU environment for a higher frame rate. The potholes and animal hotspots are mapped to form a geospatial database on which the buffer tool of geographic information system (GIS) is applied. The buffer zone was implemented on the geospatial layer to alert the driver in real-time, while the vehicle approaches the buffer zone. The system yields high precision of 0.976 mean average precision (mAP) score of entity detection and the real-time alert warning alerts the driver to ensure transportation safety while avoiding any possible accidents or fatal crashes.",Advanced driver assistance systems (ADAS); Intelligent transportation system; Geographic information system (GIS); Computer vision; Transportation safety; Intelligent system
"We present ResMLP, an architecture built entirely upon multi-layer perceptrons for image classification. It is a simple residual network that alternates (i) a linear layer in which image patches interact, independently and identically across channels, and (ii) a two-layer feed-forward network in which channels interact independently per patch. When trained with a modern training strategy using heavy data-augmentation and optionally distillation, it attains surprisingly good accuracy/complexity trade-offs on ImageNet. We also train ResMLP models in a self-supervised setup, to further remove priors from employing a labelled dataset. Finally, by adapting our model to machine translation we achieve surprisingly good results. We share pre-trained models and our code based on the Timm library.",Transformers; Training; Computer architecture; Machine translation; Decoding; Task analysis; Knowledge engineering; Multi-layer perceptron; computer-vision; NLP
"Traditional manual visual inspections have demonstrated certain shortcomings in post -earthquake assessment of urban buildings, such as being time-consuming and laborious. In contrast, computer vision (CV) and unmanned aerial vehicle (UAV) approaches have revealed competitive potentials in the fields of automatic data acquisition, data processing, and autono-mous decision-making. In UAV images, structural components of post-earthquake buildings often present different scales, which are affected by different local damage. Therefore, acquiring the feature information of structural components has precisely been significant for refined damage assessment of post-earthquake buildings. This study proposes a geometry-informed deep learning -based structural component segmentation of post-earthquake buildings. An Enhanced UNet model is established with a new synthetical loss function containing the geometric consistency (GC) term. Given an edge closure of a connected domain for homogeneous structural components, the GC term comprises split line loss and area loss to adapt to the circumference and area con-straints of each component region. The Enhanced UNet network is designed to improve the extraction capability of high-level features, and it includes six encoder stages (superior to five in the original version), of which the bottom four stages have many convolution layers, and five corresponding decoders. The investigated synthetic QuakeCity dataset includes 4,809 images with a resolution of 1,920 x 1,080 pixels. Training and test results reveal that compared to the original UNet, the proposed method achieves a more stable training process and higher test ac-curacy for structural component segmentation. The proposed method can achieve a mIoU of 97.97 %, which is 1.29 % higher than that of the original UNet. In addition, misrecognition of inner voids inside structural components is addressed, which further validates the optimization efficiency of the proposed geometric constraints. Ablation experiments are conducted to confirm the effectiveness of the proposed GC loss and Enhanced UNet network. The proposed method shows good generalization ability in robustness tests in complex real-world scenarios under various disturbances, including abnormal exposure and rain lines in various intensities.",Geometry -informed deep learning; Geometric consistency loss; Enhanced UNet; Structural component segmentation; Post -earthquake assessment
"Plant disease has a considerable influence on the safety of grain output and quality. Therefore, it is crucial to detect and diagnose plant diseases. Most plant diseases are reflected in plant leaves, and accurate identification of plant diseases require specialized knowledge. Generally, it is difficult for farmers to accurately identify plant disease. Consequently, accurate and timely automatic recognition of plant disease is extremely needed in the area of precision agricultural. To solving this challenge, many classical computer vision and deep learning-based research models have been proposed. Due to the successful performance, deep learning became the preferred approach for plant disease identification. We design a novel transformer block by use of transformer architecture to model long-range features, and of soft split token embedding to capture local information from surrounding pixels and patches, in this paper. Furthermore, inception architecture and cross channel feature learning can improve the information richness, which is especially beneficial to fine-grained feature learning. The proposed model obtains higher accuracy than previous convolution and vision transformer-based models, achieves 99.94% accuracy on VillagePlant, 99.22% accuracy on ibean, 86.89% accuracy on AI2018, and 77.54% accuracy on PlantDoc. The experiment results show its preponderance over the existing models.",Plant disease identification; Deep learning; Vision transformer; Inception convolution
"Vision-based analysis of waterbodies can provide important information required for monitoring, analyzing, and managing water resource systems, such as visual flood detection, delineation, and mapping. Water, however, is an ornery object in image processing, as it can be found in different forms and colors in nature. This makes the detection, classification, and tracking of water in images and videos difficult for computer vision models. There are still visual differences resulting from water texture and its inherent optical properties associated with different waterbodies which can be recognized and extracted to support computer models to better analyze water images. This study aims to utilize a set of early, mid-level, and high-level vision techniques, including Gabor kernels, local binary patterns (LBPs), and deep learning (DL) models to extract and analyze water texture and color of different waterbodies in digital images. For this purpose, ATLANTIS TeXture (ATeX), an image dataset for waterbodies classification and texture analysis, was used. Models were trained for the task of classification on ATeX. Then, the performance of each model in extracting texture features was evaluated and compared. Results showed that the classification accuracy achieved by the Gabor magnitude tensor, LBP, and DL model (ShuffleNet V2 x 1.0) are 29, 35, and 92%, respectively, and thus the DL model outperforms traditional vision-based techniques. Moreover, the classification results on raw images represented by different color spaces (e.g., RGB, HSV, etc.) emphasized the importance of color information for digital image processing of water. Analyzing representative visual features and properties of different water types and waterbodies can facilitate designing a customized Convolutional Neural Networks (CNNs) for water scenes, as CNNs recognize objects through the analysis of both texture and shape clues and their relationship in the entire field of view.",ATLANTIS TeXture; computer vision; machine learning; water texture analysis
"Vision Transformers (ViTs), with the magnificent potential to unravel the information contained within images, have evolved as one of the most contemporary and dominant architectures that are being used in the field of computer vision. These are immensely utilized by plenty of researchers to perform new as well as former experiments. Here, in this article, we investigate the intersection of vision transformers and medical images. We proffered an overview of various ViT based frameworks that are being used by different researchers to decipher the obstacles in medical computer vision. We surveyed the applications of Vision Transformers in different areas of medical computer vision such as image-based disease classification, anatomical structure segmentation, registration, region-based lesion detection, captioning, report generation, and reconstruction using multiple medical imaging modalities that greatly assist in medical diagnosis and hence treatment process. Along with this, we also demystify several imaging modalities used in medical computer vision. Moreover, to get more insight and deeper understanding, the self-attention mechanism of transformers is also explained briefly. Conclusively, the ViT based solutions for each image analytics task are critically analyzed, open challenges are discussed and the pointers to possible solutions for future direction are deliberated. We hope this review article will open future research directions for medical computer vision researchers.",Vision Transformers; Medical image analysis; Self attention; Medical computer vision; Diagnostic image analysis; Literature survey
"Semantic segmentation for large-scale point clouds in 3D computer vision remains challenging. Most existing studies focus on creating complex local geometry extractors without considering the sparsity of point clouds and the multi-scale problem of objects in large-scale, resulting in networks that fail to efficiently extract local features and affect segmentation accuracy. In this study, we propose a novel Feature Affine Residual (FA-Res) learnable module for this problem to learn robust point cloud semantic information from the point cloud domain. First, we create a Local Relation-shape Learning module to learn local shape relationships, thereby supplementing the structural information. Second, we propose a lightweight Feature Affine module that can conduct adaptive modifications on local point features to reduce differences between local point clouds with varying densities and the K-nearest neighbor (KNN) algorithm's domain determination. Finally, we design a Residual MLP Pooling module that can learn deep aggregation features to explore more sophisticated semantic data and provide better guidance for semantic segmentation. We compare our network with state-of-the-art networks on two separate datasets to show its effectiveness. Specifically, our method achieves 68.1% mIoU on S3DIS tested on Area 5, which is an improvement of 2.7% compared with the latest representative network.",Point clouds; 3D computer vision; Semantic segmentation; Residual MLP; Feature affine
"Human beings have a natural ability to perceive invisible things around them based on materials. This paper examines the problems encountered during image outpainting, which is widely utilized in computational photography, image editing and computer graphics. Although considerable progress has been made in image outpainting, problems such as semantic ambiguity, structural confusion and poor quality still arise. Inspired by the idea that humans draw a boundary contour, paint it with color, and then fill the content, we propose a three -stage GAN model, defined as the ECPIO network. The first-stage model is an edge prediction network (EP-net) that is used to predict the edge map of missing areas. The second-stage model is a color prediction network (CP-Net) that is utilized to predict the color map in the missing area, and the third-stage model is an image out -painting network (IO-Net) that is employed to generate the outpainting results. Since edge and semantics constrain the extended area, we introduce edge and semantic information to the discriminator to guide the image generation. Our method achieved a good performance with a PSNR of 27.15 and an SSIM of 0.78 for the City-scapes dataset. The experimental results for other public datasets also show that our method can recover accurate semantic content and accurate structure information of missing regions. In addition, our method achieves good effects in semantic coherence, structure vision, color vision and texture vision. Our method has a wide range of intelligent image processing systems, such as computer photography, image editing, and image restoration.",Image outpainting; Image extrapolation; Image completion; GAN; Edge prediction; Color prediction
"Orientation estimation is one of the core problems in several computer vision tasks. Recently deep learning techniques combined with the Bingham distribution have attracted considerable interest towards this problem when considering ambiguities and rotational symmetries of objects. However, existing works suffer from two issues. First, the computational overhead for calculating the normalisation constant of the Bingham distribution is relatively high. Second, the choice of loss functions is uncertain. In light of these problems, we present an online deep Bingham network to estimate the orientation of objects. We sharply reduce the computational overhead of the normalisation constant by directly applying a numerical integration formula. Additionally, we are the first to give theorems on the convexity and Lipschitz continuity of the Bingham distribution's negative log-likelihood, which formally indicates that it is a proper choice of the loss function. We test our method on three public datasets, namely the UPNA, the T-LESS and Pascal3D+, showing that our method outperforms the state-of-the-art in terms of orientation accuracy and time efficiency, which can reduce the runtime by more than 6 h compared to the offline methods. The ablation experiments further demonstrate the effectiveness and robustness of our model.",computer vision; pose estimation; probability; robot vision
"3D object matching and registration on point clouds are widely used in computer vision. However, most existing point cloud registration methods have limitations in handling non-rigid point sets or topology changes (e.g. connections and separations). As a result, critical characteristics such as large inter-frame motions of the point clouds may not be accurately captured. This paper proposes a statistical algorithm for non-rigid point sets registration, addressing the challenge of handling topology changes without the need to estimate correspondence. The algorithm uses a novel Break and Splice framework to treat the non-rigid registration challenges as a reproduction process and a Dirichlet Process Gaussian Mixture Model (DPGMM) to cluster a pair of point sets. Labels are assigned to the source point set with an iterative classification procedure, and the source is registered to the target with the same labels using the Bayesian Coherent Point Drift (BCPD) method. The results demonstrate that the proposed approach achieves lower registration errors and efficiently registers point sets undergoing topology changes and large inter-frame motions. The proposed approach is evaluated on several data sets using various qualitative and quantitative metrics. The results demonstrate that the Break and Splice framework outperforms state-of-the-art methods, achieving an average error reduction of about 60% and a registration time reduction of about 57.8%.",non-rigid registration; point cloud; topology changes; Gaussian Mixture Model; computer vision
"Deep learning in computer vision is becoming increasingly popular and useful for tracking object movement in many application areas, due to data collection burgeoning from the rise of the Internet of Things (IoT) and Big Data. So far, computer vision has been used in industry predominantly for quality inspection purposes such as surface defect detection; however, an emergent research area is the application for process monitoring involving tracking moving machinery in real time. In steelmaking, the deployment of computer vision for process monitoring is hindered by harsh environments, poor lighting conditions and fume presence. Therefore, application of computer vision remains unplumbed. This paper proposes a novel method for tracking hot metal ladles during pouring in poor lighting. The proposed method uses contrast-limited adaptive histogram equalisation (CLAHE) for contrast enhancement, Mask R-CNN for segmentation prediction and Kalman filters for improving predictions. Pixel-level tracking enables pouring height and rotation angle estimation which are controllable parameters. Flame severity is also estimated to indicate process quality. The method has been validated with real data collected from ladle pours. Currently, no publications presenting a method for tracking ladle pours exist. The model achieved a mean average precision (mAP) of 0.61 by the Microsoft Common Objects in Context (MSCOCO) standard. It measures key process parameters and process quality in processes with high variability, which significantly contributes to process enhancement through root-cause analysis, process optimisation and predictive maintenance. With real-time tracking, predictions could automate ladle controls for closed-loop control to minimise emissions and eliminate variability from human error.",Computer vision; Deep learning; Manufacturing; Remote condition monitoring; Segmentation
"Hand dysfunction caused by hand injuries, strokes, or other neurological degenerative diseases such as cervical spondylosis is being increasingly reported. Currently, hand function assessments for diagnosis or rehabilitation are primarily based on qualitative scales, which are subjective and may vary considerably depending on the expertise of the attending clinician. Although wearable sensors and computer vision techniques have been proposed to obtain quantitative hand movement information, both have limitations. In this study, a multiview video tracking and recording system was set up using high-speed cameras and mapping of actual hand movements. The state-of-the-art software DeepLabCut was used to obtain precise 2D and 3D finger joint positions. Kinematic parameters, such as movement count, period, phase, and Pearson coefficient were used to characterize hand movement based on the relative distance-time curves of finger joints. Experimental results in a clinical setting showed that this video-based image-recognition neural network method can accurately distinguish healthy from dysfunctional hand movements. The proposed system is inexpensive, easy to set up and use, and exhibits high accuracy. Thus, it can revolutionize medical hand motion analysis and spur the development of automated quantitative systems for early hand-related disorder detection.",Hand function; Computer vision; Machine learning; Quantitative assessment; Cervical spondylosis
"The data acquired in civil engineering tasks often involve high acquisition costs, and the available datasets tend to have a limited number of samples and are highly biased. To estimate the performance of machine learning models, k-fold cross-validation (k-CV) is widely used. However, if only limited data are available and the data distribution is biased, k-CV tends to overestimate the performance for practical applications. This study proposed a new estimator, leave one reference out and k-CV (LORO-k-CV), to determine the practical performance of machine learning models, that is, the generalization performance for population data in the target task, in case data are collected by multiple references resulting in biased data. LORO-k-CV is a combination of a new concept, LORO-CV, that estimates the performance in the extrapolation region of the training data without human intervention and k-CV, considering the ratio of the interpolation and extrapolation regions. The efficacy of LORO-k-CV was validated with its application to the regression task for the chloride-ion concentration of concrete structures. To more specifically demonstrate the advantages of LORO-k-CV in model construction, the feature selections were conducted using both k-CV and LORO-k-CV methods. These results revealed that LORO-k-CV can effectively construct a model with improved generalization performance even from the same data in cases where data are collected by multiple references, resulting in biased data.",
"Automatic Visual Captioning (AVC) generates syntactically and semantically correct sentences by describing important objects, attributes, and their relationships with each other. It is classified into two categories: image captioning and video captioning. It is widely used in various applications such as assistance for the visually impaired, human-robot interaction, video surveillance systems, scene understanding, etc. With the unprecedented success of deep-learning in Computer Vision and Natural Language Processing, the past few years have seen a surge of research in this domain. In this survey, the state-of-the-art is classified based on how they conceptualize the captioning problem, viz., traditional approaches that cast visual description either as retrieval or template-based description and deep learning approaches. A detailed review of existing methods, highlighting their pros and cons, societal impact as the number of citations, architectures used, datasets experimented on and GitHub link is presented. Moreover, the survey also provides an overview of the benchmark image and video datasets and the evaluation measures that have been developed to assess the quality of machine-generated captions. It is observed that dense or paragraph generation and Change Image Captioning (CIC) are stimulating the research community more due to the near-to-human abstraction ability. Finally, the paper explores future directions in the area of automatic visual caption generation.",Visual Captioning; Image Captioning; Video Captioning; Change Image Captioning (CIC); LSTM; CNN; RNN; Computer Vision (CV); Natural Language Processing (NLP)
"Computer vision-based detection approaches have been widely used in defect inspection tasks. However, identifying small-sized defects is still a challenge for most existing methods. It is mainly because: (1) the existing methods fail to extract sufficient information from the small-sized defects; (2) the existing detectors cannot generate effective region proposals for small-sized defects, which results in a low recall rate. To address the above issues, an adaptive loss weighting multi-task model with attention-guide proposal generation is proposed. First, the proposed multi-task model can excavate contextual information to enrich the feature information of small-sized defect areas, enhancing the model's representation capability. Additionally, to improve the recall rate of small-sized defects, an object attention-guide proposal generation module is proposed by leveraging object attention to guide the confidence enhancement of small-sized defects, which can generate more high-quality region proposals for small-sized defects. Finally, to speed up the joint optimization of the proposed multi-task framework, an adaptive loss weighting algorithm is proposed to learn the optimal combination of multi-task loss functions by maintaining the gradient direction consistency and tuning each task's loss magnitude. The experimental results on the two public defect datasets demonstrate that the proposed method outperforms other state-of-the-art methods.",Multi-task learning; Proposal generation; Adaptive loss weighting algorithm; Surface defect detection; Small object detection; Computer vision
"Gastrointestinal cancer is a prevalent disease, and analyzing pathological images is crucial for its diagnosis and treatment. Considering the characteristics of pathological images, we propose a novel cell nucleus segmentation method based on Vision Transform, namely NST. Our proposed method consists of a Deformable Attention Transformer (DAT) encoder capturing four different levels of feature; a Coordinate Attention Module (CAM) handling shallow-level features in different dimensions; a Dense Aggregation Module (DAM) integrating deep-level features; and a Similarity Aggregation Module (SAM) combining features to generate pixel-level segmentation predictions. Meanwhile, to fill the data gap in the field of cell nucleus segmentation, we acquire, annotate and present a new dataset of gastrointestinal cancer pathology images named GCNS. Moreover, we conducted a series of experiments, and the experimental results indicate that our proposed method achieves state-of-the-art performance, as high as a 0.725 Dice Score on the GCNS dataset.",Deep learning; Computer vision; Medical image segmentation; Nuclei segmentation; Gastrointestinal cancer diagnosis
"To mitigate the current COVID-19 pandemic, policy makers at the Greater London Authority, the regional governance body of London, UK, are reliant upon prompt, accurate and actionable estimations of lockdown and social distancing policy adherence. Transport for London, the local transportation department, reports they implemented over 700 interventions such as greater signage and expansion of pedestrian zoning at the height of the pandemic's first wave with our platform providing key data for those decisions. Large well-defined heterogeneous compositions of pedestrian footfall and physical proximity are difficult to acquire, yet necessary to monitor city-wide activity (busyness) and consequently discern actionable policy decisions. To meet this challenge, we leverage our existing large-scale data processing urban air quality machine learning infrastructure to process over 900 camera feeds in near real-time to generate new estimates of social distancing adherence, group detection and camera stability. In this work, we describe our development and deployment of a computer vision and machine learning pipeline. It provides near immediate sampling and contextualization of activity and physical distancing on the streets of London via live traffic camera feeds. We introduce a platform for inspecting, calibrating and improving upon existing methods, describe the active deployment on real-time feeds and provide analysis over an 18 month period.",COVID-19; Computer Vision; Real-time; Computers and Society; Machine Learning; Policy Intervention; Change Point Detection; Social Distancing
"Pedestrian detection (PD) is a vital computer vision (CV) problem that is highly employed in several real-time applications, namely autonomous driving methods, robotics, and security observing methods. Simulated by deep learning (DL) approaches to the recognition of generic objects, several investigation mechanisms have attained maximum recognition accuracy for acceptable scale and non-blocked pedestrians. However, the detection efficiency needed to be improved for complex cases like rare pose samples, crowd scenes, and cases with worse visibility due to daytime or weather. Therefore, this study develops a multimodal pedestrian detection system in crowded scenes using metaheuristics and a deep convolutional neural network (MMPD-MDCNN) technique. The MMPD-MDCNN technique's goal is to identify pedestrians in crowd scenes using different deep-learning models effectively. The proposed MMPD-MDCNN technique integrates three deep learning models: the residual network (ResNet-50), Inception v3, and the capsule network (CapsNet). In addition, the Harris Hawks Optimization (HHO) algorithm is applied for optimal hyperparameter tuning of the deep learning models. For pedestrian detection, the MMPD-MDCNN technique uses the long short-term memory (LSTM) model, and its hyperparameters can be adjusted by the shark smell optimization (SSO) algorithm. To demonstrate the superior performance of the MMPD-MDCNN approach, A comprehensive set of simulations on the INRIA and UCSD datasets was performed to illustrate the superior performance of the MMPD-MDCNN approach. The experimental results suggest that the MMPD-MDCNN model performs well on both datasets.",Pedestrian detection; Computer vision; Crowded scenes; Multi-modal; Deep learning; Hyperparameter tuning
"Hand gesture recognition (HGR) is the most important part of human-computer interaction (HCI). Static hand gesture recognition is equivalent to the classification of hand gesture images. At present, the classification of hand gesture images mainly uses the Convolutional Neural Network (CNN) method. The Vision Transformer architecture (ViT) proposes not to use the convolutional layers at all but to use the multi-head attention mechanism to learn global information. Therefore, this paper proposes a static hand gesture recognition method based on the Vision Transformer. This paper uses a self-made dataset and two publicly available American Sign Language (ASL) datasets to train and evaluate the ViT architecture. Using the depth information provided by the Microsoft Kinect camera to capture the hand gesture images and filter the background, then use the eight-connected discrimination algorithm and the distance transformation algorithm to remove the redundant arm information. The resulting images constitute a self-made dataset. At the same time, this paper studies the impact of several data augmentation strategies on recognition performance. This paper uses accuracy, F1 score, recall, and precision as evaluation metrics. Finally, the validation accuracy of the proposed model on the three datasets achieves 99.44%, 99.37%, and 96.53%, respectively, and the results obtained are better than those obtained by some CNN structures.",Hand gesture recognition; Vision Transformer; Arm removal; Data augmentation
"Convolutional neural networks (CNN) have transformed the field of computer vision by enabling the automatic extraction of features, obviating the need for manual feature engineering. Despite their success, identifying an optimal architecture for a particular task can be a time-consuming and challenging process due to the vast space of possible network designs. To address this, we propose a novel neural architecture search (NAS) framework that utilizes the clonal selection algorithm (CSA) to automatically design high-quality CNN architectures for image classification problems. Our approach uses an integer vector representation to encode CNN architectures and hyperparameters, combined with a truncated Gaussian mutation scheme that enables efficient exploration of the search space. We evaluated the proposed method on six challenging EMNIST benchmark datasets for handwritten digit recognition, and our results demonstrate that it outperforms nearly all existing approaches. In addition, our approach produces state-of-the-art performance while having fewer trainable parameters than other methods, making it low-cost, simple, and reusable for application to multiple datasets.",clonal selection algorithm (CSA); computer vision; convolutional neural networks (CNN); deep learning EMNIST; neural architecture search (NAS)
"This paper newly proposed a computer vision-based crack quantification algorithm using a statistical approach. Recently, high-resolution digital images have been effectively utilized for automated crack width and length evaluation on concrete structures. However, cracks are often difficult to accurately measure by randomly distributed and complex shapes. To overcome the technical limitation, a novel statistical crack quantification algorithm is proposed and experimentally validated through concrete structures in this paper. First, cracks on digital images are automatically detected using a deep semantic segmentation network. Then, multi-branched cracks are separated into single cracks through crack map generation. Each separated crack length and width is calculated by the Euclidean distance transform algorithm. Finally, crack width is presented as a representative value with a statistical confidence interval. The quantitative crack evaluation results for width and length were successfully compared with the actual field measurement values by average differences of 18.07% and -26.28%, respectively.",Statistical crack quantification; Automated crack detection; Crack separation; Crack map; Deep learning; Computer vision
"Computer recognition of human activity is an important area of research in computer vision. Human activity recognition (HAR) involves identifying human activities in real-life contexts and plays an important role in interpersonal interaction. Artificial intelligence usually identifies activities by analyzing data collected using different sources. These can be wearable sensors, MEMS devices embedded in smartphones, cameras, or CCTV systems. As part of HAR, computer vision technology can be applied to the recognition of the emotional state through facial expressions using facial positions such as the nose, eyes, and lips. Human facial expressions change with different health states. Our application is oriented toward the detection of the emotional health of subjects using a self-normalizing neural network (SNN) in cascade with an ensemble layer. We identify the subjects' emotional states through which the medical staff can derive useful indications of the patient's state of health.",HAR; face emotion recognition; face detection; computer vision; deep learning; SNN; ensemble; vectorflow
"This research describes the use of high-performance computing (HPC) and deep learning to create prediction models that could be deployed on edge AI devices equipped with camera and installed in poultry farms. The main idea is to leverage an existing IoT farming platform and use HPC offline to run deep learning to train the models for object detection and object segmentation, where the objects are chickens in images taken on farm. The models can be ported from HPC to edge AI devices to create a new type of computer vision kit to enhance the existing digital poultry farm platform. Such new sensors enable implementing functions such as counting chickens, detection of dead chickens, and even assessing their weight or detecting uneven growth. These functions combined with the monitoring of environmental parameters, could enable early disease detection and improve the decision-making process. The experiment focused on Faster R-CNN architectures and AutoML was used to identify the most suitable architecture for chicken detection and segmentation for the given dataset. For the selected architectures, further hyperparameter optimization was carried out and we achieved the accuracy of AP = 85%, AP50 = 98%, and AP75 = 96% for object detection and AP = 90%, AP50 = 98%, and AP75 = 96% for instance segmentation. These models were installed on edge AI devices and evaluated in the online mode on actual poultry farms. Initial results are promising, but further development of the dataset and improvements in prediction models is needed.",computer vision; convolutional neural networks; deep learning; digital farm management; edge AI; high-performance computing; machine learning; smart farms
"The manner of walking (gait) is a powerful biometric that is used as a unique fingerprinting method, allowing unobtrusive behavioral analytics to be performed at a distance without subject cooperation. As opposed to more traditional biometric authentication methods, gait analysis does not require explicit cooperation of the subject and can be performed in low-resolution settings, without requiring the subject's face to be unobstructed/clearly visible. Most current approaches are developed in a controlled setting, with clean, gold-standard annotated data, which powered the development of neural architectures for recognition and classification. Only recently has gait analysis ventured into using more diverse, large-scale, and realistic datasets to pretrained networks in a self-supervised manner. Self-supervised training regime enables learning diverse and robust gait representations without expensive manual human annotations. Prompted by the ubiquitous use of the transformer model in all areas of deep learning, including computer vision, in this work, we explore the use of five different vision transformer architectures directly applied to self-supervised gait recognition. We adapt and pretrain the simple ViT, CaiT, CrossFormer, Token2Token, and TwinsSVT on two different large-scale gait datasets: GREW and DenseGait. We provide extensive results for zero-shot and fine-tuning on two benchmark gait recognition datasets, CASIA-B and FVG, and explore the relationship between the amount of spatial and temporal gait information used by the visual transformer. Our results show that in designing transformer models for processing motion, using a hierarchical approach (i.e., CrossFormer models) on finer-grained movement fairs comparatively better than previous whole-skeleton approaches.",gait recognition; biometric authentication; vision transformer; pose estimation; self-supervised learning; contrastive learning
"As technology continues to develop, computer vision (CV) applications are becoming increasingly widespread in the intelligent transportation systems (ITS) context. These applications are developed to improve the efficiency of transportation systems, increase their level of intelligence, and enhance traffic safety. Advances in CV play an important role in solving problems in the fields of traffic monitoring and control, incident detection and management, road usage pricing, and road condition monitoring, among many others, by providing more effective methods. This survey examines CV applications in the literature, the machine learning and deep learning methods used in ITS applications, the applicability of computer vision applications in ITS contexts, the advantages these technologies offer and the difficulties they present, and future research areas and trends, with the goal of increasing the effectiveness, efficiency, and safety level of ITS. The present review, which brings together research from various sources, aims to show how computer vision techniques can help transportation systems to become smarter by presenting a holistic picture of the literature on different CV applications in the ITS context.",intelligent transportation systems; computer vision; automatic number plate recognition (ANPR); traffic sign detection; vehicle detection; pedestrian detection; lane line detection; obstacle detection; anomaly detection; structural damage detection; autonomous vehicles
"Traffic congestion detection method based on surveillance video is gradually widely used in intelligent transportation systems (ITS). Due to complex challenges such as weather change, vehicle occlusion, camera jitter, camera installation location, and so on, current methods are difficult to balance in real time and accuracy. Here, a new real-time and robust traffic congestion detection framework and a vision-based multi-dimensional congestion detection model are proposed. Firstly, the framework introduces an object detector based on the lightweight convolutional neural network (CNN) and an efficient multi-object IoU-like tracker to obtain traffic dynamic information in real time. Then, traffic density, traffic velocity, and duration of instantaneous congestion are defined and a multi-dimensional congestion detection model is established. Furthermore, an adaptive updating strategy of dynamic parameters is investigated. Finally, in multiple groups of comparative experiments, the framework is verified to be applicable to a variety of lightweight CNN detectors and IoU-like trackers. The precision and recall of the multi-dimensional congestion model can reach 95.1% and 92.1% respectively, with 43FPS. The comparative experimental results show that the proposed method is real-time, more robust and accurate, and can be employed for online traffic congestion detection based on surveillance video.",computer vision; object tracking; traffic
"Color constancy refers to our capacity to see consistent colors under different illuminations. In computer vision and image processing, color constancy is often approached by explicit estimation of the scene's illumination, followed by an image correction. In contrast, color constancy in human vision is typically measured as the capacity to extract color information about objects and materials in a scene consistently throughout various illuminations, which goes beyond illumination estimation and might require some degree of scene and color understanding. Here, we pursue an approach with deep neural networks that tries to assign reflectances to individual objects in the scene. To circumvent the lack of massive ground truth datasets labeled with reflectances, we used computer graphics to render images. This study presents a model that recognizes colors in an image pixel by pixel under different illumination conditions. (c) 2023 Optica Publishing Group under the terms of the Optica Open Access Publishing Agreement",
"Due to the frequent and sudden occurrence of urban waterlogging, targeted and rapid risk monitoring is extremely important for urban management. To improve the efficiency and accuracy of urban waterlogging monitoring, a real-time determination method of urban waterlogging based on computer vision technology was proposed in this study. First, city images were collected and then identified using the ResNet algorithm to determine whether a waterlogging risk existed in the images. Subsequently, the recognition accuracy was improved by image augmentation and the introduction of an attention mechanism (SE-ResNet). The experimental results showed that the waterlogging recognition rate reached 99.50%. In addition, according to the actual water accumulation process, real-time images of the waterlogging area were obtained, and a threshold method using the inverse weight of the time interval (T-IWT) was proposed to determine the times of the waterlogging occurrences from the continuous images. The results showed that the time error of the waterlogging identification was within 30 s. This study provides an effective method for identifying urban waterlogging risks in real-time.",urban waterlogging; real-time monitoring; computer vision; ResNet
"In recent years, many studies have been conducted on the vision-based displacement measurement system using an unmanned aerial vehicle, which has been used in actual structure measurements. In this study, the dynamic measurement reliability of a vision-based displacement measurement system using an unmanned aerial vehicle was examined by measuring various vibrations with a frequency of 0 to 3 Hz and a displacement of 0 to 100 mm. Furthermore, free vibration was applied to model structures with one and two stories, and the response was measured to examine the accuracy of identifying structural dynamic characteristics. The vibration measurement results demonstrated that the vision-based displacement measurement system using an unmanned aerial vehicle has an average root mean square percentage error of 0.662% compared with the laser distance sensor in all experiments. However, the errors were relatively large in the displacement measurement of 10 mm or less regardless of the frequency. In the structure measurements, all sensors demonstrated the same mode frequency based on the accelerometer, and the damping ratios were extremely similar, except for the laser distance sensor measurement value of the two-story structure. Mode shape estimation was obtained and compared using the modal assurance criterion value compared with the accelerometer, and the values for the vision-based displacement measurement system using an unmanned aerial vehicle were close to 1. According to these results, the vision-based displacement measurement system using an unmanned aerial vehicle demonstrated results similar to those of conventional displacement sensors and can thus replace conventional displacement sensors.",VDMS using UAV; shaking table test; system identification; reliability assessment
"Smart farming (SF) applications rely on robust and accurate computer vision systems. An important computer vision task in agriculture is semantic segmentation, which aims to classify each pixel of an image and can be used for selective weed removal. State-of-the-art implementations use convolutional neural networks (CNN) that are trained on large image datasets. In agriculture, publicly available RGB image datasets are scarce and often lack detailed ground-truth information. In contrast to agriculture, other research areas feature RGB-D datasets that combine color (RGB) with additional distance (D) information. Such results show that including distance as an additional modality can improve model performance further. Therefore, we introduce WE3DS as the first RGB-D image dataset for multi-class plant species semantic segmentation in crop farming. It contains 2568 RGB-D images (color image and distance map) and corresponding hand-annotated ground-truth masks. Images were taken under natural light conditions using an RGB-D sensor consisting of two RGB cameras in a stereo setup. Further, we provide a benchmark for RGB-D semantic segmentation on the WE3DS dataset and compare it with a solely RGB-based model. Our trained models achieve up to 70.7% mean Intersection over Union (mIoU) for discriminating between soil, seven crop species, and ten weed species. Finally, our work confirms the finding that additional distance information improves segmentation quality.",crop farming; weed detection; semantic segmentation; image dataset; RGB-D; stereo vision
"Transformers have been widely used in various computer vision applications. Compared to traditional convolutional neural networks (CNNs), transformer's inference includes plenty of non-linear operations, such as softmax and Gaussian error linear units (GELU). As the scale of transformers grows, an efficient hardware implementation of these operations is significant. However, the current works of computer vision neural network accelerators focus on CNN and less attention is paid to transformer. In addition, most current FPGA-based softmax or GELU accelerators are not designed for vision transformer (ViT). To solve this problem, this work proposes a high speed reconfigurable accelerator. The architecture can support both softmax and GELU functions in ViT by reconfiguring the data path. This architecture on Xilinx XCVU37P is implemented through mathematical transformation and hardware optimization design, and achieve the performance of 102.4 Giga bits per second (Gbps) at 200 MHz. Experimental results show that the architecture achieves a very small accuracy loss in the ViT's inference by using fixed-point 16-bit quantization. Compared with existing accelerators, the design has greater throughput and area efficiency.",
"The efficient and automatic detection of chest abnormalities is vital for the auxiliary diagnosis of medical images. Many studies utilize computer vision and deep learning approaches involving symmetry and asymmetry concepts to detect chest abnormalities, and achieve promising findings. However, an accurate instance-level and multi-label detection of abnormalities in chest X-rays remains a significant challenge. Here, a novel anomaly detection method for symmetric chest X-rays using dual-attention and multi-scale feature fusion is proposed. Three aspects of our method should be noted in comparison with the previous approaches. We improved the deep neural network with channel-dimensional and spatial-dimensional attention to capture the abundant contextual features. We then used an optimized multi-scale learning framework for feature fusion to adapt to the scale variation in the abnormalities. Considering the influence of the data imbalance and other factors, we introduced a seesaw loss function to flexibly adjust the sample weights and enhance the model learning efficiency. The rigorous experimental evaluation of a public chest X-ray dataset with fourteen different types of abnormalities demonstrates that our model has a mean average precision of 0.362 and outperforms existing methods.",thoracic disease detection; medical diagnosis; instance-level detection; deep learning; attention mechanism; computer vision
"The most negative effects caused by earthquakes are the damage and collapse of buildings. Seismic building retrofitting and repair can effectively reduce the negative impact on post-earthquake buildings. The priority to repair the construction after being damaged by an earthquake is to perform an assessment of seismic buildings. The traditional damage assessment method is mainly based on visual inspection, which is highly subjective and has low efficiency. To improve the intelligence of damage assessments for post-earthquake buildings, this paper proposed an assessment method using CV (Computer Vision) and AR (Augmented Reality). Firstly, this paper proposed a fusion mechanism for the CV and AR of the assessment method. Secondly, the CNN (Convolutional Neural Network) algorithm and gray value theory are used to determine the damage information of post-earthquake buildings. Then, the damage assessment can be visually displayed according to the damage information. Finally, this paper used a damage assessment case of seismic-reinforced concrete frame beams to verify the feasibility and effectiveness of the proposed assessment method.",damage assessment; computer vision; augmented reality; post-earthquake buildings
"Unpaved roads are an important part of the road transportation system of many countries and they contribute to the accessibility of remote communities and businesses. Despite the importance of unpaved road networks on social and economic development of remote regions, research on semiautomated and automated assessment of these roads is limited. This paper proposes low-cost computer vision-based solutions for assessment of unpaved roads using two approaches: unmanned aerial vehicle (UAV) and participatory-based imaging methods. Both methods use deep neural network to process captured images and locate major road distresses, including potholes, rutting, and corrugations. In addition, a method is proposed to estimate the size of detected potholes in the UAV-captured video frames. Each of the proposed methods was evaluated using a set of experiments, which demonstrated promising performance in assessment of these infrastructure assets that are vital for reliable access of rural and remote communities.",Unpaved roads; Road surface defects; Unmanned aerial vehicles (UAVs); Participatory-based assessment; Deep learning; Image processing
"This research article is aimed at improving the efficiency of a computer vision system that uses image processing for detecting cracks. Images are prone to noise when captured using drones or under various lighting conditions. To analyze this, the images were gathered under various conditions. To address the noise issue and to classify the cracks based on the severity level, a novel technique is proposed using a pixel-intensity resemblance measurement (PIRM) rule. Using PIRM, the noisy images and noiseless images were classified. Then, the noise was filtered using a median filter. The cracks were detected using VGG-16, ResNet-50 and InceptionResNet-V2 models. Once the crack was detected, the images were then segregated using a crack risk-analysis algorithm. Based on the severity level of the crack, an alert can be given to the authorized person to take the necessary action to avoid major accidents. The proposed technique achieved a 6% improvement without PIRM and a 10% improvement with the PIRM rule for the VGG-16 model. Similarly, it showed 3 and 10% for ResNet-50, 2 and 3% for Inception ResNet and a 9 and 10% increment for the Xception model. When the images were corrupted from a single noise alone, 95.6% accuracy was achieved using the ResNet-50 model for Gaussian noise, 99.65% accuracy was achieved through Inception ResNet-v2 for Poisson noise, and 99.95% accuracy was achieved by the Xception model for speckle noise.",cracks; deep learning; detection; images; noise; integrity; safety
"Computer vision in consideration of automated and robotic systems has come up as a steady and robust platform in sewer maintenance and cleaning tasks. The AI revolution has enhanced the ability of computer vision and is being used to detect problems with underground sewer pipes, such as blockages and damages. A large amount of appropriate, validated, and labeled imagery data is always a key requirement for learning AI-based detection models to generate the desired outcomes. In this paper, a new imagery dataset S-BIRD (Sewer-Blockages Imagery Recognition Dataset) is presented to draw attention to the predominant sewers' blockages issue caused by grease, plastic and tree roots. The need for the S-BIRD dataset and various parameters such as its strength, performance, consistency and feasibility have been considered and analyzed for real-time detection tasks. The YOLOX object detection model has been trained to prove the consistency and viability of the S-BIRD dataset. It also specified how the presented dataset will be used in an embedded vision-based robotic system to detect and remove sewer blockages in real-time. The outcomes of an individual survey conducted at a typical mid-size city in a developing country, Pune, India, give ground for the necessity of the presented work.",sewer monitoring; S-BIRD dataset; object detection; computer vision; YOLOX training; AI techniques
"In this paper we propose a novel rotation normalization technique for point cloud processing using an oriented bounding box. We use this method to create a point cloud annotation tool for part segmentation on real camera data. Custom data sets are used to train our network for classification and part segmentation tasks. Successful deployment is completed on an embedded device with limited processing power. A comparison is made with other rotation-invariant features in noisy synthetic datasets. Our method offers more auxiliary information related to the dimension, position, and orientation of the object than previous methods while performing at a similar level.",computer vision; object part segmentation; classification
"Robots play a pivotal role in the manufacturing industry. This has led to the development of computer vision. Since AlexNet won ILSVRC, convolutional neural networks (CNNs) have achieved state-of-the-art status in this area. In this work, a novel method is proposed to simultaneously detect and predict the localization of objects using a custom loop method and a CNN, performing two of the most important tasks in computer vision with a single method. Two different loss functions are proposed to evaluate the method and compare the results. The obtained results show that the network is able to perform both tasks accurately, classifying images correctly and locating objects precisely. Regarding the loss functions, when the target classification values are computed, the network performs better in the localization task. Following this work, improvements are expected to be made in the localization task of networks by refining the training processes of the networks and loss functions.",image classification; object detection; deep learning; deep convolutional neural networks; computer vision; custom training loop
"The field of computer vision has been applied in many topics and scenes, especially in the shipping business which occupies a large position in the world trade. With the development of ship intellectualization, the task of detection, tracking, segmentation and classification of interested targets become more and more important. Publicly available dataset is the foundation to promote research in shipping. Based on this intention, we systematically present a review of maritime datasets on maritime perception. In this paper, comparison is made in terms of data type, environment, ground authenticity, and applicable research directions. The aim of writing this paper is to help researchers quickly identify the most suitable dataset for their work.",Computer vision; Maritime perception; Maritime ship dataset; Ship intellectualization
"In today's era, monitoring the health of the manufacturing environment has become essential in order to prevent unforeseen repairs, shutdowns, and to be able to detect defective products that could incur big losses. Data-driven techniques and advancements in sensor technology with Internet of the Things (IoT) have made real-time tracking of systems a reality. The health of a product can also be continuously assessed throughout the manufacturing lifecycle by using Quality Control (QC) measures. Quality inspection is one of the critical processes in which the product is evaluated and deemed acceptable or rejected. The visual inspection or final inspection process involves a human operator sensorily examining the product to ascertain its status. However, there are several factors that impact the visual inspection process resulting in an overall inspection accuracy of around 80% in the industry. With the goal of 100% inspection in advanced manufacturing systems, manual visual inspection is both time-consuming and costly. Computer Vision (CV) based algorithms have helped in automating parts of the visual inspection process, but there are still unaddressed challenges. This paper presents an Artificial Intelligence (AI) based approach to the visual inspection process by using Deep Learning (DL). The approach includes a custom Convolutional Neural Network (CNN) for inspection and a computer application that can be deployed on the shop floor to make the inspection process user-friendly. The inspection accuracy for the proposed model is 99.86% on image data of casting products.",artificial intelligence; deep learning; quality control; visual inspection; industry 4; 0; smart manufacturing; image recognition; defect detection
"Simple Summary The idea of identifying persons using the fewest traits from the face, particularly the area surrounding the eye, was carried out in light of the present COVID-19 scenario. This may also be applied to doctors working in hospitals, the military, and even in certain faiths where the face is mostly covered, except the eyes. The most recent advancement in computer vision, called vision transformers, has been tested for the UBIPr dataset for different architectures. The proposed model is pretrained on an openly available ImageNet dataset with 1 K classes and 1.3 M pictures before using it on the real dataset of interest, and accordingly the input images are scaled to 224 x 224. The PyTorch framework, which is particularly helpful for creating complicated neural networks, has been utilized to create our models. To avoid overfitting, the stratified K-Fold technique is used to make the model less prone to overfitting. The accuracy results have proven that these techniques are highly effective for both person identification and gender classification. Many biometrics advancements have been widely used for security applications. This field's evolution began with fingerprints and continued with periocular imaging, which has gained popularity due to the pandemic scenario. CNN (convolutional neural networks) has revolutionized the computer vision domain by demonstrating various state-of-the-art results (performance metrics) with the help of deep-learning-based architectures. The latest transformation has happened with the invention of transformers, which are used in NLP (natural language processing) and are presently being adapted for computer vision. In this work, we have implemented five different ViT- (vision transformer) based architectures for person identification and gender classification. The experiment was performed on the ViT architectures and their modified counterparts. In general, the samples selected for train:val:test splits are random, and the trained model may get affected by overfitting. To overcome this, we have performed 5-fold cross-validation-based analysis. The experiment's performance matrix indicates that the proposed method achieved better results for gender classification as well as person identification. We also experimented with train-val-test partitions for benchmarking with existing architectures and observed significant improvements. We utilized the publicly available UBIPr dataset for performing this experimentation.",convolutional neural networks; vision transformers; computer vision; periocular biometrics
"Neural network-based solutions have revolutionized the field of computer vision by achieving outstanding performance in a number of applications. Yet, while these deep learning models outclass previous methods, they still have significant shortcomings relating to generalization and robustness to input disturbances, such as occlusion. Most existing methods that tackle this latter problem use passive neural network architectures that are unable to act on and, thus, influence the observed scene. In this paper, we argue that an active observer agent may be able to achieve superior performance by changing the parameters of the scene, thus, avoiding occlusion by moving to a different position in the scene. To demonstrate this, a reinforcement learning environment is introduced that implements OpenAI Gym's interface, and allows the creation of synthetic scenes with realistic occlusion. The environment is implemented using differentiable rendering, allowing us to perform direct gradient-based optimization of the camera position. Moreover, two additional methods are also presented, one utilizing self-supervised learning to predict occlusion segments, and optimal camera positions, while the other learns to avoid occlusion using Reinforcement Learning. We present comparative experiments of the proposed methods to demonstrate their efficiency. It was shown, via Bayesian t-tests, that the neural network-based methods credibly outperformed the gradient-based avoidance strategy by avoiding occlusion with an average of 5.0 fewer steps in multi-object scenes.",computer vision; object detection; differentiable rendering; self-supervised learning; neural networks; reinforcement learning
"Image-based methods have been applied to support structural monitoring, product and material testing, and quality control. Lately, deep learning for compute vision is the trend, requiring large and labelled datasets for training and validation, which is often difficult to obtain. The use of synthetic datasets is often applying for data augmentation in different fields. An architecture based on computer vision was proposed to measure strain during prestressing in CFRP laminates. The contact-free architecture was fed by synthetic image datasets and benchmarked for machine learning and deep learning algorithms. The use of these data for monitoring real applications will contribute towards spreading the new monitoring approach, increasing the quality control of the material and application procedure, as well as structural safety. In this paper, the best architecture was validated during experimental tests, to evaluate the performance in real applications from pre-trained synthetic data. The results demonstrate that the architecture implemented enables estimating intermediate strain values, i.e., within the range of training dataset values, but it does not allow for estimating strain values outside those range. The architecture allowed for estimating the strain in real images with an error similar to 0.5%, higher than that obtained with synthetic images. Finally, it was not possible to estimate the strain in real cases from the training performed with the synthetic dataset.",machine learning; deep learning; computer vision; CFRP laminates; strengthening RC; strain monitoring
"As the worldwide planting crop, rice feeds nearly half of the world's population. However, the continuous spread of diseases is threatening rice production. It is of great practical value to identify rice diseases precisely. Recent studies suggest that the computational approaches provide an opportunity for rice leaf disease prediction and achieve a series of achievements. However, the existing works for rice leaf disease identification are still unsatisfactory either in identification accuracy or model interpretability. To address these limitations, a residual-distilled transformer architecture is proposed in this study. Inspired by the early success of transformers in computer vision, the distillation strategy is introduced to distill weights and parameters from the pre-trained vision transformer models. The residual concatenation between vision transformer and the distilled transformer are as residual blocks for features extraction, and then fed them into multi-layer perceptron (MLP) for prediction. Experimental results demonstrate that the presented method achieves 0.89 F1-score and 0.92 top-1 accuracy, outperforms the existing state-of-the-art models on the rice leaf disease dataset which collected in paddy fields. In addition, the proposed architecture provides model interpretability to grasp the key features that are significant for positive prediction results.",Vision transformer; Rice disease identification; Agricultural artificial intelligence; Distillation strategy
"Image synthesis is a process of converting the input text, sketch, or other sources, i.e., another image or mask, into an image. It is an important problem in the computer vision field, where it has attracted the research community to attempt to solve this challenge at a high level to generate photorealistic images. Different techniques and strategies have been employed to achieve this purpose. Thus, the aim of this paper is to provide a comprehensive review of various image synthesis models covering several aspects. First, the image synthesis concept is introduced. We then review different image synthesis methods divided into three categories: image generation from text, sketch, and other inputs, respectively. Each sub-category is introduced under the proper category based upon the general framework to provide a broad vision of all existing image synthesis methods. Next, brief details of the benchmarked datasets used in image synthesis are discussed along with specifying the image synthesis models that leverage them. Regarding the evaluation, we summarize the metrics used to evaluate the image synthesis models. Moreover, a detailed analysis based on the evaluation metrics of the results of the introduced image synthesis is provided. Finally, we discuss some existing challenges and suggest possible future research directions.",Image synthesis; Image generation; Generative adversarial networks; Machine learning; Computer vision
"Timely and effective inspection ensures safe operation and optimum resource use for infrastructure maintenance and renewal. Robot advances allow rapid collection of inspection image data. However, distinguishing bridge elements from large amounts of image data is challenging. Rivets are critical elements, joining different profiles into components. However, automatic rivet identification has received little attention. This study proposes a rivet identification method based on computer vision and deep learning. A sustainable training framework is pre-sented to build a robust detector. A novel rivet dataset was collected and annotated from a full-size bridge. YOLOv5 is used to extract features and predicate classifications. The model achieved an 88.9% precision, 90.5% recall, and 90.1% F1 score. The accuracy and robustness were evaluated on another riveted bridge under various operational conditions. The rivet detector generally performs well, achieving 85% or even 95% accuracy in most situations. Out-of-focus and object occlusion have the largest negative effect.",Rivet identification; Bridge inspection; Deep learning; Convolutional neural network; Computer vision
"Sign language recognition is one of the fundamental ways to assist deaf people to communicate with others. An accurate vision-based sign language recognition system using deep learning is a fundamental goal for many researchers. Deep convolutional neural networks have been extensively considered in the last few years, and a slew of architectures have been proposed. Recently, Vision Transformer and other Transformers have shown apparent advantages in object recognition compared to traditional computer vision models such as Faster R-CNN, YOLO, SSD, and other deep learning models. In this paper, we propose a Vision Transformer-based sign language recognition method called DETR (Detection Transformer), aiming to improve the current state-of-the-art sign language recognition accuracy. The DETR method proposed in this paper is able to recognize sign language from digital videos with a high accuracy using a new deep learning model ResNet152 + FPN (i.e., Feature Pyramid Network), which is based on Detection Transformer. Our experiments show that the method has excellent potential for improving sign language recognition accuracy. For instance, our newly proposed net ResNet152 + FPN is able to enhance the detection accuracy up to 1.70% on the test dataset of sign language compared to the standard Detection Transformer models. Besides, an overall accuracy 96.45% was attained by using the proposed method.",Sign language recognition; ResNet152; Detection transformer; Feature pyramid network
"Sealing process is an essential procedure that demands standardization and monitoring during the application to ensure quality. It prevents leakages, corrosion and electrical discharges on components of different products. Most of industrial manufacturers perform the manual application method, which require a proper skill to seal parts. The sealing process could be much more effective if carried out by an automated system, capable of managing the extrusion of the sealant during its application, providing a standardized procedure and favoring the human factors caused by poor working conditions. This article proposes an automated end-effector that continuously measures the contact force of the sealant during its application and also monitors the width and texture of the fillet through a computer vision technique. In order to testify the performance of the proposed system, a case study has been performed with the developed force sensing end-effector coupled to industrial robot manipulator. The results show the possibility of employing online monitoring of sealing application in order to ensure quality during the process.",Sealing; Manufacturing; Force monitoring; Instrumentation; Computer vision
"Drowning is a significant public health concern. A video drowning detection algorithm is a helpful tool for finding drowning victims. However, there are three challenges that drowning detection research typically encounters: a lack of actual drowning video data, subtle early drowning traits, and a lack of real time. In this paper, the authors propose an underwater computer vision based drowning detection device composed of embedded AI devices, camera, and waterproof case to solve the above problems. The detection device utilizes the high-performance computing of Jetson Nano to realize real-time detection of drowning events through the proposed drowning detection algorithm on the acquired underwater video stream. The proposed drowning detection algorithm primarily consists of two stages: in the first step, to successfully solve the interference of the surroundings and to give a trustworthy basis for video drowning detection, the YOLOv5n network is used to detect the near-vertical human body based on the characteristics of the drowning person. In the second stage, the authors propose a lightweight drowning detection network (DDN) based on a deep Gaussian model for fast feature vector detection. The lightweight DDN is combined with the Gaussian model to detect anomaly in the high-level semantic features, which has higher robustness and solves the lack of drowning videos. The experimental results show that the proposed drowning detection algorithm has good comprehensive performance and practical application value.",computer vision; object detection; real-time systems; unsupervised learning
"Different quality grades of tea tend to have a high degree of similarity in appearance. Traditional image-based identification methods have limited effects, while complex deep learning architectures require much data and long-term training. In this paper, two tea quality identification methods based on deep convolutional neural networks and transfer learning are proposed. Different types and quality of tea images are collected by a self-designed computer vision system to form a data set, which is small-scale and of high inter- and intraclass similarity. The first method uses three simplified convolutional neural network (CNN) models with different image input sizes to identify the quality of tea. The second method performs transfer learning to identify the tea quality by fine-tuning the mature AlexNet and ResNet50 architecture. Classification performance and model complexity are measured and compared. The related application software is also developed. The results show that the performance of the CNN models and the transfer learning models are close, and both can achieve high identification accuracy. However, the complexity of the CNN models is two to three orders of magnitude lower than that of the transfer learning models. The study shows that deep CNNs and transfer learning have great potential to be rapid and effective methods for automated tea quality identification tasks with high inter- and intrasimilarity.",computer vision; deep learning; fine-tune; tea quality identification; transfer learning
"Computer vision-based inspection methods show promise for automating post-earthquake building inspections. These methods survey a building with unmanned aerial vehicles and automatically detect damage in the collected images. Nevertheless, assessing the damage's impact on structural safety requires localizing damage to specific building components with known design and function. This paper proposes a BIM-based automated inspection framework to provide context for visual surveys. A deep learning-based semantic segmentation algorithm is trained to automatically identify damage in images. The BIM automatically associates any identified damage with specific building components. Then, components are classified into damage states consistent with component fragility models for integration with a structural analysis. To demonstrate the framework, methods are developed to photorealistically simulate severe structural damage in a synthetic computer graphics environment. A graphics model of a real building in Urbana, Illinois, is generated to test the framework; the model is integrated with a structural analysis to apply earthquake damage in a physically realistic manner. A simulated UAV survey is flown of the graphics model and the framework is applied. The method achieves high accuracy in assigning damage states to visible structural components. This assignment enables integration with a performance-based earthquake assessment to classify building safety.",computer vision; earthquake engineering; building information model; physics-based graphics model; synthetic environment
"<bold> Using microscopy to investigate stomatal behaviour is common in plant physiology research</bold>. Manual inspection and measurement of stomatal pore features is low throughput, relies upon expert knowledge to record stomatal features accurately, requires significant researcher time and investment, and can represent a significant bottleneck to research pipelines. <bold> To alleviate this</bold>, we introduce StomaAI (SAI): a reliable, user-friendly and adaptable tool for stomatal pore and density measurements via the application of deep computer vision, which has been initially calibrated and deployed for the model plant Arabidopsis (dicot) and the crop plant barley (monocot grass). <bold> SAI is capable of producing measurements consistent with human experts and successfully reproduced conclusions of published datasets</bold>. <bold> SAI boosts the number of images that can be evaluated in a fraction of the time</bold>, so can obtain a more accurate representation of stomatal traits than is routine through manual measurement. An online demonstration of SAI is hosted at https://sai.aiml.team, and the full local application is publicly available for free on GitHub through https://github.com/xdynames/sai-app.",applied deep learning; computer vision; convolutional neural network; phenotyping; stomata
"In recent years, multimodal learning has gained acceptability because of the availability of low resource-consuming fusion techniques and robust and powerful deep learning architectures. Visual question answering (VQA) is an interdisciplinary research domain in natural language processing and computer vision. In previous works, researchers have tried to optimize the VQA problems primarily with the help of optimized bilinear fusion techniques. In this paper, we propose a novel question segregation framework for visual question answering to optimize the VQA problem where the VQA framework is segregated by the question type labels. The main contribution of the proposed question segregation framework is the reduction in the execution time and computational resource requirement of VQA models. Six VQA models are tested under the proposed framework and have promising results. The proposed question segregation framework can be extended to other VQA models and datasets, and the problem of model bias toward larger volume question type labels is rectified by having an individual model for each question type label. Also, the proposed question segregation framework provides meaningful answers for a given question by filtering out unrealistic answers by restricting the answer space.",VQA; Computer vision; NLP; Visual and language
"The use of deep learning makes it possible to achieve extraordinary results in all kinds of tasks related to computer vision. However, this performance is strongly related to the availability of training data and its relationship with the distribution in the eventual application scenario. This question is of vital importance in areas such as robotics, where the targeted environment data are barely available in advance. In this context, domain adaptation (DA) techniques are especially important to building models that deal with new data for which the corresponding label is not available. To promote further research in DA techniques applied to robotics, this work presents Kurcuma (Kitchen Utensil Recognition Collection for Unsupervised doMain Adaptation), an assortment of seven datasets for the classification of kitchen utensils-a task of relevance in home-assistance robotics and a suitable showcase for DA. Along with the data, we provide a broad description of the main characteristics of the dataset, as well as a baseline using the well-known domain-adversarial training of neural networks approach. The results show the challenge posed by DA on these types of tasks, pointing to the need for new approaches in future work.",Deep learning; Domain adaptation; Robotics; Computer vision
"Convolutional neural networks (CNNs) have shown excellent performance in numerous computer vision tasks. However, the high computational and memory demands in computer vision tasks prohibit the practical applications of CNNs on edge computing devices. Existing iterative pruning methods suffer from insufficient accuracy recovery after each pruning, which severely affects the importance evaluation of the parameters. Moreover, channel pruning based on the magnitude of parameters often results in performance loss. In this context, we propose an iterative clustering pruning method named ICP together with knowledge transfer for channels. First, channel clustering pruning is performed based on the similarity between feature maps. Then, the intermediate and output features of the original network are applied to guide the learning of the compressed network after each pruning step to quickly recover the network performance and then implement the next pruning operation. Pruning and knowledge transfer are performed alternately to achieve accurate compression of the convolutional network. Finally, we demonstrate the effectiveness of the proposed method on the CIFAR-10, CIFAR-100, and ILSVRC-2012 datasets by pruning VGGNet, ResNet, and GoogLeNet. Our pruning scheme can typically reduce parameters and Floating-point Operations (FLOPs) of the network without harming accuracy significantly. In addition, the ICP was verified to have good practical generalization by compressing the SSD network on the object detection dataset PASCAL VOC.(c) 2023 Elsevier B.V. All rights reserved.",Convolutional neural network; Model compression; Cluster; Image classification; Object detection
"Over the last few years, advanced deep learning-based computer vision algorithms are revolutionizing the manufacturing field. Thus, several industry-related hard problems can be solved by training these algorithms, including flaw detection in various materials. Therefore, identifying steel surface defects is considered one of the most important tasks in the steel industry. In this paper, we propose a deep learning-based model to classify six of the most common steel strip surface defects using the NEU-CLS dataset. We investigate the effectiveness of two state-of-the-art CNN architectures (MobileNet-V2 and Xception) combined with the transfer learning approach. The proposed approach uses an ensemble of two pre-trained state-of-the-art Convolutional Neural Networks, which are MobileNet-V2 and Xception. To perform a comparative analysis of the proposed architectures, several evaluation metrics are adopted, including loss, accuracy, precision, recall, F1-score, and execution time. The experimental results show that the proposed deep ensemble learning approach provides higher performance achieving an accuracy of 99.72% compared to MobileNet-V2 (98.61%) and Xception (99.17%) while preserving fast execution time and small models' size.",Computer vision; Deep learning; Manufacturing; Surface defect; CNN; Hot-rolled steel strips
"Contextual information plays an important role in many computer vision tasks, such as object detection, video action detection, image classification, etc. Recognizing a single object or action out of context could be sometimes very challenging, and context information may help improve the understanding of a scene or an event greatly. Appearance context information, e.g., colors or shapes of the background of an object can improve the recognition accuracy of the object in the scene. Semantic context (e.g. a keyboard on an empty desk vs. a keyboard next to a desktop computer ) will improve accuracy and exclude unrelated events. Context information that are not in the image itself, such as the time or location of an images captured, can also help to decide whether certain event or action should occur. Other types of context (e.g. 3D structure of a building) will also provide additional information to improve the accuracy. In this survey, different context information that has been used in computer vision tasks is reviewed. We categorize context into different types and different levels. We also review available machine learning models and image/video datasets that can employ context information. Furthermore, we compare context-based integration and context-free integration in mainly two classes of tasks: image-based and video-based. Finally, this survey is concluded by a set of promising future directions in context learning and utilization.",Context; Computer Vision; Deep Learning Models; Context Integration
"With the rise of Transformers in computer vision, more and more people believe Transformer-based models would serve as a standard in various vision tasks. However, due to its unprecedented scale and large amount of training data, it is difficult for researchers with less data and limited computing resources to use the Transformer-based model. Recently, a paper proposed ConvMixer to prove the excellent performance of Transformer-based models due to their patch-based representation. Although ConvMixer performs well on image classification, its isotropic architecture is inefficient and unsuitable for other vision tasks. This paper proposes a hierarchical and data-efficient network based on patch-based representation, which we call HEConvMixer. Unlike original Transformer-based models, we use some unsophisticated convolutional blocks to replace Transformer blocks and add two downsample layers in our network. We trained our network on small datasets from scratch by using one GPU. The empirical result shows that our HEConvMixer trained on CIFAR-10 with no extra data for 200 epochs achieves 97.07% accuracy, outperforming previous Transformer-based models and ConvNets.",Transformer; Patch-based representation; Data-efficient; Convolutional neural network
"With the development of neural networking techniques, several architectures for symmetric positive definite (SPD) matrix learning have recently been put forward in the computer vision and pattern recognition (CV&PR) community for mining fine-grained geometric features. However, the degradation of structural information during multi-stage feature transformation limits their capacity. To cope with this issue, this paper develops a U-shaped neural network on the SPD manifolds (U-SPDNet) for visual classification. The designed U-SPDNet contains two subsystems, one of which is a shrinking path (encoder) making up of a prevailing SPD manifold neural network (SPDNet (Huang and Van Gool, 2017)) for capturing compact representations from the input data. Another is a constructed symmetric expanding path (decoder) to upsample the encoded features, trained by a reconstruction error term. With this design, the degradation problem will be gradually alleviated during training. To enhance the representational capacity of U-SPDNet, we also append skip connections from encoder to decoder, realized by manifold-valued geometric operations, namely Riemannian barycenter and Riemannian optimization. On the MDSD, Virus, FPHA, and UAV-Human datasets, the accuracy achieved by our method is respectively 6.92%, 8.67%, 1.57%, and 1.08% higher than SPDNet, certifying its effectiveness.(c) 2022 Published by Elsevier Ltd.",SPD manifold; Neural network; Visual classification; Skip connection; Riemannian barycenter; Riemannian optimization
"Information on fabric material is necessary in washing and ironing clothing. However, indication on a care tag may peel off or the tag may come off due to deterioration over time. Discrimination of the material from the fabric itself is not easy for a general person. Estimating the material of an object is one of the challenging tasks in computer vision. This paper deals with the identification of cloth materials using computer vision. We studied a method to discriminate the fabric material from the image of clothing taken by a smartphone camera. First, we investigated the relationship between image resolution and discrimination accuracy using a convolutional neural network (CNN). As a result, we observed that the accuracy changes with resolution and that the resolution at which the accuracy is highest differs depending on the material. Based on these results, we proposed a fabric material discrimination method using multi-resolution images by combining two CNNs. As a result of the evaluation experiment, the proposed method discriminated six kinds of fabric materials with 87.1% accuracy, and the accuracy was significantly higher than that of the comparison method without using multi-resolution images.",classification; convolutional neural network; fabric material; image resolution
"Object recognition is quite a well known task in computer vision. Objects are often associated with attributes. It becomes further challenging to correctly classify the object and associated attribute as a composition. Most of the methods for attribute-object pair detection involve discriminating approach that detects the attribute and object separately. Such approaches fail to consider some important facts regarding the composition viz. appearance of attributes is dependent on object and that of an object changes with the attribute. Making use of this interdepen-dence, we propose a model, ContribNet to learn attribute-object composition representation. The model uses the semantic linguistic features to learn robust visual composition while highlighting the importance of component features in identifying its counterpart of the composition. The factors responsible for model performance are also discussed.(c) 2023 Elsevier B.V. All rights reserved.",Compositional zero shot learning; Composite representation learning; Attribute detection; Computer vision
"Multimodal human behaviour recognition is a research hotspot in computer vision. To fully use both skeleton and depth data, this paper constructs a new multimodal network identification scheme combined with the self-attention mechanism. The system comprises a transformer-based skeleton self-attention subnetwork and a depth self-attention subnetwork based on CNN. In the skeleton self-attention subnetwork, this paper proposes a motion synergy space feature that can integrate the information of each joint point according to the entirety and synergy of human motion and puts forward a quantitative standard for the contribution degree of each joint motion. In this paper, the results from the skeleton self-attention subnetwork and the depth self-attention subnetwork are integrated and they are verified on the NTU RGB+D and UTD-MHAD datasets. The authors have achieved 90% recognition rate on UTD-MHAD dataset, and the CS recognition rate of the authors' method on the NTU RGB+D dataset reaches 90.5% and the recognition rate of CV is 94.7%. Experimental results show that the network structure proposed in this paper achieves a high recognition rate, and its performance is better than most current methods.",computer vision; image fusion
"Workers on construction sites face numerous health and safety risks. Authorities have made numerous attempts to enhance safety management; yet incidents continue to occur, impacting both worker health and the project's forward momentum. To that end, developing strategies to improve construction site safety management is crucial. The goal of this project is to employ computer vision and deep learning methods to create a model that can recognize construction workers, their PPE and the surrounding heavy equipment from CCTV footage. Then, the hazards can be discovered and identified based on an analysis of the imagery data and other criteria including weather conditions, and the on-site safety officer can be contacted. Our own dataset was used to train the You Only Look Once model, version 5 (YOLO-v5), which was put to use as an object detection model. The detection model's performance in tests showed promise for fast and accurate object recognition in the field.",object detection; PPE; heavy equipment; YOLO-5
"Error detection has a vital function in the production stages. Computer-aided error detection applications bring significant technological innovation to the production process to control the quality of products. As a result, the control of product quality has reached an essential point because of computer-aided image processing technologies. Artificial intelligence methods, such as Convolutional Neural Network (CNN), can detect and classify product errors. However, detecting acceptable and small defects on base parts cannot be done with a high rate of accuracy. At this point, it is possible to detect such minor errors with the help of the graph convolutional network, which has emerged as a new method. In this study, the defect elements on the surfaces of metal nut parts are determined through the graph convolutional network, and quality control is ensured. First, the surface images of the metal nut parts are captured. For this, a python-based Raspberry pi card and a modified camera system were installed. Adapters with three different zoom options are used on the camera system, depending on the part to be captured. The images obtained in the second step are sent to the other computer, which is used for image processing via the local server. In the third stage, image transformations are obtained by graphically separating the obtained images in white and black color tones on the second computer, and histogram maps of these images are drawn. Value ranges of these maps are determined and classified according to the value ranges obtained from the images of the defective parts. As a result, nine different models were analyzed. According to the analysis results, the graph convolutional neural network method gives 2.9554% better results than conventional methods.",metal; computer vision; neural network; graph; surface defect
"In computer vision and mobile robotics, autonomous navigation is crucial. It enables the robot to navigate its environment, which consists primarily of obstacles and moving objects. Robot navigation employing impediment detections, such as walls and pillars, is not only essential but also challenging due to real-world complications. This study provides a real-time solution to the problem of obtaining hallway scenes from an exclusive image. The authors predict a dense scene using a multi-scale fully convolutional network (FCN). The output is an image with pixel-by-pixel predictions that can be used for various navigation strategies. In addition, a method for comparing the computational cost and precision of various FCN architectures using VGG-16 is introduced. The binary semantic segmentation and optimal obstacle avoidance navigation of autonomous mobile robots are two areas in which our method outperforms the methods of competing works. The authors successfully apply perspective correction to the segmented image in order to construct the frontal view of the general area, which identifies the available moving area. The optimal obstacle avoidance strategy is comprised primarily of collision-free path planning, reasonable processing time, and smooth steering with low steering angle changes.",computer vision; fully convolutional networks; mobile robot; navigation; obstacle avoidance; semantic segmentation
"Multiple-object detection, localization, and tracking are desirable in many areas and applications, as the field of deep learning has developed and has drawn the attention of academics in computer vision, having a plethora of networks now achieving excellent accuracy in detecting multiple objects in an image. Tracking and localizing objects still remain difficult processes which require significant effort. This work describes an optical camera-based target detection, tracking, and localization solution for Unmanned Aerial Vehicles (UAVs). Based on the well-known network YOLOv4, a custom object detection model was developed and its performance was compared to YOLOv4-Tiny, YOLOv4-608, and YOLOv7-Tiny. The target tracking algorithm we use is based on Deep SORT, providing cutting-edge tracking. The proposed localization approach can accurately determine the position of ground targets identified by the custom object detection model. Moreover, an implementation of a global tracker using localization information from up to four UAV cameras at a time. Finally, a guiding approach is described, which is responsible for providing real-time movement commands for the UAV to follow and cover a designated target. The complete system was evaluated in Gazebo with up to four UAVs utilizing Software-In-The-Loop (SITL) simulation.",YOLO; multi-object detection; target tracking; target localization; UAV; computer vision; deep SORT; SORT
"Amidst the unprecedented demographic boom, coupled with climate change, more pressure is being exerted on road networks. Asset managers are thus in search for time and cost-effective state-of-the-art technologies for road inspection and condition monitoring. This paper provides an up-to-date comprehensive review of Computer Vision (CV) models and applications in pavement distress detection, classification, segmentation, quantification and condition assessment. To this end, the objectives of this review are: (1) review and bibliometric analysis of 190 related recent publications; (2) identification of trending tools, research gaps, emerging technologies, challenges and limitations of using CV for pavement distress and condition assessment; and (3) guiding of future research related to CV pavement asset management. While CV related models saw a sharp increase recently, increased collaboration between the academia and the industry is still needed to improve applicability levels of such models by pavement management agencies.",Computer vision; Condition assessment; Distress detection; Pavement management
"Personal identification using analysis of the internal and external characteristics of the human finger is currently an intensively developed topic. The work in this field concerns new methods of feature extraction and image analysis, mainly using modern artificial intelligence algorithms. However, the quality of the data and the way in which it is obtained determines equally the effectiveness of identification. In this article, we present a novel device for extracting vision data from the internal as well as external structures of the human finger. We use spatially selective backlight consisting of NIR diodes of three wavelengths. The fast image acquisition allows for insight into the pulse waveform. Thanks to the external illuminator, images of the skin folds of the finger are acquired as well. This rich collection of images is expected to significantly enhance identification capabilities using existing and future classic and AI-based computer vision techniques. Sample data from our device, before and after data processing, have been shared in a publicly available database.",biometry; finger vasculature; image processing
"Affected by shooting angle and light intensity, shooting through transparent media may cause light reflections in an image and influence picture quality, which has a negative effect on the research of computer vision tasks. In this paper, we propose a Residual Attention Based Reflection Removal Network (RABRRN) to tackle the issue of single image reflection removal. We hold that reflection removal is essentially an image separation problem sensitive to both spatial and channel features. Therefore, we integrate spatial attention and channel attention into the model to enhance spatial and channel feature representation. For a more feasible solution to solve the problem of gradient disappearance in the iterative training of deep neural networks, the attention module is combined with a residual network to design a residual attention module so that the performance of reflection removal can be ameliorated. In addition, we establish a reflection image dataset named the SCAU Reflection Image Dataset (SCAU-RID), providing sufficient real training data. The experimental results show that the proposed method achieves a PSNR of 23.787 dB and an SSIM value of 0.885 from four benchmark datasets. Compared with the other most advanced methods, our method has only 18.524M parameters, but it obtains the best results from test datasets.",artificial neural network; image processing; image restoration; computer vision; artificial intelligence; supervised learning; multi-layer neural network
"Monitoring damage in concrete structures is crucial for maintaining the health of structural systems. The implementation of computer vision has been the key for providing accurate and quantitative monitoring. Recent development uses the robustness of deep-learning-aided computer vision, especially the convolutional neural network model. The convolutional neural network is not only accurate but also flexible in various scenarios. The convolutional neural network has been constructed to classify image in terms of individual pixel, namely pixel-level detection, which is especially useful in detecting and classifying damage in fine-grained detail. Moreover, in the real-world scenario, the scenes are mostly very complex with varying foreign objects other than concrete. Therefore, this study will focus on implementing a pixel-level convolutional neural network for concrete surface damage detection with complicated surrounding image settings. Since there are multiple types of damage on concrete surfaces, the convolutional neural network model will be trained to detect three types of damages, namely cracks, spallings, and voids. The training architecture will adopt U-Net and DeepLabV3+. Both models are compared using the evaluation metrics and the predicted results. The dataset used for the neural network training is self-built and contains multiple concrete damages and complex foregrounds on every image. To deal with overfitting, the dataset is augmented, and the models are regularized using L1 and Spatial dropout. U-Net slightly outperforms DeepLabV3+ with U-Net scores 0.7199 and 0.5993 on F1 and mIoU, respectively, while DeepLabV3+ scores 0.6478 and 0.5174 on F1 and mIoU, respectively. Given the complexity of the dataset and extensive image labeling, the neural network models achieved satisfactory results.",convolutional neural network; damage detection; semantic segmentation; deep learning; computer vision
"The camera imaging built with high dynamic range (HDR) techniques can effectively improve the quality of images and so increase the recognition rate for computer vision systems. This paper presents the parallel architecture with a pipelined schedule to realize a real-time HDR processor based on a high-performance algorithm. With the hardware-oriented design, the processing kernel employs a near approach to reduce the computational circuit. The full HDR chip is realized with the module-by-module design and simulation. The main modules include the inverse module, the dark enhancement circuit, the parameter statistics circuit, the picture type judgment circuit, the mixing circuit and the bright enhancement circuit. Finally, these modules are combined with the pipelined schedule to realize a high-speed HDR core. In total, the latency time of the circuit is 4 line-buffer length added 14 clocks. This circuit is mapping to one FPGA (Field Programmable Gate Array) chip to verify its performance. The results demonstrate that the operation frequency can achieve to near 120 MHz, and data throughput rate is 360 M bytes per second. This chip can output one RGB (Red, Green, Blue) pixel per cycle, which can meet the requirement of high-resolution HDR camera performance.",HDR; Camera; Computer vision; Pipelined; FPGA; VLSI (Very Large-Scale Integration)
"Minimum cut/maximum flow (min-cut/max-flow) algorithms solve a variety of problems in computer vision and thus significant effort has been put into developing fast min-cut/max-flow algorithms. As a result, it is difficult to choose an ideal algorithm for a given problem. Furthermore, parallel algorithms have not been thoroughly compared. In this paper, we evaluate the state-of-the-art serial and parallel min-cut/max-flow algorithms on the largest set of computer vision problems yet. We focus on generic algorithms, i.e., for unstructured graphs, but also compare with the specialized GridCut implementation. When applicable, GridCut performs best. Otherwise, the two pseudoflow algorithms, Hochbaum pseudoflow and excesses incremental breadth first search, achieves the overall best performance. The most memory efficient implementation tested is the Boykov-Kolmogorov algorithm. Amongst generic parallel algorithms, we find the bottom-up merging approach by Liu and Sun to be best, but no method is dominant. Of the generic parallel methods, only the parallel preflow push-relabel algorithm is able to efficiently scale with many processors across problem sizes, and no generic parallel method consistently outperforms serial algorithms. Finally, we provide and evaluate strategies for algorithm selection to obtain good expected performance. We make our dataset and implementations publicly available for further research.",Computer vision; Parallel algorithms; Benchmark testing; Image segmentation; Sun; Partitioning algorithms; Heuristic algorithms; Algorithms; computer vision; graph algorithms; graph-theoretic methods; parallel algorithms; performance evaluation of algorithms and systems
"In recent years, deep learning (DL) has been widely studied using various methods across the globe, especially with respect to training methods and network structures, proving highly effective in a wide range of tasks and applications, including image, speech, and text recognition. One important aspect of this advancement is involved in the effort of designing and upgrading neural architectures, which has been consistently attempted thus far. However, designing such architectures requires the combined knowledge and know-how of experts from each relevant discipline and a series of trial-and-error steps. In this light, automated neural architecture search (NAS) methods are increasingly at the center of attention; this paper aimed at summarizing the basic concepts of NAS while providing an overview of recent studies on the applications of NAS. It is worth noting that most previous survey studies on NAS have been focused on perspectives of hardware or search strategies. To the best knowledge of the present authors, this study is the first to look at NAS from a computer vision perspective. In the present study, computer vision areas were categorized by task, and recent trends found in each study on NAS were analyzed in detail.",artificial intelligence (AI); deep learning (DL); convolutional neural network (CNN); automated machine learning (Auto-ML); neural architecture search (NAS); computer vision (CV)
"In the five years between 2017 and 2022, IP video traffic tripled, according to Cisco. User-Generated Content (UGC) is mainly responsible for user-generated IP video traffic. The development of widely accessible knowledge and affordable equipment makes it possible to produce UGCs of quality that is practically indistinguishable from professional content, although at the beginning of UGC creation, this content was frequently characterized by amateur acquisition conditions and unprofessional processing. In this research, we focus only on UGC content, whose quality is obviously different from that of professional content. For the purpose of this paper, we refer to in the wild as a closely related idea to the general idea of UGC, which is its particular case. Studies on UGC recognition are scarce. According to research in the literature, there are currently no real operational algorithms that distinguish UGC content from other content. In this study, we demonstrate that the XGBoost machine learning algorithm (Extreme Gradient Boosting) can be used to develop a novel objective in the wild video content recognition model. The final model is trained and tested using video sequence databases with professional content and in the wild content. We have achieved a 0.916 accuracy value for our model. Due to the comparatively high accuracy of the model operation, a free version of its implementation is made accessible to the research community. It is provided via an easy-to-use Python package installable with Pip Installs Packages (pip).",Quality of Experience (QoE); Quality of Service (QoS); metrics; evaluation; performance; Computer Vision (CV); Video Quality Indicators (VQI); Key Performance Indicators (KPI); User-Generated Content (UGC); in the wild content
"In recent years, bridge collapses and fractures have occurred in various countries mostly following a lack of inspection and maintenance. External inspection processes can be very time-consuming and pose labor safety hazards. Terrain obstacles may also prevent the thorough inspection of some structures. The use of artificial intelligence instead of visual inspection by bridge inspectors is state-of-the-art. This study develops a Bayesian-optimized deep learning model for use on an unmanned aerial vehicle (UAV) to identify the deterioration patterns and segment areas of composite decks under bridges by computer vision-based techniques. The proposed module alters traditional labor-intensive methods of visual bridge inspections, reduces labor safety hazards, and increases inspection accuracy. It can be embedded in an artificial intelligence chip, which is then installed in a consumer-grade UAV, making it a dedicated drone for the external inspection of composite bridges.",Bridge deck deterioration; Unmanned aerial vehicle; Computer vision-based deep learning; Pattern recognition; Instance segmentation; Bayesian-optimized Mask R-CNN
"A smart city is a city that binds together technology, society, and government to enable the existence of a smart economy, smart mobility, smart environment, smart living, smart people, and smart governance in order to reduce the environmental impact of cities and improve life quality. The first step to achieve a fully connected smart city is to start with smaller modules such as smart homes and smart buildings with energy management systems. Buildings are responsible for a third of the total energy consumption; moreover, heating, ventilation, and air conditioning (HVAC) systems account for more than half of the residential energy consumption in the United States. Even though connected thermostats are widely available, they are not used as intended since most people do not have the expertise to control this device to reduce energy consumption. It is commonly set according to their thermal comfort needs; therefore, unnecessary energy consumption is often caused by wasteful behaviors and the estimated energy saving is not reached. Most studies in the thermal comfort domain to date have relied on simple activity diaries to estimate metabolic rate and fixed values of clothing parameters for strategies to set the connected thermostat's setpoints because of the difficulty in tracking those variables. Therefore, this paper proposes a strategy to save energy by dynamically changing the setpoint of a connected thermostat by human activity recognition based on computer vision preserving the occupant's thermal comfort. With the use of a depth sensor in conjunction with an RGB (Red-Green-Blue) camera, a methodology is proposed to eliminate the most common challenges in computer vision: background clutter, partial occlusion, changes in scale, viewpoint, lighting, and appearance on human detection. Moreover, a Recurrent Neural Network (RNN) is implemented for human activity recognition (HAR) because of its data's sequential characteristics, in combination with physiological parameters identification to estimate a dynamic metabolic rate. Finally, a strategy for dynamic setpoints based on the metabolic rate, predicted mean vote (PMV) parameter and the air temperature is simulated using EnergyPlus (TM) to evaluate the energy consumption in comparison with the expected energy consumption with fixed value setpoints. This work contributes with a strategy to reduce energy consumption up to 15% in buildings with connected thermostats from the successful implementation of the proposed method.",energy savings; recurrent neural networks; metabolic rate; thermal comfort; depth sensor; computer vision
"Human action recognition systems use data collected from a wide range of sensors to accurately identify and interpret human actions. One of the most challenging issues for computer vision is the automatic and precise identification of human activities. A significant increase in feature learning-based representations for action recognition has emerged in recent years, due to the widespread use of deep learning-based features. This study presents an in-depth analysis of human activity recognition that investigates recent developments in computer vision. Augmented reality, human-computer interaction, cybersecurity, home monitoring, and surveillance cameras are all examples of computer vision applications that often go in conjunction with human action detection. We give a taxonomy-based, rigorous study of human activity recognition techniques, discussing the best ways to acquire human action features, derived using RGB and depth data, as well as the latest research on deep learning and hand-crafted techniques. We also explain a generic architecture to recognize human actions in the real world and its current prominent research topic. At long last, we are able to offer some study analysis concepts and proposals for academics. In-depth researchers of human action recognition will find this review an effective tool.",human action recognition; computer vision; deep learning; hand-crafted; taxonomy; survey
"The construction industry is on the path to digital transformation. One of the main challenges in this process is inspecting, assessing, and maintaining civil infrastructures and construction elements. However, Artificial Intelligence (AI) and Unmanned Aerial Vehicles (UAVs) can support the tedious and time-consuming work inspection processes. This article presents an innovative object detection-based system which enables the detection and geo-referencing of different traffic signs from RGB images captured by a drone's onboard camera, thus improving the realization of road element inventories in civil infrastructures. The computer vision component follows the typical methodology for a deep-learning-based SW: dataset creation, election and training of the most accurate object detection model, and testing. The result is the creation of a new dataset with a wider variety of traffic signs and an object detection-based system using Faster R-CNN to enable the detection and geo-location of traffic signs from drone-captured images. Despite some significant challenges, such as the lack of drone-captured images with labeled traffic signs and the imbalance in the number of images for traffic signal detection, the computer vision component allows for the accurate detection of traffic signs from UAV images.",UAVs; traffic signs; Faster R-CNN; object detection
"Annotation and analysis of sports videos is a time-consuming task that, once automated, will provide benefits to coaches, players, and spectators. American football, as the most watched sport in the United States, could especially benefit from this automation. Manual annotation and analysis of recorded videos of American football games is an inefficient and tedious process. Currently, most college football programs focus on annotating offensive formations to help them develop game plans for their upcoming games. As a first step to further research for this unique application, we use computer vision and deep learning to analyze an overhead image of a football play immediately before the play begins. This analysis consists of locating individual football players and labeling their position or roles, as well as identifying the formation of the offensive team. We obtain greater than 90% accuracy on both player detection and labeling, and 84.8% accuracy on formation identification. These results prove the feasibility of building a complete American football strategy analysis system using artificial intelligence. Collecting a larger dataset in real-world situations will enable further improvements. This would likewise enable American football teams to analyze game footage quickly.",computer vision; deep learning; machine learning; American football; formation analysis
"In robotics and computer vision communities, extensive studies have been widely conducted regarding surveillance tasks, including human detection, tracking, and motion recognition with a camera. Additionally, deep learning algorithms are widely utilized in the aforementioned tasks as in other computer vision tasks. Existing public datasets are insufficient to develop learning-based methods that handle various surveillance for outdoor and extreme situations such as harsh weather and low illuminance conditions. Therefore, we introduce a new large-scale outdoor surveillance dataset named eXtremely large-scale Multi-modAl Sensor dataset (X-MAS) containing more than 500,000 image pairs and the first-person view data annotated by well-trained annotators. Moreover, a single pair contains multi-modal data (e.g. an IR image, an RGB image, a thermal image, a depth image, and a LiDAR scan). This is the first large-scale first-person view outdoor multi-modal dataset focusing on surveillance tasks to the best of our knowledge. We present an overview of the proposed dataset with statistics and present methods of exploiting our dataset with deep learning-based algorithms.",Surveillance; Robots; Task analysis; Cameras; Videos; Multimodal sensors; Robot vision systems; Dataset; field robot; multi-modal perception; surveillance robot
"The number of people who suffer from diabetes in the world has been considerably increasing recently. It affects people of all ages. People who have had diabetes for a long time are affected by a condition called Diabetic Retinopathy (DR), which damages the eyes. Automatic detection using new technologies for early detection can help avoid complications such as the loss of vision. Currently, with the development of Artificial Intelligence (AI) techniques, especially Deep Learning (DL), DL-based methods are widely preferred for developing DR detection systems. For this purpose, this study surveyed the existing literature on diabetic retinopathy diagnoses from fundus images using deep learning and provides a brief description of the current DL techniques that are used by researchers in this field. After that, this study lists some of the commonly used datasets. This is followed by a performance comparison of these reviewed methods with respect to some commonly used metrics in computer vision tasks.",diabetic retinopathy grading; diabetic retinopathy detection; deep learning; convolutional neural network; retinal fundus images
"Polyurethane-based adhesives are applied on the windshields of vehicles in the automotive industry to fix the windshield and seal the cabin. A failure in the adhesive bead could allow water to ingress between the windshield and the vehicle body. If not detected in the leak test, it can lead to high cost due to warranty repairs, inconvenience to customers and damage to the brand. Commercial solutions are available in the market to detect an interruption in the adhesive bead right after its application on the windshield, before it is fitted to the vehicle, but at high cost. This paper proposes an automatic inspection system based on computer vision, low-cost hardware, programming in Python language and making use of open-source libraries. A batch of defect-free windshields was inspected using the proposed inspection system. In the impossibility of obtaining defective parts for validation, windshield images were modified to simulate defects and the images were evaluated by the developed algorithm. The algorithm showed quite good results at the end, and we could establish the system's effectiveness at 100% for defect detection capability and 21% of false detections.",Computer vision; Automatic bead inspection; Open-source software; Low-cost hardware
"Distance estimation is one of the oldest and most challenging tasks in computer vision using only a monocular camera. This can be challenging owing to the presence of occlusions, noise, and variations in the lighting, texture, and shape of objects. Additionally, the motion of the camera and objects in the scene can affect the accuracy of the distance estimation. Various techniques have been proposed to overcome these challenges, including stereo matching, structured light, depth from focus, depth from defocus, depth from motion, and time of flight. The addition of information from a high-resolution 3D view of the surroundings simplifies the distance calculation. This paper describes a novel distance estimation method that operates with converted point cloud data. The proposed method is a reliable map-based bird's eye view (BEV) that calculates the distance to the detected objects. Using the help of the Euler-region proposal network (E-RPN) model, a LiDAR-to-image-based method for metric distance estimation with 3D bounding box projections onto the image was proposed. We demonstrate that despite the general difficulty of the BEV representation in understanding features related to the height coordinate, it is possible to extract all parameters characterizing the bounding boxes of the objects, including their height and elevation. Finally, we applied the triangulation method to calculate the accurate distance to the objects and statistically proved that our methodology is one of the best in terms of accuracy and robustness.",3D object detection; deep neural networks; sensor fusion; computer vision
"More sustainable technologies in agriculture are important not only for increasing crop yields, but also for reducing the use of agrochemicals and improving energy efficiency. Recent advances rely on computer vision systems that differentiate between crops, weeds, and soil. However, manual dataset capture and annotation is labor-intensive, expensive, and time-consuming. Agricultural robots provide many benefits in effectively performing repetitive tasks faster and more accurately than humans, and despite the many advantages of using robots in agriculture, the solutions are still often expensive. In this work, we designed and built a low-cost autonomous robot (DARob) in order to facilitate image acquisition in agricultural fields. The total cost to build the robot was estimated to be around $850. A low-cost robot to capture datasets in agriculture offers advantages such as affordability, efficiency, accuracy, security, and access to remote areas. Furthermore, we created a new dataset for the segmentation of plants and weeds in bean crops. In total, 228 RGB images with a resolution of 704 x 480 pixels were annotated containing 75.10% soil area, 17.30% crop area and 7.58% weed area. The benchmark results were provided by training the dataset using four different deep learning segmentation models.",autonomous robots; agriculture; data acquisition; computer vision
"Automatically translating chromaticity-free thermal infrared (TIR) images into realistic color visible (CV) images is of great significance for autonomous vehicles, emergency rescue, robot navigation, nighttime video surveillance, and many other fields. Most recent designs use end-to-end neural networks to translate TIR directly to CV; however, compared to these networks, TIR has low contrast and an unclear texture for CV translation. Thus, directly translating the TIR temperature value of only one channel to the RGB color value of three channels without adding additional constraints or semantic information does not handle the one-to-three mapping problem between different domains in a good way, causing the translated CV images not only to have blurred edges but also color confusion. As for the methodology of the work, considering that in the translation from TIR to CV the most important process is to map information from the temperature domain into the color domain, an improved CycleGAN (GMA-CycleGAN) is proposed in this work in order to translate TIR images to grayscale visible (GV) images. Although the two domains have different properties, the numerical mapping is one-to-one, which reduces the color confusion caused by one-to-three mapping when translating TIR to CV. Then, a GV-CV translation network is applied to obtain CV images. Since the process of decomposing GV images into CV images is carried out in the same domain, edge blurring can be avoided. To enhance the boundary gradient between the object (pedestrian and vehicle) and the background, a mask attention module based on the TIR temperature mask and the CV semantic mask is designed without increasing the network parameters, and it is added to the feature encoding and decoding convolution layers of the CycleGAN generator. Moreover, a perceptual loss term is applied to the original CycleGAN loss function to bring the translated images closer to the real images regarding the space feature. In order to verify the effectiveness of the proposed method, the FLIR dataset is used for experiments, and the obtained results show that, compared to the state-of-the-art model, the subjective quality of the translated CV images obtained by the proposed method is better, as the objective evaluation metric FID (Frechet inception distance) is reduced by 2.42 and the PSNR (peak signal-to-noise ratio) is improved by 1.43.",thermal infrared image; image translation; CycleGAN; temperature information; semantic mask
"Image inpainting is an important research direction in the study of computer vision, and is widely used in image editing and photo inpainting etc. Traditional image inpainting algorithms are often difficult to deal with large-scale image deletion, since these algorithms are prone to inconsistent image semantics. With the rapid development of deep learning (DL) in recent years, the advantages of DL in image processing have become increasingly prominent, it can solve the problems existing in traditional image inpainting algorithms to a certain extent. At present, image inpainting based on deep learning becomes a research hotspot in computer vision. In this article, we systematically summarize and analyze the literature on image inpainting based on deep learning. First, we review the specific research status of deep learning technology in the field of image inpainting in the past 15 years; then, We deeply study and analyze the existing image restoration methods based on different neural network structures and their information fusion methods. In addition, we also classify and summarize the different tasks of image inpainting according to the application scenarios of image inpainting. Finally, we point out some problems that urgently need to be solved for deep learning in the field of image inpainting, provide constructive suggestions and discuss the future development direction.",Image inpainting; Fusion; Deep learning; CNN; GAN
"Traditional human-computer interaction technology relies heavily on input devices such as mice and keyboards, which limit the speed and naturalness of interaction and can no longer meet the more advanced interaction needs of users. With the development of computer vision (CV) technology, research on contactless gesture recognition has become a new research hotspot. However, current CV-based gesture recognition technology has the limitation of a limited number of gesture recognition and cannot achieve fast and accurate text input operations. To solve this problem, this paper proposes an over-the-air handwritten character recognition system based on the coordinate correction YOLOv5 algorithm and a lightweight convolutional neural network (LGR-CNN), referred to as Air-GR. Unlike the direct recognition of captured gesture pictures, the system uses the trajectory points of gesture actions to generate images for gesture recognition. Firstly, by combining YOLOv5 with the gesture coordinate correction algorithm proposed in this paper, the system can effectively improve gesture detection accuracy. Secondly, considering that the captured gesture coordinates may contain multiple gestures, this paper proposes a time-window-based algorithm for segmenting the gesture coordinates. Finally, the system recognizes user gestures by plotting the segmented gesture coordinates in a two-dimensional coordinate system and feeding them into the constructed lightweight convolutional neural network, LGR-CNN. For the gesture trajectory image classification task, the accuracy of LGR-CNN is 13.2%, 12.2%, and 4.5% higher than that of the mainstream networks VGG16, ResNet, and GoogLeNet, respectively. The experimental results show that Air-GR can quickly and effectively recognize any combination of 26 English letters and numbers, and its recognition accuracy reaches 95.24%.",Air-GR; YOLOv5; LGR-CNN; gesture coordinate correction algorithm; time window algorithm; gesture recognition
"Point clouds provide a flexible geometric representation for computer vision research. However, the harsh demands for the number of input points and computer hardware are still significant challenges, which hinder their deployment in real applications. To address these challenges, we design a simple and effective module named cyclic self-attention module (CSAM). Specifically, three attention maps of the same input are obtained by cyclically pairing the feature maps, thus exploring the features sufficiently of the attention space of the original input. CSAM can adequately explore the correlation between points to obtain sufficient feature information despite the multiplicative decrease in inputs. Meanwhile, it can direct the computational power to the more essential features, relieving the burden on the computer hardware. We build a point cloud classification network by simply stacking CSAM called cyclic self-attention network (CSAN). We also propose a novel framework for point cloud semantic segmentation called full cyclic self-attention network (FCSAN). By adaptively fusing the original mapping features and the CSAM extracted features, it can better capture the context information of point clouds. Extensive experiments on several benchmark datasets show that our methods can achieve competitive performance in classification and segmentation tasks.",Point cloud; self-attention; cyclic pairing; adaptive fuse
"Large-volume hydraulic concrete structures, such as concrete dams, often suffer from damage due to the influence of alternating loads and material aging during the service process. The occurrence and further expansion of cracks will affect the integrity, impermeability, and durability of the dam concrete. Therefore, monitoring the changing status of cracks in hydraulic concrete structures is very important for the health service of hydraulic engineering. This study combines computer vision and artificial intelligence methods to propose an automatic damage detection and diagnosis method for hydraulic structures. Specifically, to improve the crack feature extraction effect, the Xception backbone network, which has fewer parameters than the ResNet backbone network, is adopted. With the aim of addressing the problem of premature loss of image detail information and small target information of tiny cracks in hydraulic concrete structures, an adaptive attention mechanism image semantic segmentation algorithm based on Deeplab V3+ network architecture is proposed. Crack images collected from concrete structures of different types of hydraulic structures were used to develop crack datasets. The experimental results show that the proposed method can realize high-precision crack identification, and the identification results have been obtained in the test set, achieving 90.537% Intersection over Union (IOU), 91.227% Precision, 91.301% Recall, and 91.264% F1_score. In addition, the proposed method has been verified on different types of cracks in actual hydraulic concrete structures, further illustrating the effectiveness of the method.",structural damage detection; computer vision; concrete structures; crack detection; feature extraction
"A long-standing challenge in pneumonia diagnosis is recognizing the pathological lung texture, especially the ground-glass appearance pathological texture. One main difficulty lies in precisely extracting and recognizing the pathological features. The patients, especially those with mild symptoms, show very little difference in lung texture, neither conventional computer vision methods nor convolutional neural networks perform well on pneumonia diagnosis based on chest X-ray (CXR) images. In the meanwhile, the Coronavirus Disease 2019 (COVID-19) pandemic continues wreaking havoc around the world, where quick and accurate diagnosis backed by CXR images is in high demand. Rather than simply recognizing the patterns, extracting feature maps from the original CXR image is what we need in the classification process. Thus, we propose a Vision Transformer (VIT)-based model called PneuNet to make an accurate diagnosis backed by channel-based attention through X-ray images of the lung, where multi-head attention is applied on channel patches rather than feature patches. The techniques presented in this paper are oriented toward the medical application of deep neural networks and VIT. Extensive experiment results show that our method can reach 94.96% accuracy in the three-categories classification problem on the test set, which outperforms previous deep learning models.",Deep learning; Pneumonia diagnosis; COVID-19; Vision Transformer; Multi-head attention
"Synthetic datasets, for which we propose the term synthsets, are not a novelty but have become a necessity. Although they have been used in computer vision since 1989, helping to solve the problem of collecting a sufficient amount of annotated data for supervised machine learning, intensive development of methods and techniques for their generation belongs to the last decade. Nowadays, the question shifts from whether you should use synthetic datasets to how you should optimally create them. Motivated by the idea of discovering best practices for building synthetic datasets to represent dynamic environments (such as traffic, crowds, and sports), this study provides an overview of existing synthsets in the computer vision domain. We have analyzed the methods and techniques of synthetic datasets generation: from the first low-res generators to the latest generative adversarial training methods, and from the simple techniques for improving realism by adding global noise to those meant for solving domain and distribution gaps. The analysis extracts nine unique but potentially intertwined methods and reveals the synthsets generation diagram, consisting of 17 individual processes that synthset creators should follow and choose from, depending on the specific requirements of their task.",Computer vision; Synthetic dataset; Synthset; Generation methods
"Image-based 3D reconstruction is a long-established, ill-posed problem defined within the scope of computer vision and graphics. The purpose of image-based 3D reconstruction is to retrieve the 3D structure and geometry of a target object or scene from a set of input images. This task has a wide range of applications in various fields, such as robotics, virtual reality, and medical imaging. In recent years, learning-based methods for 3D reconstruction have attracted many researchers worldwide. These novel methods can implicitly estimate the 3D shape of an object or a scene in an end-to-end manner, eliminating the need for developing multiple stages such as key-point detection and matching. Furthermore, these novel methods can reconstruct the shapes of objects from a single input image. Due to rapid advancements in this field, as well as the multitude of opportunities to improve the performance of 3D reconstruction methods, a thorough review of algorithms in this area seems necessary. As a result, this research provides a complete overview of recent developments in the field of image-based 3D reconstruction. The studied methods are examined from several viewpoints, such as input types, model structures, output representations, and training strategies. A detailed comparison is also provided for the reader. Finally, unresolved challenges, underlying issues, and possible future work are discussed.",3D Object reconstruction; 3D Shape representation; Deep learning; Computer vision
"Histopathology is a critical approach for diagnostic tasks and precision treatment. However, histopathological deep learning tools for auto-identification remain poorly developed. Meanwhile, the interpretation of the computer vision attention into a cellular process is less efficient in a systematic way. Herein, it is identified that histone acetyltransferase 1 (HAT1) is an aging-associated gene in the esophagus epithelium by machine learning. An interpretable deep learning model is developed to distinguish morphological changes with varied HAT1 expressions in esophageal squamous carcinoma cells (ESCC). The gradient-weighted class activation mapping and prediction score analysis reveal that the computer's vision focuses on the nuclear sizes of ESCC. The hypothesized phenotype is verified in HAT1-knockdown ESCCs. Finally, HAT1 regulating cell senescence by affecting the H3K27 acetylation and E2F transcription factor 7 (E2F7) expression is shown. Herein, the feasibility and benefits of applying histopathological deep learning assistance systems in routine practice scenarios and connecting phenotype and genotype for further genetic research are suggested.",aging; deep learning; esophageal cancer; histone acetyltransferase 1; histopathology
"With the rapid development of food production and health management, analyses of food samples have been essential for preventing diseases and understanding human culture. Recently, food analyses have become increasingly complex and are not limited in food categorization. They also contain many advanced tasks (e.g., nutrition estimation and recipe retrieval). From existing works, two points can be concluded. First, food features are much more comprehensive and sophisticated than general samples. Second, for food analyses, multiple learning strategies (MLSs) usually achieve outperformance over general deep learning methods. However, there are few survey papers reporting food analyses with MLSs, and the main factors lead to difficulty of operation. Therefore, we intend to conduct a survey for applications of MLSs to food analyses. In this survey paper, three types of common MLSs, which are multi-task learning (MTL), multi-view learning (MVL) and multi-scale learning (MSL) strategies, are presented in terms of their guidance, typical works, algorithms and final aggregation methods. Additionally, food characteristics are proposed to be closely related to the difficulty of food analyses. We comprehensively conclude food characteristics as nonrigid, complex in arrangement, and large (small) in intraclass (interclass) variance. Moreover, some experimental results of MLSs are also presented and analyzed in this paper. Based on these results, insightful suggestions for MLSs implementation are proposed. Finally, the promising tendency of MLSs applications in the future is discussed.",Computer vision; Food analyses; Multi-learning; Multi-task learning (MTL); Multi-view learning (MVL); Multi-scale learning (MSL)
"Tea leaf diseases seriously affect the yield and quality of tea. Early warning and severity estimation of the dis-eases can be used to guide tea farmers to spray pesticide reasonably. Tea leaves infected with leaf blight are usually damaged, deformed, and occluded. An insufficient number of disease image samples will lead to over -fitting of the estimated model. Thus, existing methods based on machine learning can only estimate the severity of tea diseases in natural scene images with low accuracy. Aiming to solve these problems, this study proposes a computer vision based method for the severity estimation of tea leaf blight in RGB images obtained under natural scenes. In this method, the influence of complex backgrounds is reduced by segmenting diseased tea leaves and spots, the problems of partial occlusion, deformation and damage of diseased leaves are solved by area fitting, and the severity of tea leaf blight is accurately estimated by the gradient boosting machine. Compared with classical machine learning methods and conventional convolution neural network methods, the method pre-sented in this study only needs a small number of manually labeled samples and has better accuracy and robustness for the severity estimation of tea leaf blight in natural scene images.",Computer vision; Disease severity estimation; Support vector machine; B -spline restoration; Gradient boosting machine
"White blood cell (WBC) detection in microscopic images is indispensable in medical diagnostics; however, this work, based on manual checking, is time-consuming, labor-intensive, and easily results in errors. Using object detectors for WBCs with deep convolutional neural networks can be regarded as a feasible solution. In this paper, to improve the examination precision and efficiency, a one-stage and lightweight CNN detector with an attention mechanism for detecting microscopic WBC images, and a white blood cell detection vision system are proposed. The method integrates different optimizing strategies to strengthen the feature extraction capability through the combination of an improved residual convolution module, hybrid spatial pyramid pooling module, improved coordinate attention mechanism, efficient intersection over union (EIOU) loss and Mish activation function. Extensive ablation and contrast experiments on the latest public Raabin-WBC dataset verify the effectiveness and robustness of the proposed detector for achieving a better overall detection performance. It is also more efficient than other existing studies for blood cell detection on two additional classic public BCCD and LISC datasets. The novel detection approach is significant and flexible for medical technicians to use for blood cell microscopic examination in clinical practice.",Computer vision; Object detection; White blood cell detection; YOLO; Detection vision system
"Efficient management of water resources is an important task given the significance of water in daily lives and economic growth. Water resource management is a specific field of study which deals with the efficient management of water resources towards fulfilling the needs of society and preventing from water-related disasters. Many activities within this domain are getting benefitted with the recent technological advancements. Within many others, computer vision-based solutions have emerged as disruptive technologies to address complex real-world problems within the water resource management domain (e.g., flood detection and mapping, satellite-based water bodies monitoring, monitoring and inspection of hydraulic structures, blockage detection and assessment, drainage inspection and sewer monitoring). However, there are still many aspects within the water resource management domain which can be explored using computer vision technologies. Therefore, it is important to investigate the trends in current research related to these technologies to inform the new researchers in this domain. In this context, this paper presents the bibliometric analysis of the literature from the last two decades where computer vision technologies have been used for addressing problems within the water resource management domain. The analysis is presented in two categories: (a) performance analysis demonstrating highlighted trends in the number of publications, number of citations, top contributing countries, top publishing journals, top contributing institutions and top publishers and (b) science mapping to demonstrate the relation between the bibliographic records based on the co-occurrence of keywords, co-authorship analysis, co-citation analysis and bibliographic coupling analysis. Bibliographic records (i.e., 1059) are exported from the Web of Science (WoS) core collection database using a comprehensive query of keywords. VOSviewer opensource tool is used to generate the network and overlay maps for the science mapping of bibliographic records. Results highlighted important trends and valuable insights related to the use of computer vision technologies in water resource management. An increasing trend in the number of publications and focus on deep learning/artificial intelligence (AI)-based approaches has been reported from the analysis. Further, flood mapping, crack/fracture detection, coastal flood detection, blockage detection and drainage inspections are highlighted as active areas of research.",artificial intelligence; bibliometric analysis; computer vision; deep learning; remote sensing; water resources
"Object detection and recognition have become integral components across various applications. Detecting desired objects of interest and analysing the same is used across several sophisticated applications like video surveillance, anomaly detectors, vehicle detection and tracking, person identification, etc. The same object recognition technique can be extended to analyse the images of medicinal leaves used in Siddha medicine and classify whether the right herbal leaf is picked for preparing medicine or therapy. This work focuses on developing a model that can detect and distinguish the right medicinal leaf from a look alike ordinary leaf using computer vision and machine learning. A leaf dataset was created that comprises of medicinal leaf and its look alike ordinary leaf. Computer vision techniques were used to extract features and pre-process the leaf images and the model uses Deep Convolution Neural Network to classify the right medicinal leaf from other look alike leaves. The proposed work has been tested with the dataset created and the results are shared.",Medicine leaf detection; Leaf detection; Edge detection; Convolution neural network; Leaf vein pattern; Siddha leaves
"Border tracking in binary images is an important operation in many computer vision applications. The problem consists in finding borders in a 2D binary image (where all of the pixels are either 0 or 1). There are several algorithms available for this problem, but most of them are sequential. In a former paper, a parallel border tracking algorithm was proposed. This algorithm was designed to run in Graphics Processing units, and it was based on the sequential algorithm known as the Suzuki algorithm. In this paper, we adapt the previously proposed GPU algorithm so that it can be executed in multicore computers. The resulting algorithm is evaluated against its GPU counterpart. The results show that the performance of the GPU algorithm worsens (or even fails) for very large images or images with many borders. On the other hand, the proposed multicore algorithm can efficiently cope with large images.",Border tracking; Computer vision; Parallel computing; GPU computing; OpenMP; Multicore computing
"Although structural damage recognition has been extensively investigated using deep learning and computer vision (CV) techniques, the following limitations exist for real-world applications: (1) the accuracy heavily relies on a large volume of network parameters; (2) the sensitivity to tiny cracks is limited due to low contrast between microcrack and background pixels; (3) the robustness on complex cracks with various morphological features and surface disturbances is inadequate. To address these issues, this study proposes a lightweight, accurate, and robust semantic segmentation method of complex structural damage recognition for actual bridges. Firstly, a modified DeepLabv3+ model is established using the lightweight MobileNetV2 backbone and transposed convolutions to reduce parameter volume and enhance the recognition capability of local minor damages. Secondly, the depthwise separable convolution is utilized instead of the standard convolution to decouple the spatial and channel interactions of feature maps. Thirdly, a refined atrous spatial pyramid pooling (ASPP) module is constructed at the backbone end using multilevel dilated convolutions to expand the receptive fields. Finally, a piecewise synthetical loss function based on focal and dice losses is designed for different training stages. A total of 3226 actual crack images in different scales, resolutions, and scenes are utilized to verify the proposed method. The results show that the mean intersection-over-union for complex cracks in various real-world scenarios reaches 0.776 with significant reductions of 91.5% in parameter volume and 38.9% in recognition time. Comparative studies demonstrate the superiority of the proposed method over existing lightweight crack segmentation models based on SegNet and DenseNet. In addition, ablation experiments demonstrate the necessity and effectiveness of the MobileNetV2 backbone, refined ASPP module, and piecewise synthetical loss function. Moreover, the robustness and expandability of the proposed method on new structural damage categories (including concrete spalling, rebar exposure, and cable corrosion) are also verified.",Structural damage segmentation; computer vision; lightweight model; modified DeepLabv3+; complex images from actual bridges
"Referring expression grounding is an important and challenging task in computer vision. To avoid the laborious annotation in conventional referring grounding, unpaired referring grounding is introduced, where the training data only contains a number of images and queries without correspondences. The few existing solutions to unpaired referring grounding are still preliminary, due to the challenges of learning vision-language correlation and lack of the top-down guidance with unpaired data. Existing works are only able to learn vision-language correlation by modality conversion, where critical informa-tion are lost. They also heavily rely on pre-extracted object proposals and thus cannot generate correct predictions with defective proposals.In this paper, we propose a novel bidirectional cross-modal matching (BiCM) framework to address these challenges. Particularly, we design a query-aware attention map (QAM) module that introduces top-down perspective via generating query-specific visual attention maps to avoid the over-reliance on pre-extracted object proposals. A cross-modal object matching (COM) module is further introduced to predict the target objects from a bottom-up perspective. This module exploits the recently emerged image-text matching pretrained model, CLIP, to learn cross-modal correlation without modality conver-sion. The top-down and bottom-up predictions are then integrated via a similarity fusion (SF) module. We also propose a knowledge adaptation matching (KAM) module that leverages unpaired training data to adapt pretrained knowledge to the target dataset and task. Experiments show that our framework sig-nificantly outperforms previous works on five grounding datasets.(c) 2022 Elsevier B.V. All rights reserved.",Referring grounding; Vision and language; Top -down and bottom -up model
"Because the steel structure trestle has been in service under heavy load for a long time, the steel structure trestle is prone to cracks around the welds or bolt holes, which can lead to structural collapse in severe cases. Aiming at the characteristics of stable and high-quality images obtained by the unmanned consumer-grade camera monitoring system, this paper proposed structure health monitoring (SHM) system which is based on consumer-grade camera. The SHM system can identify crack damage and locate steadily in long term, which provides the technical support of practical application in intelligent SHM system. The method first performed edge detection on the trestle structure, followed by pixel-level semantic segmentation and crack localization. Canny edge detection algorithm was used to identify trestle structures in the camera image. The panorama trestle structure was divided into areas of suitable size, and the camera focused on each divided area one by one. Then the improved DeepLab V3+ model was trained by constructing global and local datasets. Then the improved DeepLab V3+ model was used to perform pixel-level semantic segmentation on the trestle images of the divided regions. Finally, based on the Speeded Up Robust Features and combined with the image, a panorama crack location output method was proposed. The system was used to test a section of a trestle in a coal mining industrial park, and the system showed that the method could efficiently and accurately identify and locate the crack damage.",Structural damage identification; Semantic segmentation; Crack localization; Computer vision; Steel structure trestle
"Fatigue cracks that develop in civil infrastructure such as steel bridges due to repetitive loads pose a major threat to structural integrity. Despite being the most common practice for fatigue crack detection, human visual inspection is known to be labor intensive, time-consuming, and prone to error. In this study, a computer vision-based fatigue crack detection approach using a short video recorded under live loads by a moving consumer-grade camera is presented. The method detects fatigue crack by tracking surface motion and identifies the differential motion pattern caused by opening and closing of the fatigue crack. However, the global motion introduced by a moving camera in the recorded video is typically far greater than the actual motion associated with fatigue crack opening/closing, leading to false detection results. To overcome the challenge, global motion compensation (GMC) techniques are introduced to compensate for camera-induced movement. In particular, hierarchical model-based motion estimation is adopted for 2D videos with simple geometry and a new method is developed by extending the bundled camera paths approach for 3D videos with complex geometry. The proposed methodology is validated using two laboratory test setups for both in-plane and out-of-plane fatigue cracks. The results confirm the importance of motion compensation for both 2D and 3D videos and demonstrate the effectiveness of the proposed GMC methods as well as the subsequent crack detection algorithm.",global motion compensation; fatigue crack detection; computer vision; parallax effect; distortion induced fatigue crack; video stabilization; camera motion; in-plane fatigue crack; out-of-plane fatigue crackanalysis
"Recent advances in computer vision and deep learning have shown that the fusion of depth information can significantly enhance the performance of RGB-based damage detection and segmentation models. However, alongside the advantages, depth-sensing also presents many practical challenges. For instance, the depth sensors impose an additional payload burden on the robotic inspection platforms limiting the operation time and increasing the inspection cost. Additionally, some lidar-based depth sensors have poor outdoor performance due to sunlight contamination during the daytime. In this context, this study investigates the feasibility of abolishing depth-sensing at test time without compromising the segmentation performance. An autonomous damage segmentation framework is developed, based on recent advancements in vision-based multi-modal sensing such as modality hallucination (MH) and monocular depth estimation (MDE), which require depth data only during the model training. At the time of deployment, depth data becomes expendable as it can be simulated from the corresponding RGB frames. This makes it possible to reap the benefits of depth fusion without any depth perception per se. This study explored two different depth encoding techniques and three different fusion strategies in addition to a baseline RGB-based model. The proposed approach is validated on computer-generated RGB-D data of reinforced concrete buildings subjected to seismic damage. It was observed that the surrogate techniques can increase the segmentation IoU by up to 20.1% with a negligible increase in the computation cost. Overall, this study is believed to make a positive contribution to enhancing the resilience of critical civil infrastructure.",multimodal data fusion; depth sensing; vision-based inspection; UAV-assisted inspection; damage segmentation; post-disaster reconnaissance; modality hallucination; monocular depth estimation
"Although deep learning has achieved satisfactory performance in computer vision, a large volume of im-ages is required. However, collecting images is often expensive and challenging. Many image augmenta-tion algorithms have been proposed to alleviate this issue. Understanding existing algorithms is, therefore, essential for finding suitable and developing novel methods for a given task. In this study, we perform a comprehensive survey of image augmentation for deep learning using a novel informative taxonomy. To examine the basic objective of image augmentation, we introduce challenges in computer vision tasks and vicinity distribution. The algorithms are then classified among three categories: model-free, model-based, and optimizing policy-based. The model-free category employs the methods from image process-ing, whereas the model-based approach leverages image generation models to synthesize images. In con-trast, the optimizing policy-based approach aims to find an optimal combination of operations. Based on this analysis, we believe that our survey enhances the understanding necessary for choosing suitable methods and designing novel algorithms.(c) 2023 The Author(s). Published by Elsevier Ltd. This is an open access article under the CC BY-NC-ND license ( http://creativecommons.org/licenses/by-nc-nd/4.0/ )",Image augmentation; Deep learning; Image variation; Vicinity distribution; Data augmentation; Computer vision
"Recent studies for computer vision and deep learning-based, post-earthquake inspections on RC structures mainly perform well for specific tasks, while the trained models must be fine-tuned and re-trained when facing new tasks and datasets, which is inevitably time-consuming. This study proposes a multi-task learning approach that simultaneously accomplishes the semantic segmentation of seven-type structural components, three-type seismic damage, and four-type deterioration states. The proposed method contains a CNN-based encoder-decoder backbone subnetwork with skip-connection modules and a multi-head, task-specific recognition subnetwork. The backbone subnetwork is designed to extract multi-level features of post-earthquake RC structures. The multi-head, task-specific recognition subnetwork consists of three individual self-attention pipelines, each of which utilizes extracted multi-level features from the backbone network as a mutual guidance for the individual segmentation task. A synthetical loss function is designed with real-time adaptive coefficients to balance multi-task losses and focus on the most unstably fluctuating one. Ablation experiments and comparative studies are further conducted to demonstrate their effectiveness and necessity. The results show that the proposed method can simultaneously recognize different structural components, seismic damage, and deterioration states, and that the overall performance of the three-task learning models gains general improvement when compared to all single-task and dual-task models.",post-earthquake evaluation; multi-task learning; computer vision; structural component segmentation; seismic damage recognition; deterioration state assessment
"Crowd analysis from drones has attracted increasing attention in recent times due to the ease of use and affordable cost of these devices. However, how this technology can provide a solution to crowd flow detection is still an unexplored research question. To this end, we propose a crowd flow detection method for video sequences shot by a drone. The method is based on a fully-convolutional network that learns to perform crowd clustering in order to detect the centroids of crowd-dense areas and track their movement in consecutive frames. The proposed method proved effective and efficient when tested on the Crowd Counting datasets of the VisDrone challenge, characterized by video sequences rather than still images. The encouraging results show that the proposed method could open up new ways of analyzing high-level crowd behavior from drones.",Drone vision; Computer vision; Deep learning; Crowd flow detection; Crowd density estimation; Clustering
"Smart agriculture is the application of modern information and communication technologies (ICT) to agriculture, leading to what we might call a third green revolution. These include object detection and classification such as plants, leaves, weeds, fruits as well as animals and pests in the agricultural domain. Object detection, one of the most fundamental and difficult issues in computer vision has attracted a lot of attention lately. Its evolution over the previous two decades can be seen as the pinnacle of computer vision advancement. The detection of objects can be done via digital image processing. Machine learning has achieved significant advances in the field of digital image processing in current years, significantly outperforming previous techniques. One of the techniques that is popular is Few-Shot Learning (FSL). FSL is a type of meta-learning in which a learner is given practice on several related tasks during the meta-training phase to be able to generalize successfully to new but related activities with a limited number of instances during the meta-testing phase. Here, the application of FSL in smart agriculture, with particular in the detection and classification is reported. The aim is to review the state of the art of currently available FSL models, networks, classifications, and offer some insights into possible future avenues of research. It is found that FSL shows a higher accuracy of 99.48% in vegetable disease recognition on a limited dataset. It is also shown that FSL is reliable to use with very few instances and less training time.",smart agriculture; object detection; computer vision; digital image processing; machine learning; few-shot learning
"In this work we propose a new non-monotonic activation function: the modulus. The majority of the reported research on nonlinearities is focused on monotonic functions. We empirically demonstrate how by using the modulus activation function on computer vision tasks the models generalize better than with other nonlinearities - up to a 15% accuracy increase in CIFAR100 and 4% in CIFAR10, relative to the best of the benchmark activations tested. With the proposed activation function the vanishing gradient and dying neurons problems disappear, because the derivative of the activation function is always 1 or -1. The simplicity of the proposed function and its derivative make this solution specially suitable for TinyML and hardware applications.",Deep learning; Activation functions; Optimization
"Post-earthquake inspection of structures based on computer vision is developing rapidly due to the advantages of high efficiency and without manual feature extraction. However, it is still necessary to investigate how to accurately recognize structural components and damage from the perspective of pixels. Fortunately, refinement network which named RefineNet has been developed for semantic segmentation of images, which helps to combine low-level features and high-level semantics to generate high resolution segmented images for efficient end-to-end learning. Therefore, RefineNet is used in this study as a network architecture for semantic segmen-tation tasks of recognizing railway viaducts components and damages. Moreover, it is proposed to embed the convolutional block attention mechanism in the down-sampling process of the RefineNet to extract image fea-tures, which helps the network to assign different weights to image regions of different importance and effec-tively improve the extraction effect of intermediate features. With the provided large-scale synthetic railway viaduct image dataset, which named Tokaido Dataset, the proposed RefineNet with Attention Mechanism (RefineNet-AM) is used for structural condition assessment of railway viaduct, including semantic segmentation tasks of components and damages of railway viaduct. Based on the test dataset, it is shown that proposed RefineNet-AM can inspect the structural components and damage of railway viaduct with satisfactory accuracy.",Structural health monitoring; Deep learning; Computer vision; Semantic segmentation; Bridge component recognition; Bridge damage recognition
"In recent years, with the development of deep learning technology, computer vision and natural lan-guage processing have made significant progress, and establishing the relationship between computer vision and natural language processing has attracted more and more attention. The spatio-temporal images taken by satellites or aircrafts and scene images with people and other things are the main focus area. Existing methods have yielded excellent results in image-text matching, but there is still room for improvement in effectively using coarse and fine-grained information. We propose a method to solve this problem using multi-scale graph convolutional neural networks. We extracted the multi-scale features of images and texts for matching separately. Global and local matching are used to calculate the overall image sentence and local image-word similarity. Local matching is divided into two stages, first, the node level matches the correspondence between the learning region and the word. Next, the structure level matches the correspondence between the learning region and the phrase to make the matching more comprehensive. Finally, we verified our model on Flickr30k, MSCOCO and RSICD datasets.(c) 2023 The Author(s). Published by Elsevier B.V. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).",Multimodal matching; Multi-scale; GNN; Feature extraction
"Vision Transformer (ViT) models have achieved good results in computer vision tasks, their performance has been shown to exceed that of convolutional neural networks (CNNs). However, the robustness of the ViT model has been less studied recently. To address this problem, we investigate the robustness of the ViT model in the face of adversarial attacks, and enhance the robustness of the model by introducing the ResNet-SE module, which acts on the Attention module of the ViT model. The Attention module not only learns edge and line information, but also can extract increasingly complex feature information; ResNet-SE module highlights the important information of each feature map and suppresses the minor information, which helps the model to perform the extraction of key features. The experimental results show that the accuracy of the proposed defense method is 19.812%, 17.083%, 18.802%, 21.490%, and 18.010% against Basic Iterative Method (BIM), C&W, DeepFool, DI2FGSM, and MDI2FGSM attacks, respectively. The defense method in this paper shows strong robustness compared with several other models.",Adversarial attack; Defence against adversarial examples; Vision transformer; SE module
"Vision transformers have shown great potential in various computer vision tasks owing to their strong capability to model long-range dependency using the self-attention mechanism. Nevertheless, they treat an image as a 1D sequence of visual tokens, lacking an intrinsic inductive bias (IB) in modeling local visual structures and dealing with scale variance, which is instead learned implicitly from large-scale training data with longer training schedules. In this paper, we leverage the two IBs and propose the ViTAE transformer, which utilizes a reduction cell for multi-scale feature and a normal cell for locality. The two kinds of cells are stacked in both isotropic and multi-stage manners to formulate two families of ViTAE models, i.e., the vanilla ViTAE and ViTAEv2. Experiments on the ImageNet dataset as well as downstream tasks on the MS COCO, ADE20K, and AP10K datasets validate the superiority of our models over the baseline and representative models. Besides, we scale up our ViTAE model to 644 M parameters and obtain the state-of-the-art classification performance, i.e., 88.5% Top-1 classification accuracy on ImageNet validation set and the best 91.2% Top-1 classification accuracy on ImageNet Real validation set, without using extra private data. It demonstrates that the introduced inductive bias still helps when the model size becomes large. The source code and pretrained models are publicly available atcode.",Vision transformer; Neural networks; Image classification; Object detection; Inductive bias
"Automated dewarping of camera-captured handwritten documents is a challenging research problem in Computer Vision and Pattern Recognition. Most available systems assume the shape of the camera-captured image boundaries to be anywhere between trapezoidal and octahedral, with linear distortion in areas between the boundaries for dewarping. The majority of the state-of-the-art applications successfully dewarp the simple-to-medium range geometrical distortions with partial selection of control points by a user. The proposed work implements a fully automated technique for control point detection from simple-to-complex geometrical distortions in camera-captured document images. The input image is subject to preprocessing, corner point detection, document map generation, and rendering of the de-warped document image. The proposed algorithm has been tested on five different camera-captured document datasets (one internal and four external publicly available) consisting of 958 images. Both quantitative and qualitative evaluations have been performed to test the efficacy of the proposed system. On the quantitative front, an Intersection Over Union (IoU) score of 0.92, 0.88, and 0.80 for document map generation for low-, medium-, and high-complexity datasets, respectively. Additionally, accuracies of the recognized texts, obtained from a market leading OCR engine, are utilized for quantitative comparative analysis on document images before and after the proposed enhancement. Finally, the qualitative analysis visually establishes the system's reliability by demonstrating improved readability even for severely distorted image samples.",Dewarping; Document images; Smart innovations; Computer vision model; Control point generation
"Multitask learning (MTL) is a challenging puzzle, particularly in the realm of computer vision (CV). Setting up vanilla deep MTL requires either hard or soft parameter sharing schemes that employ greedy search to find the optimal network designs. Despite its widespread application, the performance of MTL models is vulnerable to under-constrained parameters. In this article, we draw on the recent success of vision transformer (ViT) to propose a multitask representation learning method called multitask ViT (MTViT), which proposes a multiple branch transformer to sequentially process the image patches (i.e., tokens in transformer) that are associated with various tasks. Through the proposed cross-task attention (CA) module, a task token from each task branch is regarded as a query for exchanging information with other task branches. In contrast to prior models, our proposed method extracts intrinsic features using the built-in self-attention mechanism of the ViT and requires just linear time on memory and computation complexity, rather than quadratic time. Comprehensive experiments are carried out on two benchmark datasets, including NYU-Depth V2 (NYUDv2) and CityScapes, after which it is found that our proposed MTViT outperforms or is on par with existing convolutional neural network (CNN)-based MTL methods. In addition, we apply our method to a synthetic dataset in which task relatedness is controlled. Surprisingly, experimental results reveal that the MTViT exhibits excellent performance when tasks are less related.",Task analysis; Transformers; Neural networks; Visualization; Biological system modeling; Benchmark testing; Correlation; Deep neural network algorithms; machine learning applications; multitask learning (MTL)
"Contemporary deep neural networks offer state-of-the-art results when applied to visual reasoning, e.g., in the context of 3D point cloud data. Point clouds are an important data type for the precise modeling of three-dimensional environments, but effective processing of this type of data proves to be challenging. In the world of large, heavily-parameterized network architectures and continuously-streamed data, there is an increasing need for machine learning models that can be trained on additional data. Unfortunately, currently available models cannot fully leverage training on additional data without losing their past knowledge. Combating this phenomenon, called catastrophic forgetting, is one of the main objectives of continual learning. Continual learning for deep neural networks has been an active field of research, primarily in 2D computer vision, natural language processing, reinforcement learning, and robotics. However, in 3D computer vision, there are hardly any continual learning solutions specifically designed to take advantage of point cloud structure. This work proposes a novel neural network architecture capable of continual learning on 3D point cloud data. We utilize point cloud structure properties for preserving a heavily compressed set of past data. By using rehearsal and reconstruction as regularization methods of the learning process, our approach achieves a significant decrease of catastrophic forgetting compared to the existing solutions on several most popular point cloud datasets considering two continual learning settings: when a task is known beforehand, and in the challenging scenario of when task information is unknown to the model.",Continual learning; Point cloud; Deep learning; Data compression
"People with speech and motor impairments may experience difficulties in interaction and learning, among other situations that can lead to emotional, social, and cognitive problems. Augmentative and alternative communication (AAC) is a research area that involves using non-oral modes as a complement or substitute for spoken language. The AAC supported by computer vision (CV) systems can benefit from recognizing the user's remaining functional movements as an alternative design approach to interaction. The complete MyPGI, Methodology to yield Personalized Gestural Interaction, is presented. MyPGI guides the design of AAC systems for people with motor and speech difficulties, using CV techniques and machine learning to enable personalized and noninvasive gestural interaction. The MyPGI methodology was used to develop an AAC system, named PGCA (Personal Gesture Communication Assistant), employing a low-cost approach, used in experiments conducted with volunteers, including students with motor and speech difficulties. Experiments, interviews, and usability evaluation were conducted to evaluate the feasibility of the methodology and the system developed. The results suggest the methodology as promising to support the design of AAC systems capable of enabling personalized gestural interaction, also showing benefits of this approach, technical challenges, and means to overcome them. The results also add knowledge about specific challenges and needs of the target audience. The MyPGI methodology, developed after several iterations and evaluations, is capable to support the design of AAC systems that enable personalized gestural interaction. This article presents an overview of the methodological steps performed, results obtained, and future perspectives for the methodology.",Computer vision; Human-computer interaction; Augmentative and alternative communication; Assistive technology; Machine learning
"For fashion outfits to be considered aesthetically pleasing, the garments that constitute them need to be compatible in terms of visual aspects, such as style, category and color. Previous works have defined visual compatibility as a binary classification task with items in a garment being considered as fully compatible or fully incompatible. However, this is not applicable to Outfit Maker applications where users create their own outfits and need to know which specific items may be incompatible with the rest of the outfit. To address this, we propose the Visual InCompatibility TransfORmer (VICTOR) that is optimized for two tasks: 1) overall compatibility as regression and 2) the detection of mismatching items and utilize fashion-specific contrastive language-image pre-training for fine tuning computer vision neural networks on fashion imagery. We build upon the Polyvore outfit benchmark to generate partially mismatching outfits, creating a new dataset termed Polyvore-MISFITs, that is used to train VICTOR. A series of ablation and comparative analyses show that the proposed architecture can compete and even surpass the current state-of-the-art on Polyvore datasets while reducing the instance-wise floating operations by 88%, striking a balance between high performance and efficiency. We release our code at https://github.com/stevejpapad/Visual-InCompatibility-Transformer",Recommendation system; Outfit matching; Visual compatibility; Computer vision; Deep learning
"This paper describes the implementation of a solution for detecting the machining defects from an engine block, in the piston chamber. The solution was developed for an automotive manufacturer and the main goal of the implementation is the replacement of the visual inspection performed by a human operator with a computer vision application. We started by exploring different machine vision applications used in the manufacturing environment for several types of operations, and how machine learning is being used in robotic industrial applications. The solution implementation is re-using hardware that is already available at the manufacturing plant and decommissioned from another system. The re-used components are the cameras, the IO (Input/Output) Ethernet module, sensors, cables, and other accessories. The hardware will be used in the acquisition of the images, and for processing, a new system will be implemented with a human-machine interface, user controls, and communication with the main production line. Main results and conclusions highlight the efficiency of the CCD (charged-coupled device) sensors in the manufacturing environment and the robustness of the machine learning algorithms (convolutional neural networks) implemented in computer vision applications (thresholding and regions of interest).",computer vision; sensors; machine learning; industry; manufacturing; robotics
"Fully-supervised object detection and instance segmentation models have accomplished notable results on large-scale computer vision benchmark datasets. However, fully-supervised machine learning algorithms' performances are immensely dependent on the quality of the training data. Preparing computer vision datasets for object detection and instance segmentation is a labor-intensive task requiring each instance in an image to be annotated. In practice, this often results in the quality of bounding box and polygon mask annotations being suboptimal. This paper quantifies empirically the ground truth annotation quality and COCO's mean average precision (mAP) performance by introducing two separate noise measures, uniform and radial, into the ground truth bounding box and polygon mask annotations for the COCO and Cityscapes datasets. Mask-RCNN models are trained on various levels of noise measures to investigate the performance of each level of noise. The results showed degradation of mAP as the level of both noise measures increased. For object detection and instance segmentation respectively, using the highest level of noise measure resulted in a mAP degradation of 0.185 & 0.208 for uniform noise with reductions of 0.118 & 0.064 for radial noise on the COCO dataset. As for the Cityscapes datasets, reductions of mAP performance of 0.147 & 0.142 for uniform noise and 0.101 & 0.033 for radial noise were recorded. Furthermore, a decrease in average precision is seen across all classes, with the exception of the class motorcycle. The reductions between classes vary, indicating the effects of annotation uncertainty are class-dependent.",Annotations; Object detection; Uncertainty; Task analysis; Noise measurement; Computer vision; Supervised learning; Annotation uncertainty; computer vision; instance segmentation; object detection; supervised learning
"Civil infrastructure (e.g., buildings, roads, underground tunnels) could lose its expected physical and functional conditions after years of operation. Timely and accurate inspection and assessment of such infrastructures are essential to ensure safety and serviceability, e.g., by preventing unsafe working conditions and hazards. Cracks, which are one of the most common distress, can indicate severe structural integrity issues that threaten the safety of the structure and people in the environment. As such, accurate, fast, and automatic detection of cracks on structure surfaces is a major issue for a variety of civil engineering applications. Due to advances in hardware data acquisition systems, significant progress has been made in the automatic detection and quantification of cracks in recent decades. This paper provides a comprehensive review of the research progress and prospects in computer vision frameworks for crack detection of civil infrastructures from multiple materials, including asphalt, concrete, and metal-like materials. The review encompasses major components of typical frameworks, i.e., data acquisition techniques, publicly available datasets, detection algorithms, and evaluation metrics. In particular, we provide a taxonomy of detection algorithms with a detailed discussion of the advantages, limitations, and application scenarios of the methods in each category, as well as the relationships between methods of different categories. We also discuss unsolved issues and key challenges in crack detection that could drive future research directions.",Crack detection; Civil infrastructure; Computer vision based methods; Data acquisition; Structural health monitoring
"Intelligent UAV video analysis has drawn the attention of many researchers due to the increasing demand for unmanned aerial vehicles (UAVs) in computer vision-related applications. Applications such as search and rescue, the military, and surveillance demand automatic detection of human targets in large-scale UAV images, which is very challenging due to the small size and inadequate feature representation of person objects. Despite the significant advancements in generic object detection tasks, the performance of the state-of-the-art small object detection algorithms falls below the satisfactory level due to the lack of a representative dataset and the limited information available for small objects. To facilitate advancements in UAV and small object detection research, we present a Manipal-UAV person detection dataset1 collected from two UAVs flying at varying altitudes, locations, and weather conditions. The dataset contains 13,462 sampled images from 33 videos having 1,53,112 person object instances. The videos are captured in an unconstrained environment with complex scenes covering small objects of varying scales, poses, illumination, and occlusion, making person detection extremely challenging on this newly created dataset. This article compares the characteristics of the Manipal-UAV dataset with the standard VisDrone and Okutama datasets having aerial view person objects. In addition, it provides baseline evaluation results of the various state-of-the-art object detection algorithms applied to the newly created Manipal-UAV Person detection dataset. The dataset is made publicly available at https://github.com/Akshathakrbhat/Manipal-UAV-Person-Dataset.",Small object detection; Unmanned aerial vehicles; Convolutional neural networks; Deep learning; Computer vision
"The demand for flexible large-area optoelectronic devices has been growing significantly during recent years. Roll-to-roll (R2R) printing facilitates the cost-efficient industrial production of different optoelectronic devices. Nonetheless, the performance of these devices is highly dependent on the printing quality and number of defects of R2R printed conductors. The image processing technique is an efficient nondestructive testing (NDT) methodology used to detect such defects. In this study, a computer vision-based assessment tool was utilized to visualize R2R printed silver conductors' defects on flexible plastic substrates. A multistage defect detection technique was proposed to detect and classify both printing-induced defects and imperfections as well as the misalignment of the printed conductors with respect to the reference design. The method proved to be a very reliable approach that can be used independently or in conjunction with electrical testing methods for quality assurance purposes during the production of R2R prints.",automated defects recognition; roll-to-roll; printing; organic photovoltaic; thin film; nondestructive testing; image processing; computer vision
"The growing awareness of the influence of what we eat on lifestyle and health has led to an increase in the use of embedded food analysis and recognition systems. These solutions aim to effectively monitor daily food consumption, and therefore provide dietary recommendations to enable and support lifestyle changes. Mobile applications, due to their high accessibility, are ideal for real-life food recognition, volume estimation and calorific estimation. In this study, we conducted a systematic review based on articles that proposed mobile computer vision-based solutions for food recognition, volume estimation and calorific estimation. In addition, we assessed the extent to which these applications provide explanations to aid the users to understand the related classification and/or predictions. Our results show that 90.9% of applications do not distinguish between food and non-food. Similarly, only one study that proposed a mobile computer vision-based application for dietary intake attempted to provide explanations of features that contribute towards classification. Mobile computer vision-based applications are attracting a lot of interest in healthcare. They have the potential to assist in the management of chronic illnesses such as diabetes, ensuring that patients eat healthily and reducing complications associated with unhealthy food. However, to improve trust, mobile computer vision-based applications in healthcare should provide explanations of how they derive their classifications or volume and calorific estimations.",computer vision; mobile applications; food recognition; volume estimation; nutritional monitoring
"Automatic food recognition systems have been receiving increasing attention in the research community with the advancements in inductive learning (e.g., classification in computer vision) due to their applicability in the healthcare and hospitality industry. However, food recognition is challenging due to its fine-grained nature and its high correlation with culture, geo-location, and language. To make food recognition systems feasible for the Middle Eastern region, we present a large-scale dataset (MEFood) of commonly consumed food items in the Middle East, thereby providing a dataset for current development and establishing a benchmark for future research. We have also thoroughly examined the MEFood dataset highlighting its challenging aspects and its real-world nature. Additionally, we have conducted a thorough experimental study benchmarking the mainstream computer vision and mobile networks on classification, runtime, and resource utilization metrics. Our results highlight that EfficientNet-V2 achieves performance closer to the best-performing individual model on the MEFood dataset while having the least resource utilization and minimal inference times. Finally, we have performed a thorough error analysis study to glean additional insights about the networks and MEFood dataset.",Benchmark testing; Feature extraction; Neural networks; Task analysis; Deep learning; Computer vision; Computational modeling; Food recognition; benchmark dataset; computer vision; Middle Eastern cuisine
"Diabetic Retinopathy (DR) is a serious hazard that can result in irreversible blindness if not addressed in a timely manner. Hence, numerous techniques have been proposed for the accurate and timely detection of this disease. Out of these, Deep Learning (DL) and Computer Vision (CV) methods for multiclass categorization of color fundus images diagnosed with Diabetic Retinopathy have sparked considerable attention. In this paper, we attempt to develop an extended ResNet152V2 architecture-based Deep Learning model, named ResNet2.0 to aid the timely detection of DR. The APTOS-2019 dataset was used to train the model. This consists of 3662 fundus images belonging to five different stages of DR: no DR (Class 0), mild DR (Class 1), moderate DR (Class 2), severe DR (Class 3), and proliferative DR (Class 4). The model was gauged based on ability to detect stage-wise DR. The images were pre-processed using negative and positive weighted Gaussian-based masks as feature engineering to further enhance the quality of the fundus images by removing the noise and normalizing the images. Upsampling and data augmentation methods were used to address the skewness of the original dataset. The proposed model achieved an overall accuracy of 91% and an area under the receiver-operating characteristic curve (AUC) score of 95.1%, outperforming existing Deep Learning models by around 10%. Furthermore, the class-wise F1 score for No DR was 92%, Mild DR was 82%, Moderate DR was 66%, Severe was DR 89% and Proliferative DR was 80%.",Diabetic retinopathy; deep learning; transfer learning; image; processing; image classification
"Every year, the VISion Understanding and Machine intelligence (VISUM) summer school runs a competition where participants can learn and share knowledge about Computer Vision and Machine Learning in a vibrant environment. 2021 VISUM's focused on applying those methodologies in fashion. Recently, there has been an increase of interest within the scientific community in applying computer vision methodologies to the fashion domain. That is highly motivated by fashion being one of the world's largest industries presenting a rapid development in e-commerce mainly since the COVID-19 pandemic. Computer Vision for Fashion enables a wide range of innovations, from personalized recommendations to outfit matching. The competition enabled students to apply the knowledge acquired in the summer school to a real-world problem. The ambition was to foster research and development in fashion outfit complementary product retrieval by leveraging vast visual and textual data with domain knowledge. For this, a new fashion outfit dataset (acquired and curated by FARFETCH) for research and benchmark purposes is introduced. Additionally, a competitive baseline with an original negative sampling process for triplet mining was implemented and served as a starting point for participants. The top 3 performing methods are described in this paper since they constitute the reference state-of-the-art for this particular problem. To our knowledge, this is the first challenge in fashion outfit complementary product retrieval. Moreover, this joint project between academia and industry brings several relevant contributions to disseminating science and technology, promoting economic and social development, and helping to connect early-career researchers to real-world industry challenges.",Image retrieval; Summer school competition; Computer vision; Deep learning; Fashion intelligence
"Urban environments are evolving rapidly in big cities; keeping track of these changes is becoming harder. Information regarding urban features, such as the number of trees, lights, or shops in a particular region, can be crucial for tasks, such as urban planning, commercial campaigns, or inferring various social indicators. StreetScouting is a platform that aims to automate the process of detecting, visualizing, and exporting the urban features of a particular region. Recently, the advent of deep learning has revolutionized the way many computer vision tasks are tackled. In this work, we present StreetScouting, an extensible platform for the automatic detection of particular urban features of interest. StreetScouting utilizes several state-of-the-art computer vision approaches including Cascade R-CNN and RetinaFace architectures for object detection, the ByteTrack method for object tracking, DNET architecture for depth estimation, and DeepLabv3+ architecture for semantic segmentation. As a result, the platform is able to detect and geotag urban features from visual data. The extracted information can be utilized by many commercial or public organizations, eliminating the need for manual inspection.",object detection; object tracking; deep learning; web application
"In the past two decades, there has been a lot of work on computer vision technology that incorporates many tasks which implement basic filter -ing to image classification. The major research areas of this field include object detection and object recognition. Moreover, wireless communication tech-nologies are presently adopted and they have impacted the way of education that has been changed. There are different phases of changes in the traditional system. Perception of three-dimensional (3D) from two-dimensional (2D) image is one of the demanding tasks. Because human can easily perceive but making 3D using software will take time manually. Firstly, the blackboard has been replaced by projectors and other digital screens so such that peo-ple can understand the concept better through visualization. Secondly, the computer labs in schools are now more common than ever. Thirdly, online classes have become a reality. However, transferring to online education or e-learning is not without challenges. Therefore, we propose a method for improving the efficiency of e-learning. Our proposed system consists of two-and-a-half dimensional (2.5D) features extraction using machine learning and image processing. Then, these features are utilized to generate 3D mesh using ellipsoidal deformation method. After that, 3D bounding box estimation is applied. Our results show that there is a need to move to 3D virtual reality (VR) with haptic sensors in the field of e-learning for a better understanding of real-world objects. Thus, people will have more information as compared to the traditional or simple online education tools. We compare our result with the ShapeNet dataset to check the accuracy of our proposed method. Our proposed system achieved an accuracy of 90.77% on plane class, 85.72% on chair class, and car class have 72.14%. Mean accuracy of our method is 70.89%.",Artificial intelligence; e -learning; online education system; computer vision; virtual reality; 3D haptic
"Image semantic segmentation is an important branch of computer vision of a wide variety of practical applications such as medical image analysis, autonomous driving, virtual or augmented reality, etc. In recent years, due to the remarkable performance of transformer and multilayer perceptron (MLP) in computer vision, which is equivalent to convolutional neural network (CNN), there has been a substantial amount of image semantic segmentation works aimed at developing different types of deep learning architecture. This survey aims to provide a comprehensive overview of deep learning methods in the field of general image semantic segmentation. Firstly, the commonly used image segmentation datasets are listed. Next, extensive pioneering works are deeply studied from multiple perspectives (e.g., network structures, feature fusion methods, attention mechanisms), and are divided into four categories according to different network architectures: CNN-based architectures, transformer-based architectures, MLP-based architectures, and others. Furthermore, this paper presents some common evaluation metrics and compares the respective advantages and limitations of popular techniques both in terms of architectural design and their experimental value on the most widely used datasets. Finally, possible future research directions and challenges are discussed for the reference of other researchers.",Deep learning; semantic segmentation; CNN; MLP; transformer
"It is important for humans to remain hydrated, particularly for older adults who are at a greater risk of dehydration and may forget to drink. Monitoring liquid intake and getting reminders to drink throughout the day is a useful solution to increase hydration levels. The objective of this paper is to automatically detect drink events from multiple containers in a simulated home environment using a vision-based approach. The proposed work compares the use of depth and RGB (red, green, blue) cameras for this task. In this paper, we compared 2D and 3D Convolutional Neural Networks (CNN) using RGB and depth cameras. We collected data from nine participants performing drinking, eating and other Activities of Daily Living (ADL) in a simulated home environment. We found that for the 3D models, the RGB and depth camera inputs provided very similar F1-scores for both 10-Fold (94.3% vs 93.9%, respectively) and Leave-One-Subject-Out (LOSO) cross validation (84.2% vs 86.2%, respectively). This is a promising result as depth cameras also mitigate the challenges to privacy of RGB-based models. The 3D CNN models outperformed the 2D models, thereby creating a more robust system. Depth cameras are a useful alternative to RGB cameras with equal performance in identifying drinking events.",Cameras; Three-dimensional displays; Computer vision; Artificial neural networks; Water; Video signal processing; Older adults; computer vision; depth cameras; fluid intake monitoring; image recognition; intake gesture detection; video signal processing
"Recent advancements in artificial intelligence (AI) have led to numerous medical discoveries. The field of computer vision (CV) for medical diagnosis has received particular attention. Using images of peripheral blood (PB) smears, CV has been utilized in hematology to detect acute leukemia (AL). Significant research has been undertaken in the area of AL diagnosis automation in order to deliver an accurate diagnosis. This study addresses the morphological classification of atypical white blood cells (WBCs), including immature WBCs and atypical lymphocytes, in acute myeloid leukemia (AML), as observed in peripheral blood (PB) smear images. The purpose of this work is to build a classification model for atypical AML WBCs based on their distinctive features. Using a hybrid model based on geometric transformation (GT) and a deep convolutional autoencoder (DCAE), this work provides a novel technique in the field of AI for resolving the issue of imbalanced distribution of WBCs in blood samples, nicknamed the GT-DCAE WBC augmentation model. In addition, to extract context-free atypical WBC features, this study develops a stable learning paradigm by incorporating WBC segmentation into deep learning. In order to classify atypical WBCs into eight distinct subgroups, a hybrid multiclassification model termed the two-stage DCAE-CNN atypical WBC classification model (DCAE-CNN) was developed. The model achieved an average accuracy of 97%, a sensitivity of 97%, and a precision of 98%. Overall and by class, the model's discriminating abilities were exceptional, with an AUC of 99.7% and a class-wise range of 80% to 100%.",acute myeloid leukemia; atypical white blood cells; autoencoder; CNN; augmentation
"Small object detection (SOD) is significant for many real-world applications, including criminal investigation, autonomous driving and remote sensing images. SOD has been one of the most challenging tasks in computer vision due to its low resolution and noise representation. With the development of deep learning, it has been introduced to boost the performance of SOD. In this paper, focusing on the difficulties of SOD, we analyze the deep learning-based SOD research papers from four perspectives, including boosting the resolution of input features, scale-aware training, incorporating contextual information and data augmentation. We also review the literature on crucial SOD tasks, including small face detection, small pedestrian detection and aerial image object detection. In addition, we conduct a thorough performance evaluation of generic SOD algorithms and methods for crucial SOD tasks on four well-known small object datasets. Our experimental results show that network configuring to boost the resolution of input features can enable significant performance gains on WIDER FACE and Tiny Person. Finally, several potential directions for future research in the area of SOD are provided.",small object detection; deep learning; computer vision; neural network; benchmark
"In October 2020, Google researchers present a promising Deep Learning architecture paradigm for Computer Vision that outperforms the already standard Convolutional Neural Networks (CNNs) on multiple image recognition state-of-the-art datasets: Vision Transformers (ViTs). Based on the self-attention concept inherited from Natural Language Processing (NLP), this new structure surpasses the CNN image classification task on ImageNet, CIFAR-100, and VTAB, among others, when it is fine-tuned (Transfer Leaning) after a previous pre-training on larger datasets. In this work, we confirm this theory and move one step further over the CNN structures applied for Vascular Biometric Recognition (VBR): to the best of our knowledge, we introduce for the first time multiple pure pre-trained and fine-tuned Vision Transformers in this evolving biometric modality to address the challenge of the limited number of samples in VBR datasets. For this purpose, the ViTs have been trained to extract unique image features on the ImageNet-1k and ImageNet-21k and then fine-tuned for the four main existing VBR variants, i.e., finger, palm, hand dorsal, and wrist vein areas. Fourteen existing vascular datasets have been used to perform the vein identification task in the four previously mentioned modalities, based on the True-Positive Identification Rate (TPIR) and 75-25% train-test sets obtaining the following results: HKPU (99.52%), and FV-USM (99.1%); Vera (99.39%), and CASIA (96.00%); Bosphorus (99.86%); PUT-wrist (99.67%), and UC3M-CV1+CV2 (99.67%). Furthermore, we introduce UC3M-CV3: a hygienic contactless wrist database collected on smartphones and consisting of 4800 images from 100 different subjects. The promising results show the Vision Transformer's versatility in VBR under Transfer Learning and reinforce this new Neural Network architecture paradigm.",Machine learning; Transformers; Biometrics (access control); Databases; Image recognition; Convolutional neural networks; Transfer learning; Vision transformers; vein biometric recognition; deep learning; convolutional neural networks; finger veins; transfer learning; machine learning; artificial intelligence; biometrics on mobile devices; contactless wrist vascular database; hand palm identification
"The accuracy and the overall performances of ophthalmic instrumentation, where specific analysis of eye images is involved, can be negatively influenced by invalid or incorrect frames acquired during everyday measurements of unaware or non-collaborative human patients and non-technical operators. Therefore, in this paper, we investigate and compare the adoption of several vision-based classification algorithms belonging to different fields, i.e., Machine Learning, Deep Learning, and Expert Systems, in order to improve the performance of an ophthalmic instrument designed for the Pupillary Light Reflex measurement. To test the implemented solutions, we collected and publicly released PopEYE as one of the first datasets consisting of 15 k eye images belonging to 22 different subjects acquired through the aforementioned specialized ophthalmic device. Finally, we discuss the experimental results in terms of classification accuracy of the eye status, as well as computational load analysis, since the proposed solution is designed to be implemented in embedded boards, which have limited hardware resources in computational power and memory size.",pupillary light reflex; ophthalmic instrumentation; eye status classification; computer vision-based classification; machine learning; deep learning; expert systems
"Multi-object tracking (MOT) is essential for solving the majority of computer vision issues related to crowd analytics. In an MOT system designing object detection and association are the two main steps. Every frame of the video stream is examined to find the desired objects in the first step. Their trajectories are determined in the second step by comparing the detected objects in the current frame to those in the previous frame. Less missing detections are made possible by an object detection system with high accuracy, which results in fewer segmented tracks. We propose a new deep learning-based model for improving the performance of object detection and object tracking in this research. First, object detection is performed by using the adaptive Mask-RCNN model. After that, the ResNet-50 model is used to extract more reliable and significant features of the objects. Then the effective adaptive feature channel selection method is employed for selecting feature channels to determine the final response map. Finally, an adaptive combination kernel correlation filter is used for multiple object tracking. Extensive experiments were conducted on large object-tracking databases likeMOT-20 and KITTIMOTS. According to the experimental results, the proposed tracker performs better than other cutting-edge trackers when faced with various problems. The experimental simulation is carried out in python. The overall success rate and precision of the proposed algorithm are 95.36% and 93.27%.",Computer vision; surveillance; tracking; correlation filters; holistic samples
"Current computer vision research uses huge datasets with millions of images to pre-train vision models. This results in escalation of time and capital, ethical issues, moral issues, privacy issues, copyright issues, fairness issues, and others. To address these issues, several alternative learning schemes have been developed. One such scheme is formula-based supervised learning (FDSL). It is a form of supervised learning, which involves the use of mathematically generated images for the pre-training of deep models. Promising results have been obtained for computer-vision-related applications. In this comprehensive survey paper, a gentle introduction to FDSL is presented. The supporting theory, databases, experimentation and ensuing results are discussed. The research outcomes, issues and scope are also discussed. Finally, some of the most promising future directions for FDSL research are discussed. As FDSL is an important learning technique, this survey represents a useful resource for interested researchers working on solving various problem in computer vision and related areas of application.",formula-driven supervised learning; fractals; deep learning; visual transformers; ViTs; CNNs; object recognition; computer vision
"Transformer, first applied to the field of natural language processing, is a type of deep neural network mainly based on the self-attention mechanism. Thanks to its strong representation capabilities, researchers are looking at ways to apply transformer to computer vision tasks. In a variety of visual benchmarks, transformer-based models perform similar to or better than other types of networks such as convolutional and recurrent neural networks. Given its high performance and less need for vision-specific inductive bias, transformer is receiving more and more attention from the computer vision community. In this paper, we review these vision transformer models by categorizing them in different tasks and analyzing their advantages and disadvantages. The main categories we explore include the backbone network, high/mid-level vision, low-level vision, and video processing. We also include efficient transformer methods for pushing transformer into real device-based applications. Furthermore, we also take a brief look at the self-attention mechanism in computer vision, as it is the base component in transformer. Toward the end of this paper, we discuss the challenges and provide several further research directions for vision transformers.",Transformers; Task analysis; Encoding; Computer vision; Computational modeling; Visualization; Object detection; high-level vision; low-level vision; self-attention; transformer; video
"Automatic detection and analysis of rice crop diseases is widely required in the farming industry, which can be utilized to avoid squandering financial and other resources, reduce yield losses, and improve treatment efficiency, resulting in healthier crop output. An automated approach was proposed for accurately detecting and classifying diseases from a supplied photograph. The proposed system for the recognition of rice plant diseases adopts a computer vision-based approach that employs the techniques of image processing, machine learning, and deep learning, reducing the reliance on conventional methods to protect paddy crops from diseases like bacterial leaf blight, false smut, brown leaf spot, rice blast, and sheath rot, the five primary diseases that frequently plague the Indian rice fields. Following image pre-processing, image segmentation is employed to determine the diseased section of the paddy plant, with the diseases listed above being identified purely on the basis of their visual contents. An integration of a support vector machine classifier and convolutional neural networks are used to recognize and classify specific varieties of paddy plant diseases. With ReLU and softmax functions, the suggested deep learning-based strategy attained the highest validation accuracy of 0.9145. Following recognition, a predictive remedy is recommended, which can assist agriculture-related individuals and organizations in taking suitable measures to combat these diseases.",Computer vision; Machine learning; Deep learning; Convolutional neural network; Support vector machine; Image segmentation
"Target tracking is one of the challenging tasks in computer vision. Usually, the center of target origins from the position with the largest response value, and the key to improving tracking performance is to learn reliable feature maps. This paper analyzes the characteristics of the tracking task, designs a global feature comparison function to extract the context, and proposes a feature supplement module based on the global comparison information for further performance improvement. In addition, we also design a template feature update module to supplement template features based on the search area features of the current frame to dynamically adjust model features, improve model generalization capabilities, and avoid model feature fixation. The proposed feature supplement model based on global feature comparison (FSGFC) is evaluated on five visual tracking benchmarks including OTB100, VOT2016, VOT2018, VOT2019 and UAV123. The experimental results show that the model obtains the state-of-the-art performance with a real-time speed.",Feature extraction; Target tracking; Computer vision; Data mining; Computational modeling; Residual neural networks; Benchmark testing; Siamese network; feature extraction; template update
"To train deep learning models for vision-based action recognition of elders' daily activities, we need large-scale activity datasets acquired under various daily living environments and conditions. However, most public datasets used in human action recognition either differ from or have limited coverage of elders' activities in many aspects, making it challenging to recognize elders' daily activities well by only utilizing existing datasets. Recently, such limitations of available datasets have actively been compensated by generating synthetic data from realistic simulation environments and using those data to train deep learning models. In this paper, based on these ideas we develop ElderSim, an action simulation platform that can generate synthetic data on elders' daily activities. For 55 kinds of frequent daily activities of the elders, ElderSim generates realistic motions of synthetic characters with various adjustable data-generating options and provides different output modalities including RGB videos, two- and three-dimensional skeleton trajectories. We then generate KIST SynADL, a large-scale synthetic dataset of elders' activities of daily living, from ElderSim and use the data in addition to real datasets to train three state-of-the-art human action recognition models. From the experiments following several newly proposed scenarios that assume different real and synthetic dataset configurations for training, we observe a noticeable performance improvement by augmenting our synthetic data. We also offer guidance with insights for the effective utilization of synthetic data to help recognize elders' daily activities.",Data models; Videos; Training; Three-dimensional displays; Feature extraction; Deep learning; Two dimensional displays; Classification algorithms; computer graphics; computer simulation; computer vision; supervised learning
"Human pose estimation (HPE) is a procedure for determining the structure of the body pose and it is considered a challenging issue in the computer vision (CV) communities. HPE finds its applications in several fields namely activity recognition and human-computer interface. Despite the benefits of HPE, it is still a challenging process due to the variations in visual appearances, lighting, occlusions, dimensionality, etc. To resolve these issues, this paper presents a squirrel search optimization with a deep convolutional neural network for HPE (SSDCNN-HPE) technique. The major intention of the SSDCNN-HPE technique is to identify the human pose accurately and efficiently. Primarily, the video frame conversion process is performed and pre-processing takes place via bilateral filtering-based noise removal process. Then, the EfficientNet model is applied to identify the body points of a person with no problem constraints. Besides, the hyperparameter tuning of the EfficientNet model takes place by the use of the squirrel search algorithm (SSA). In the final stage, the multiclass support vector machine (M-SVM) technique was utilized for the identification and classification of human poses. The design of bilateral filtering followed by SSA based EfficientNet model for HPE depicts the novelty of the work. To demonstrate the enhanced outcomes of the SSDCNN-HPE approach, a series of simulations are executed. The experimental results reported the betterment of the SSDCNN-HPE system over the recent existing techniques in terms of different measures.",Parameter tuning; human pose estimation; deep learning; squirrel search algorithm; activity recognition
"In the last few years, due to the continuous advancement of technology, human behavior detection and recognition have become important scientific research in the field of computer vision (CV). However, one of the most challenging problems in CV is anomaly detection (AD) because of the complex environment and the difficulty in extracting a particular feature that correlates with a particular event. As the number of cameras monitoring a given area increases, it will become vital to have systems capable of learning from the vast amounts of available data to identify any potential suspicious behavior. Then, the introduction of deep learning (DL) has brought new development directions for AD. In particular, DL models such as convolution neural networks (CNNs) and recurrent neural networks (RNNs) have achieved excellent performance dealing with AD tasks, as well as other challenging domains like image classification, object detection, and speech processing. In this review, we aim to present a comprehensive overview of those research methods using DL to address the AD problem. Firstly, different classifications of anomalies are introduced, and then the DL methods and architectures used for video AD are discussed and analyzed, respectively. The revised contributions have been categorized by the network type, architecture model, datasets, and performance metrics that are used to evaluate these methodologies. Moreover, several applications of video AD have been discussed. Finally, we outlined the challenges and future directions for further research in the field.",deep learning; anomaly detection; human behavior; video surveillance
"Unmanned aerial vehicles (UAVs), in conjunction with computer vision techniques, have shown great potential for bridge inspections. Close-range images captured in proximity to the structural surface are generally required to detect damage and also need to be linked to the corresponding structural component to enable assessment of the health of the global structure. However, the lack of contextual information makes automated identification of bridge components in close-range images challenging. This study proposes a framework for automated bridge component recognition based on close-range images collected by UAVs. First, a 3D point cloud is generated from the UAV survey of the bridge and segmented into bridge components. The segmented point cloud is subsequently projected onto the camera coordinates to categorize each of the images into the bridge component. The proposed approach is successfully validated using a local highway bridge, pointing the way for improved inspection of full-scale bridges.",3D semantic segmentation; Automated structural inspection; Bridge components; Close-range images; Computer vision; Point cloud; Unmanned aerial vehicle (UAV)
"Object detection, one of the core research topics in computer vision, is extensively used in various industrial activities. Although there have been many studies of daytime images where objects can be easily detected, there is relatively little research on nighttime images. In the case of nighttime, various types of noises, such as darkness, haze, and light blur, deteriorate image quality. Thus, an appropriate process for removing noise must precede to improve object detection performance. Although there are many studies on removing individual noise, only a few studies handle multiple noises simultaneously. In this paper, we propose a convolutional denoising autoencoder (CDAE)-based architecture trained on various types of noises. We also present various composing modules for each noise to improve object detection performance for night images. Using the exclusively dark (ExDark) Image dataset, experimental results show that the Sequential filtering architecture showed superior mean average precision(mAP) compared to other architectures.",Object detection; computer vision; nighttime; multiple noises; convolutional denoising autoencoder
"Development of deep learning has led to progress in computer vision, including metric learning tasks such as image retrieval, through convolutional neural networks. In image retrieval, the metric distance (i.e., the similarity) between the images needs to be computed and then compared to return similar images. Global descriptors are good at extracting holistic features of an image, such as the overall shape of the main object and the silhouette. On the other hand, the local features extract the detailed features which the model uses to help classify similar images together. This paper proposes a descriptor mixer which takes advantage of both local and global descriptors (group of features combined into one) as well as different types of global descriptors for an effect of a lighter version of an ensemble of models (i.e., fewer parameters and smaller model size than those of actual ensemble of networks). As a result, the model's performance improved about 1.36% (recall @ 32) when the combination of the descriptors were used. We empirically found out that the combination of GeM and MAC achieved the highest performance.",image retrieval; deep metric learning; computer vision
"This paper presents ArtVision, a Semantic Web application that integrates computer vision APIs with the ResearchSpace platform, allowing for the matching of similar artworks and photographs across cultural heritage image collections. The field of Digital Art History stands to benefit a great deal from computer vision, as numerous projects have already made good progress in tackling issues of visual similarity, artwork classification, style detection, gesture analysis, among others. Pharos, the International Consortium of Photo Archives, is building its platform using the ResearchSpace knowledge system, an open-source semantic web platform that allows heritage institutions to publish and enrich collections as Linked Open Data through the CIDOC-CRM, and other ontologies. Using the images and artwork data of Pharos collections, this paper outlines the methodologies used to integrate visual similarity data from a number of computer vision APIs, allowing users to discover similar artworks and generate canonical URIs for each artwork.",Linked Open Data; Computer Vision; cultural heritage; Semantic Web; visual similarity
"Sign language is the most common form of communication for the hearing impaired. To bridge the communication gap with such impaired people, a normal people should be able to recognize the signs. Therefore, it is necessary to introduce a sign language recognition system to assist such impaired people. This paper proposes the Transformer Encoder as a useful tool for sign language recognition. For the recognition of static Indian signs, the authors have implemented a vision transformer. To recognize static Indian sign language, proposed methodology archives noticeable performance over other state-of-the-art convolution architecture. The suggested methodology divides the sign into a series of positional embedding patches, which are then sent to a transformer block with four self-attention layers and a multilayer perceptron network. Experimental results show satisfactory identification of gestures under various augmentation methods. Moreover, the proposed approach only requires a very small number of training epochs to achieve 99.29 percent accuracy.",Assistive technologies; Gesture recognition; Transformers; Feature extraction; Deep learning; Computer vision; Computer architecture; Sign language; Transformer; deep learning; sign language; multi head attention; healthcare; mental health
"Defect detection is an essential part of quality management for bare printed circuit board (PCB) production. Existing vision-based methods are not effective in detecting PCB defects when uncertainty exists. This article proposes a multiscale convolution-based detection methodology to classify bare PCB defects under uncertainty. First, a novel window-based loss function is designed to tackle the inter-class imbalance and uncertainty. Then, a multiscale convolution network is constructed to process the defects with intra-class variance, and large scale extraction features are fused on the small scale to guide the extraction process. After that, the classification probability is extracted and assembled into a multiscale probability matrix, on which entropy-based probabilistic decisions are integrated for the final decision. Finally, experimental studies indicate that the proposed methodology can achieve satisfactory detection performance and demonstrate visual interpretability compared to baseline methods.",Feature extraction; Convolution; Uncertainty; Probabilistic logic; Production; Learning systems; Kernel; Bare printed circuit board (PCB); computer vision; convolution network; defect detection; multiscale classification
"Compared with the CPUs and GPUs, the AI accelerators are able to achieve higher performance and energy efficiency for accelerating the DNNs. However, besides the DNNs, the computer vision also involves other tasks such as conventional image filtering and stereo matching. These tasks are not supported by the AI accelerators. In addition, the newly proposed DNN structures are not supported by the existing AI accelerators, making them difficult to catch up with the ever-evolving AI algorithms. To address this challenge, the Google has proposed the Pixel Visual Core (PVC) processor with a flexible architecture to accelerate diverse computer vision tasks including the DNNs while achieving higher efficiency. However, the architecture of the PVC is not well optimized, leading to limited energy efficiency. In this brief, we have proposed a flexible and efficient processor architecture (named NVP) with several design techniques to address the limitations of the PVC. The NVP is able to accelerate diverse computer vision tasks including DNN structures, conventional image filtering and stereo matching, while achieving significantly improved energy efficiency than the PVC and comparable energy efficiency with the AI accelerators.",Computer architecture; Task analysis; Convolution; Computer vision; Arrays; AI accelerators; Kernel; Neural network; visual processor; hardware accelerator; DNN
"Gastric Cancer (GC) has been identified as the world's fifth most general tumor. So, it is important to diagnose the GC at initial stages itself to save the lives. Histopathological analysis remains the gold standard for accurate diagnosis of the disease. Though Computer-Aided Diagnostic approaches are prevalently applied in recent years for the diagnosis of diseases, it is challenging to apply in this case, due to the lack of accessible gastric histopathological image databases. With a rapid progression in the Computer Vision (CV) technologies, particularly, the emergence of medicinal image classifiers, it has become feasible to examine all the types of electron micrographs in a rapid and an effective manner. Therefore, the current research article presents an Anas Platyrhynchos Optimizer with Deep Learning-based Gastric Cancer Classification (APODL-GCC) method for the classification of GC using the endoscopic images. The aim of the proposed APODL-GCC method is to identify the presence of GC with the help of CV and Deep Learning concepts. Primarily, the APODL-GCC technique employs a contrast enhancement technique. Next, the feature extraction process is performed using a neural architectural search network model to generate a collection of feature vectors. For hyperparameter optimization, the Anas Platyrhynchos Optimizer (APO) algorithm is used which enhances the classification performance. Finally, the GC classification process is performed using the Deep Belief Network method. The proposed APODL-GCC technique was simulated using medical images and the experimental results established that the APODL-GCC technique accomplishes enhanced performance over other models.",gastric cancer; artificial intelligence; deep learning; computer vision; NASNet model; endoscopic images
"Deep Learning algorithms have achieved state-of-the-art performance for Image Classification. For this reason, they have been used even in security-critical applications, such as biometric recognition systems and self-driving cars. However, recent works have shown those algorithms, which can even surpass human capabilities, are vulnerable to adversarial examples. In Computer Vision, adversarial examples are images containing subtle perturbations generated by malicious optimization algorithms to fool classifiers. As an attempt to mitigate these vulnerabilities, numerous countermeasures have been proposed recently in the literature. However, devising an efficient defense mechanism has proven to be a difficult task, since many approaches demonstrated to be ineffective against adaptive attackers. Thus, this article aims to provide all readerships with a review of the latest research progress on Adversarial Machine Learning in Image Classification, nevertheless, with a defender's perspective. This article introduces novel taxonomies for categorizing adversarial attacks and defenses, as well as discuss possible reasons regarding the existence of adversarial examples. In addition, relevant guidance is also provided to assist researchers when devising and evaluating defenses. Finally, based on the reviewed literature, this article suggests some promising paths for future research.",Computer vision; image classification; adversarial images; deep neural networks; adversarial attacks; defense methods
"Recent advancements in transformers exploited computer vision problems which results in state-of-the-art models. Transformer-based models in various sequence prediction tasks such as language translation, sentiment classification, and caption generation have shown remarkable performance. Auto report generation scenarios in medical imaging through caption generation models is one of the applied scenarios for language models and have strong social impact. In these models, convolution neural networks have been used as encoder to gain spatial information and recurrent neural networks are used as decoder to generate caption or medical report. However, using transformer architecture as encoder and decoder in caption or report writing task is still unexplored. In this research, we explored the effect of losing spatial biasness information in encoder by using pre-trained vanilla image transformer architecture and combine it with different pre-trained language transformers as decoder. In order to evaluate the proposed methodology, the Indiana University Chest X-Rays dataset is used where ablation study is also conducted with respect to different evaluations. The comparative analysis shows that the proposed methodology has represented remarkable performance when compared with existing techniques in terms of different performance parameters.",Vision transformers; language models; radiology report; decoder
"Understanding actions in videos remains a significant challenge in computer vision, which has been the subject of several pieces of research in the last decades. Convolutional neural networks (CNN) are a significant component of this topic and play a crucial role in the renown of Deep Learning. Inspired by the human vision system, CNN has been applied to visual data exploitation and has solved various challenges in various computer vision tasks and video/image analysis, including action recognition (AR). However, not long ago, along with the achievement of the transformer in natural language processing (NLP), it began to set new trends in vision tasks, which has created a discussion around whether the Vision Transformer models (ViT) will replace CNN in action recognition in video clips. This paper conducts this trending topic in detail, the study of CNN and Transformer for Action Recognition separately and a comparative study of the accuracy-complexity trade-off. Finally, based on the performance analysis's outcome, the question of whether CNN or Vision Transformers will win the race will be discussed.",convolutional neural networks; vision transformers; recurrent neural networks; conversational systems; action recognition; natural language understanding; action recognitions
"Accidents have contributed a lot to the loss of lives of motorists and serious damage to vehicles around the globe. Potholes are the major cause of these accidents. It is very important to build a model that will help in recognizing these potholes on vehicles. Several object detection models based on deep learning and computer vision were developed to detect these potholes. It is very important to develop a lightweight model with high accuracy and detection speed. In this study, we employed a Mask RCNN model with ResNet-50 and MobileNetv1 as the backbone to improve detection, and also compared the performance of the proposed Mask RCNN based on original training images and the images that were filtered using a Gaussian smoothing filter. It was observed that the ResNet trained on Gaussian filtered images outperformed all the employed models.",Mask RCNN; pothole; computer vision; smart cities; object detection; Gaussian filter
"Improved picture quality is critical to the effectiveness of object recog-nition and tracking. The consistency of those photos is impacted by night-video systems because the contrast between high-profile items and different atmospheric conditions, such as mist, fog, dust etc. The pictures then shift in intensity, colour, polarity and consistency. A general challenge for computer vision analyses lies in the horrid appearance of night images in arbitrary illumination and ambient envir-onments. In recent years, target recognition techniques focused on deep learning and machine learning have become standard algorithms for object detection with the exponential growth of computer performance capabilities. However, the iden-tification of objects in the night world also poses further problems because of the distorted backdrop and dim light. The Correlation aware LSTM based YOLO (You Look Only Once) classifier method for exact object recognition and deter-mining its properties under night vision was a major inspiration for this work. In order to create virtual target sets similar to daily environments, we employ night images as inputs; and to obtain high enhanced image using histogram based enhancement and iterative wiener filter for removing the noise in the image. The process of the feature extraction and feature selection was done for electing the potential features using the Adaptive internal linear embedding (AILE) and uplift linear discriminant analysis (ULDA). The region of interest mask can be segmen-ted using the Recurrent-Phase Level set Segmentation. Finally, we use deep con-volution feature fusion and region of interest pooling to integrate the presently extremely sophisticated quicker Long short term memory based (LSTM) with YOLO method for object tracking system. A range of experimental findings demonstrate that our technique achieves high average accuracy with a precision of 99.7% for object detection of SSAN datasets that is considerably more than that of the other standard object detection mechanism. Our approach may therefore satisfy the true demands of night scene target detection applications. We very much believe that our method will help future research.",Object monitoring; night vision image; SSAN dataset; adaptive internal linear embedding; uplift linear discriminant analysis; recurrent-phase level set segmentation; correlation aware LSTM based yolo classifier algorithm
"Computer vision research in detecting and classifying the subtype Acute Lymphoblastic Leukemia (ALL) has contributed to computer-aided diagnosis with improved accuracy. Another contribution is to serve as an assistant and second opinion for doctors and hematologists in diagnosing the ALL subtype. Early detection can also rely on computer-aided diagnosis to determine initial treatment. The purpose of this study is to review the progress of research in the detection and classification of ALL subtypes. The method's discussion focuses on the application of deep learning to the domain of object detection and classification. Motivations, challenges, and future research recommendations are thoroughly discussed to improve understanding and progress in this field of study. The study was carried out methodically by analyzing a collection of papers on the detection and classification of ALL subtypes published in science direct, IEEE, and PubMed from 2018 to 2022. The analysis of this paper field is included in the results of the selected paper. The paper selection from among 65 papers was based on inclusion and exclusion methods. Based on research methods and objectives, papers are divided into two large groups. The first group discusses the classification of ALL subtypes, while the second group discusses the detection of ALL subtypes. The discussion of prior research reveals some challenging issues and future work, such as the limited availability of the ALL subtypes dataset, the high computational complexity of the deep learning model, and further exploration of transformers in computer vision as a reference for research gaps that can contribute to future research.",Deep learning; Cells (biology); Object detection; Feature extraction; Microscopy; Machine learning; White blood cells; Acute lymphoblastic leukemia subtypes; object detection; deep learning; CNN; blood microscopy; blood cancer
"In the future, sensors mounted on uncrewed aerial systems (UASs) will play a critical role in increasing both the speed and safety of structural inspections. Environmental and safety concerns make structural inspections and maintenance challenging when conducted using traditional methods, especially for large structures. The methods developed and tested in the laboratory need to be tested in the field on real-size structures to identify their potential for full implementation. This paper presents results from a full-scale field implementation of a novel sensor equipped with UAS to measure non-contact transverse displacement from a pedestrian bridge. To this end, the authors modified and upgraded a low-cost system that previously showed promise in laboratory and small-scale outdoor settings so that it could be tested on an in-service bridge. The upgraded UAS system uses a commodity drone platform, low-cost sensors including a laser range-finder, and a computer vision-based algorithm with the aim of measuring bridge displacements under load indicative of structural problems. The aim of this research is to alleviate the costs and challenges associated with sensor attachment in bridge inspections and deliver the first prototype of a UAS-based non-contact out-of-plane displacement measurement. This work helps to define the capabilities and limitations of the proposed low-cost system in obtaining non-contact transverse displacement in outdoor experiments.",UAS; bridge; computer vision; low-cost sensing; field implementation
Computer vision is augmented in various manufacturing industries to perform automated inspection operations accurately and efficiently. It has been observed that the performance of vision-based inspection approaches degrades considerably upon utilizing images captured under shop-floor conditions. This work proposes utilizing Histogram Equalization and adversarial training through Neural Structure Learning (NSL) for developing a robust vision-based Surface Defect Classification framework. A novel deep neural network architecture obtains adversarial samples in the extracted feature space instead of obtaining the same in the original input image space. The architecture can be easily integrated and employed with various machine learning models. A commonly employed steel surface defect dataset (NEU) with practical relevance to industrial cases is selected for the model training and experimental studies. The robustness of the proposed approach is evaluated over the Extended Diversity Enhanced (ENEU) dataset derived by simulating image acquisition variations similar to shop floor conditions. The results reveal that the proposed approach enhances the recognition accuracy of the baseline method from 87.7% to 92.4% over ENEU. The prediction accuracy of the proposed approach is considerably better than the traditional methods and deep learning competitors over ENEU. The qualitative and quantitative comparison of results obtained using the present approach with methods reported in the literature demonstrates the effectiveness of adversarial training in improving the generalization abilities of machine learning models.,Computer vision; Defect detection; Steel surface; Adversarial training; Histogram Equalization
"Self-supervised learning (SSL) has gained remarkable success, for which contrastive learning (CL) plays a key role. However, the recent development of new non-CL frameworks has achieved comparable or better performance with high improvement potential, prompting researchers to enhance these frameworks further. Assimilating CL into non-CL frameworks has been thought to be beneficial, but empirical evidence indicates no visible improvements. In view of that, this paper proposes a strategy of performing CL along the dimensional direction instead of along the batch direction as done in conventional contrastive learning, named Dimensional Contrastive Learning (DimCL). DimCL aims to enhance the feature diversity, and it can serve as a regularizer to prior SSL frameworks. DimCL has been found to be effective, and the hardness-aware property is identified as a critical reason for its success. Extensive experimental results reveal that assimilating DimCL into SSL frameworks leads to performance improvement by a non-trivial margin on various datasets and backbone architectures.",Transfer learning; Task analysis; Self-supervised learning; Learning systems; Loss measurement; Computer vision; Self-supervise learning; computer vision; contrastive learning; deep learning; transfer learning
"Skin cancers are the most cancers diagnosed worldwide, with an estimated > 1.5 million new cases in 2020. Use of computer-aided diagnosis (CAD) systems for early detection and classification of skin lesions helps reduce skin cancer mortality rates. Inspired by the success of the transformer network in natural language processing (NLP) and the deep convolutional neural network (DCNN) in computer vision, we propose an end-to-end CNN transformer hybrid model with a focal loss (FL) function to classify skin lesion images. First, the CNN extracts low-level, local feature maps from the dermoscopic images. In the second stage, the vision transformer (ViT) globally models these features, then extracts abstract and high-level semantic information, and finally sends this to the multi-layer perceptron (MLP) head for classification. Based on an evaluation of three different loss functions, the FL-based algorithm is aimed to improve the extreme class imbalance that exists in the International Skin Imaging Collaboration (ISIC) 2018 dataset. The experimental analysis demonstrates that impressive results of skin lesion classification are achieved by employing the hybrid model and FL strategy, which shows significantly high performance and outperforms the existing work.",deep learning; skin lesion; hybrid model; focal loss
"Recently introduced self-supervised methods for image representation learning provide on par or superior results to their fully supervised competitors, yet the corresponding efforts to explain the self-supervised approaches lag behind. Motivated by this observation, we introduce a novel visual probing framework for explaining the self-supervised models by leveraging probing tasks employed previously in natural language processing. The probing tasks require knowledge about semantic relationships between image parts. Hence, we propose a systematic approach to obtain analogs of natural language in vision, such as visual words, context, and taxonomy. Our proposal is grounded in Marr's computational theory of vision and concerns features like textures, shapes, and lines. We show the effectiveness and applicability of those analogs in the context of explaining self-supervised representations. Our key findings emphasize that relations between language and vision can serve as an effective yet intuitive tool for discovering how machine learning models work, independently of data modality. Our work opens a plethora of research pathways towards more explainable and transparent AI.",Computer vision; explainability; probing tasks self-supervised representation
"The growth of the Internet has led to the emergence of servers that perform increasingly heavy tasks. Some servers must remain active 24 h a day, but the evolution of network cards has facilitated the use of Data Processing Units (DPUs) to reduce network traffic and alleviate server workloads. This capability makes DPUs good candidates for load alleviation in systems that perform continuous data processing when the data can be pre-filtered. Computer vision systems that use some form of artificial intelligence, such as facial recognition or weapon detection, tend to have high workloads and high power consumption, which is becoming increasingly costly. Reducing the workload is therefore desirable and possible in some scenarios. The main contributions of this study are threefold: (1) to explore the potential benefits of using a DPU to alleviate the workload of a 24-h active server; (2) to present a study that measures the workload reduction of a CCTV weapon detection system and evaluate its performance under different conditions. We observed a 43,123% reduction in workload over the 24 h of video used in the experimentation, reaching more than 98% savings during night hours, which significantly reduces system stress and has a direct impact on electrical energy expenditure; and (3) to provide a framework that can be adapted to other computer vision-based detection systems.",BlueField-2; data processing unit (DPU); deep learning; computer vision; weapon detection; green AI; energy saving
"Presently, precision agriculture processes like plant disease, crop yield prediction, species recognition, weed detection, and irrigation can be accom-plished by the use of computer vision (CV) approaches. Weed plays a vital role in influencing crop productivity. The wastage and pollution of farmland's natural atmosphere instigated by full coverage chemical herbicide spraying are increased. Since the proper identification of weeds from crops helps to reduce the usage of herbicide and improve productivity, this study presents a novel computer vision and deep learning based weed detection and classification (CVDL-WDC) model for precision agriculture. The proposed CVDL-WDC technique intends to prop-erly discriminate the plants as well as weeds. The proposed CVDL-WDC techni-que involves two processes namely multiscale Faster RCNN based object detection and optimal extreme learning machine (ELM) based weed classification. The parameters of the ELM model are optimally adjusted by the use of farmland fertility optimization (FFO) algorithm. A comprehensive simulation analysis of the CVDL-WDC technique against benchmark dataset reported the enhanced out-comes over its recent approaches interms of several measures.",Precision agriculture; smart farming; weed detection; computer vision; deep learning
"The application of Artificial Intelligence (AI) is a popular trend to make damage inspection and analysis in structural health monitoring more intelligent and automatic. However, the existing AI-based approaches, especially vision-based methods, mainly focus on damage identification and quantification from images without further analysis to obtain structural load-carrying performance. This paper proposes a Damage-T Generative Adversarial Network (Damage-T GAN) to achieve fast translation from real-world crack images to numerical damage contours. To verify its applicability and effectiveness, two datasets from different reinforced concrete beams were built, and the performance of the trained GAN model was evaluated against the metrics IS, FID, etc. After obtaining the damage contour, a purely visual approach was applied to quantify the damage. As a result, the proposed framework greatly helps field engineers to quickly judge the damage stage of beams in site scenes by simultaneous acquisition of real-world crack images and the generated damage contours of numerical model.",Reinforced concrete beam; Damage-T GAN; Computer vision; Damage analysis; Deep learning; Structural health monitoring
"X-ray imaging technology has been used for decades in clinical tasks to reveal the internal condition of different organs, and in recent years, it has become more common in other areas such as industry, security, and geography. The recent development of computer vision and machine learning techniques has also made it easier to automatically process X-ray images and several machine learning-based object (anomaly) detection, classification, and segmentation methods have been recently employed in X-ray image analysis. Due to the high potential of deep learning in related image processing applications, it has been used in most of the studies. This survey reviews the recent research on using computer vision and machine learning for X-ray analysis in industrial production and security applications and covers the applications, techniques, evaluation metrics, datasets, and performance comparison of those techniques on publicly available datasets. We also highlight some drawbacks in the published research and give recommendations for future research in computer vision-based X-ray analysis.",X-ray imaging; Security; Computer vision; Imaging; Industrial engineering; Three-dimensional displays; Deep learning; deep learning; X-ray; industrial applications; security applications
"The most important component that can express a person's mental condition is facial expressions. A human can communicate around 55% of information non-verbally and the remaining 45% audibly. Automatic facial expression recognition (FER) has now become a challenging task in the surveying of computers. Applications of FER include understanding the behavior of humans and monitoring moods and psychological states. It even penetrates other domains-namely, robotics, criminology, smart healthcare systems, entertainment, security systems, holographic images, stress detection, and education. This study introduces a novel Robust Facial Expression Recognition using an Evolutionary Algorithm with Deep Learning (RFER-EADL) model. RFER-EADL aims to determine various kinds of emotions using computer vision and DL models. Primarily, RFER-EADL performs histogram equalization to normalize the intensity and contrast levels of the images of identical persons and expressions. Next, the deep convolutional neural network-based densely connected network (DenseNet-169) model is exploited with the chimp optimization algorithm (COA) as a hyperparameter-tuning approach. Finally, teaching and learning-based optimization (TLBO) with a long short-term memory (LSTM) model is employed for expression recognition and classification. The designs of COA and TLBO algorithms aided in the optimal parameter selection of the DenseNet and LSTM models, respectively. A brief simulation analysis of the benchmark dataset portrays the greater performance of the RFER-EADL model compared to other approaches.",image processing; facial expression recognition; computer vision; deep learning; evolutionary algorithm
"Yoga has been a great form of physical activity and one of the promising applications in personal health care. Several studies prove that yoga is used as one of the physical treatments for cancer, musculoskeletal disorder, depression, Parkinson's disease, and respiratory heart diseases. In yoga, the body should be mechanically aligned with some effort on the muscles, ligaments, and joints for optimal posture. Postural-based yoga increases flexibility, energy, overall brain activity and reduces stress, blood pressure, and back pain. Body Postural Alignment is a very important aspect while performing yogic asanas. Many yogic asanas including uttanasana, kurmasana, ustrasana, and dhanurasana, require bending forward or backward, and if the asanas are performed incorrectly, strain in the joints, ligaments, and backbone can result, which can cause problems with the hip joints. Hence it is vital to monitor the correct yoga poses while performing different asanas. Yoga posture prediction and automatic movement analysis are now possible because of advancements in computer vision algorithms and sensors. This research investigates a thorough analysis of yoga posture identification systems using computer vision, machine learning, and deep learning techniques.",Injuries; Heart rate; Machine learning; Biomedical monitoring; Ligaments; COVID-19; Deep learning; Yogic posture recognition; optimal posture; machine learning methods; deep learning methods
"Recently computer vision and NLP based techniques have been employed for document layout analysis where different types of elements in the document and their relative position are identified. This process is trickier as there are blocks which are structurally similar but semantically different such as title, text etc. This works attempts to use region-based CNN architecture (F-RCNN) for determining five different sections in the scientific articles. To improve the performance of detection algorithm, reading order is used as an additional feature and this model is known as MF-RCNN. First, an algorithm is formulated to find the reading order in documents which adopts Manhattan-layout using a color-coding scheme. Secondly, this information is fused with the input image without changing its shape. Experimental results show that MF-RCNN which uses the reading order performs better when compared with F-RCNN when tested on Publaynet dataset.",FRCNN; reading order; XY tree; multiple channels; manhattan layout
The advancement of Deep Learning and Computer Vision in the field of agriculture has been found to be an effective tool in detecting harmful plant diseases. Classification and detection of healthy and diseased crops play a very crucial role in determining the rate and quality of production. Thus the present work highlights a well-proposed novel method of detecting Tomato leaf diseases using Deep Neural Networks to strengthen agro-based industries. The present novel framework is utilized with a combination of classical Machine Learning model Principal Component Analysis (PCA) and a customized Deep Neural Network which has been named as PCA DeepNet. The hybridized framework also consists of Generative Adversarial Network (GAN) for obtaining a good mixture of datasets. The detection is carried out using the Faster Region-Based Convolutional Neural Network (F-RCNN). The overall work generated a classification accuracy of 99.60% with an average precision of 98.55%; giving a promising Intersection over Union (IOU) score of 0.95 in detection. Thus the presented work outperforms any other reported state-of-the-art.,Diseases; Deep learning; Feature extraction; Principal component analysis; Convolutional neural networks; Generative adversarial networks; Computer architecture; Crops; Tomato leaf diseases; artificial intelligence; deep learning; computer vision; generative adversarial networks; convolutional neural network; faster region-based convolutional neural network
"Navigating a mobile robot in an indoor or outdoor environment is an interesting research area for human-robot collaboration (HRC). In such a scenario, hand gesture offers some unique abilities for HRC to provide nonverbal communication between the user and the robot. This article proposes a novel real-time hand gesture recognition (HGR) technique for mobile robot control application. A compact convolutional neural network (CNN)-based HGR system, denoted as densely connected residual channel attention module (DRCAM), is proposed to recognize the vision-based hand gestures effectively. Since fingers are the most vital sign for HGR, an attention mechanism using multiscale representation is proposed, which emphasizes finger information more effectively. The proposed CNN model employs the cascading structure of residual blocks with a multiscale channel attention module to learn low- to high-level information of hand gestures. In addition, the cascading structures are connected through dense connectivity, which strengthens the feature propagation and facilitates feature reuse. Experiments are conducted on an ingenuously developed dataset and a publicly available American sign language finger-spelling (ASL-FS) benchmarked dataset to evaluate the performance of the proposed technique. The experimental result illustrates that the proposed DRCAM network outperforms the state-of-the-art methods in terms of mean accuracy (MA) using the leave-one-subject-out cross-validation (LOO CV) test. Finally, the training model is used to develop a software-based user interface system for the control of a mobile robot in a real-time environment.",Convolutional neural networks; Gesture recognition; Robots; Mobile robots; Three-dimensional displays; Computer architecture; Cameras; Convolutional neural network (CNN); dense connectivity; densely connected residual channel attention module (DRCAM); depth sensor; hand gesture recognition (HGR); human-robot collaboration (HRC); mobile robot control; multiscale attention
"There is great interest in automatically detecting road weather and understanding its impacts on the overall safety of the transport network. This can, for example, support road condition-based maintenance or even serve as detection systems that assist safe driving during adverse climate conditions. In computer vision, previous work has demonstrated the effectiveness of deep learning in predicting weather conditions from outdoor images. However, training deep learning models to accurately predict weather conditions using real-world road-facing images is difficult due to: (1) the simultaneous occurrence of multiple weather conditions; (2) imbalanced occurrence of weather conditions throughout the year; and (3) road idiosyncrasies, such as road layouts, illumination, and road objects, etc. In this paper, we explore the use of a focal loss function to force the learning process to focus on weather instances that are hard to learn with the objective of helping address data imbalances. In addition, we explore the attention mechanism for pixel-based dynamic weight adjustment to handle road idiosyncrasies using state-of-the-art vision transformer models. Experiments with a novel multi-label road weather dataset show that focal loss significantly increases the accuracy of computer vision approaches for imbalanced weather conditions. Furthermore, vision transformers outperform current state-of-the-art convolutional neural networks in predicting weather conditions with a validation accuracy of 92% and an F1-score of 81.22%, which is impressive considering the imbalanced nature of the dataset.",computer vision; deep learning; image classification; loss functions; vision transformers; weather detection; autonomous vehicles
"The accuracy of data captured by sensors highly impacts the performance of a computer vision system. To derive highly accurate data, the computer vision system must be capable of identifying critical objects and activities in the field of sensors and reconfiguring the configuration space of the sensors in real time. The majority of modern reconfiguration systems rely on complex computations and thus consume lots of resources. This may not be a problem for systems with a continuous power supply, but it can be a major set-back for computer vision systems employing sensors with limited resources. Further, to develop an appropriate understanding of the scene, the computer vision system must correlate past and present events of the scene captured in the sensor's field of view (FOV). To address the abovementioned problems, this article provides a simple yet efficient framework for a sensor's reconfiguration. The framework performs a spatiotemporal evaluation of the scene to generate adaptive activity maps, based on which the sensors are reconfigured. The activity maps contain normalized values assigned to each pixel in the sensor's FOV, called normalized pixel sensitivity, which represents the impact of activities or events on each pixel in the sensor's FOV. The temporal relationship between the past and present events is developed by utilizing standard half-width Gaussian distribution. The framework further proposes a federated optical-flow-based filter to determine critical activities in the FOV. Based on the activity maps, the sensors are re-configured to align the center of the sensors to the most sensitive area (i.e., region of importance) of the field. The proposed framework is tested on multiple surveillance and sports datasets and outperforms the contemporary reconfiguration systems in terms of multi-object tracking accuracy (MOTA).",computer vision; activity mapping; reconfiguration; multi-object tracking; spatiotemporal analysis; federated optical flow
"This work presents a novel method for motion sensor placement within smart homes. Using recordings from 3D depth cameras within six real homes, clusters are created with the resident's tracked location. The resulting clusters identify the possible position of a sensor and its field of view. By using a sequence of clusters as input to a Recurrent Neural Network, we evaluate our method on the task of activity recognition and prediction. These results are compared to using sensor events as input sequence, from motion sensors that were installed empirically in the same homes. Different clustering methods are investigated and all outperform the installed motion sensors, achieving a significant increase of prediction accuracy and F1-score.",Robot sensing systems; Motion detection; Activity recognition; Sensor placement; Older adults; Cameras; Smart homes; Assisted living; Computer vision; clustering; computer vision; sensor prediction; sensor data
"Identification of human actions from video has gathered much attention in past few years. Most of the computer vision tasks such as Health Care Activity Detection, Suspicious Activity detection, Human Computer Interactions etc. are based on the principle of activity detection. Automatic labelling of activity from videos frames is known as activity detection. With the introduction of deep networks, the process of activity detection is clustered into two groups known as hand-crafted feature based approach and automatic feature extraction approach. This paper focuses on various approaches used in recent literature based on traditional and automatic approach. Moreover, hierarchy for different approaches under them such as space based, motion based, genetic based, fuzzy based, dictionary based are discussed. With introduction of Convolutional Neural Networks and Recurrent Neural Networks, automatic learning capability from input modality makes them first choice to be implemented for activity recognition. In this paper various approaches have been analyzed according to methodology, ac-curacy, classifier and datasets.",HAR; MHI; LBP; CNN; RNN; Deep learning; Handcrafted features
"Deep Neural Networks (DNNs) trained on one dataset (source domain) do not perform well on another set of data (target domain), which is different but has similar properties as the source domain. Domain Adaptation (DA) strives to alleviate this problem and has great potential in its application in practical settings, real-world scenarios, industrial applications and many data domains. Various DA methods aimed at individual data domains have been reported in the last few years; however, there is no comprehensive survey that encompasses all these data domains, focuses on the datasets available, the methods relevant to each domain, and importantly the applications and challenges. To that end, this survey paper discusses how DA can help DNNs work efficiently in these settings by reviewing DA methods and techniques. We have considered five data domains: computer vision, natural language processing, speech, time-series, and multi-modal data. We present a comprehensive taxonomy, including the methods, datasets, challenges, and applications corresponding to each domain. Our goal is to discuss industrial use cases and DA implementation for those. Our final aim is to provide future research directions based on evolving methods and results, the datasets used, and industrial applications.",Artificial intelligence; computer vision; deep neural network; domain adaptation; multi-modal data; natural language processing
"Robots have been increasingly used in applications involving welding of large metal structures, such as the naval industry, ensuring higher efficiency and repeatability at lower costs. However, inadequate communication between the robotics and the welding system can lead to internal and surface defects in the final product. Problems that occur during welding can be detected with the help of visual inspection. In the present work a passive monocular camera was used to quantify the texture found in weld beads as part of a fully-computerised vision system. The textures identified were associated with the presence of welding discontinuities. An algorithm based on Principal Component Analysis was developed to analyse weld beads, where part of the beads was produced using conditions that purposefully resulted in welding discontinuities, identifying the most important features that characterized one group with healthy beads and another group containing discontinuities. After this stage, a machine learning method was used in new weld beads, in order to classify them as healthy or defective. The accuracy of the proposed method for texture identification was 96.4%.",Welding; Machine learning; Robots; Computer vision; Welding defects
"In the previous years, vision transformer has demonstrated a global information extraction capability in the field of computer vision that convolutional neural network (CNN) lacks. Due to the lack of inductive bias in vision transformer, it requires a large amount of data to support its training. In the field of remote sensing, it costs a lot to obtain a significant number of high-resolution remote sensing images. Most existing change detection networks based on deep learning rely heavily on the CNN, which cannot effectively utilize the long-distance dependence between pixels for difference discrimination. Therefore, this work aims to use a high-performance vision transformer to conduct change detection research with limited data. A bibranch fusion network based on axial cross attention (ACABFNet) is proposed. The network extracts local and global information of images through the CNN branch and transformer branch, respectively, and then, fuses local and global features by the bidirectional fusion approach. In the upsampling stage, similar feature information and difference feature information of the two branches are explicitly generated by feature addition and feature subtraction. Considering that the self-attention mechanism is not efficient enough for global attention over small datasets, we propose the axial cross attention. First, global attention along the height and width dimensions of images is performed respectively, and then cross attention is used to fuse the global feature information along two dimensions. Compared with the original self-attention, the structure is more graphics processing unit friendly and efficient. Experimental results on three datasets reveal that the ACABFNet outperforms existing change detection algorithms.",Attention; change detection; remote sensing image; vision transformer
"Connecting Vision and Language plays an essential role in Generative Intelligence. For this reason, large research efforts have been devoted to image captioning, i.e. describing images with syntactically and semantically meaningful sentences. Starting from 2015 the task has generally been addressed with pipelines composed of a visual encoder and a language model for text generation. During these years, both components have evolved considerably through the exploitation of object regions, attributes, the introduction of multi-modal connections, fully-attentive approaches, and BERT-like early-fusion strategies. However, regardless of the impressive results, research in image captioning has not reached a conclusive answer yet. This work aims at providing a comprehensive overview of image captioning approaches, from visual encoding and text generation to training strategies, datasets, and evaluation metrics. In this respect, we quantitatively compare many relevant state-of-the-art approaches to identify the most impactful technical innovations in architectures and training strategies. Moreover, many variants of the problem and its open challenges are discussed. The final goal of this work is to serve as a tool for understanding the existing literature and highlighting the future directions for a research area where Computer Vision and Natural Language Processing can find an optimal synergy.",Image captioning; vision-and-language; deep learning; survey
"Remote-vision-based image processing plays a vital role in the safety helmet and harness monitoring of construction sites, in which computer-vision-based automatic safety helmet and harness monitoring systems have attracted significant attention for practical applications. However, many problems have not been well solved in existing computer-vision-based systems, such as the shortage of safety helmet and harness monitoring datasets and the low accuracy of the detection algorithms. To address these issues, an attribute-knowledge-modeling-based safety helmet and harness monitoring system is constructed in this paper, which elegantly transforms safety state recognition into images' semantic attribute recognition. Specifically, a novel transformer-based end-to-end network with a self-attention mechanism is proposed to improve attribute recognition performance by making full use of the correlations between image features and semantic attributes, based on which a security recognition system is constructed by integrating detection, tracking, and attribute recognition. Experimental results for safety helmet and harness detection demonstrate that the accuracy and robustness of the proposed transformer-based attribute recognition algorithm obviously outperforms the state-of-the-art algorithms, and the presented system is robust to challenges such as pose variation, occlusion, and a cluttered background.",automated safety checking system; safety helmets and harnesses; attribute recognition based on transformer; construction site datasets
"The automatically defect detection method using vision inspection is a promising direction. In this paper, an efficient defect detection method for detecting surface damage to cables on a cable-stayed bridge automatically is developed. A mechanism design method for the protective layer of cables of a bridge based on vision inspection and diameter measurement is proposed by combining computer vision and diameter measurement techniques. A detec-tion system for the surface damages of cables is de-signed. Images of cable surfaces are then enhanced and subjected to threshold segmentation by utiliz-ing the improved local grey contrast enhancement method and the improved maximum correlation method. Afterwards, the data obtained through diame-ter measurement are mined by employing the moving average method. Image enhancement, threshold segmentation, and diameter measurement methods are separately validated experimentally. The experimental test results show that the system delivers recall ratios for type-I and II surface defects of cables reaching 80.4% and 85.2% respectively, which accurately detects bulges on cable surfaces.",Defect detection; computer vision; bridge cable; image; enhancement
"Although vision-based drowsiness detection approaches have achieved great success on empirically organized datasets, it remains far from being satisfactory for deployment in practice. One crucial issue lies in the scarcity and lack of datasets that represent the actual challenges in real-world applications, e.g. tremendous variation and aggregation of visual signs, challenges brought on by different camera positions and camera types. To promote research in this field, we introduce a new large-scale dataset, FatigueView, that is collected by both RGB and infrared (IR) cameras from five different positions. It contains real sleepy driving videos and various visual signs of drowsiness from subtle to obvious, e.g. with 17,403 different yawning sets totaling more than 124 million frames, far more than recent actively used datasets. We also provide hierarchical annotations for each video, ranging from spatial face landmarks and visual signs to temporal drowsiness locations and levels to meet different research requirements. We structurally evaluate representative methods to build viable baselines. With FatigueView, we would like to encourage the community to adapt computer vision models to address practical real-world concerns, particularly the challenges posed by this dataset.",Drowsiness detection; driver monitoring system; intelligent cockpit; autonomous vehicles
"As an essential low-level computer vision task for remotely operated underwater robots and unmanned underwater vehicles to detect and understand the underwater environment, underwater image enhancement is facing challenges of light scattering, absorption, and distortion. Instead of using a specific underwater imaging model to mitigate the degradation of underwater images, we propose an end-to-end underwater-image-enhancement framework that combines fractional integral-based Retinex and an encoder-decoder network. The proposed variant of Retinex aims to alleviate haze and color distortion in the input image while preserving edges to a large extent by utilizing a modified fractional integral filter. The encoder-decoder network with channel-wise attention modules trained in an unsupervised manner to overcome the lack of paired underwater image datasets is designed to refine the output of the Retinex. Our framework was evaluated under qualitative and quantitative metrics on several public underwater image datasets and yielded satisfactory enhancement results on the evaluation set.",underwater image enhancement; fractional integral Retinex; unsupervised autoencoder
"The health monitoring technology of transmission towers based on vibration data had become a research hotspot. At present, vibration data mainly relied on sensors installed on the tower, which was time-consuming and laborious. Nevertheless, the ROI computer vision method could achieve long-distance, multi-point, and non-contact monitoring, which offers a new possibility for the structure-safety identification of power transmission towers. However, transmission towers are generally located in the field environment, and the background is complicated, resulting in the ROI key point method for vibration data acquisition encountering various types of noise. Thus, the key point in practice was clearing the noise and reducing the impact of noise on identification accuracy. The subpixel corner method was used to detect a minor error with the research object of pixel sets. The dilation + erosion method could reduce image noise. Under white noise with a variance of 0.05, the dilation + erosion could reduce average error (E-mae) and mean square error (E-mse) by 27% and 23% and increase percentages of data with absolute error less than 5 mm and 10 mm in the total number of data (sigma(5) and sigma(10)) by 8% and 4.3%, respectively, which was compared to median filter + sharpen. The histogram equalization method was used to balance background lighting conditions and reduce identification errors from non-uniform illumination. E-mae and E-mse were reduced by 92% and 99%, and sigma(5) and sigma(10) were increased by 5 and 3 times, respectively, and the identification time was cut by 62% with the histogram equalization method. Under white noise with a variance of 0.15 or lower, the three methods combined increased the numerical stability of E-mae, E-mse, sigma(5), and sigma(10), which indicated that the combination of the three methods could improve the anti-noise performance, robustness, and identification accuracy of the ROI computer vision method for transmission tower displacement identification.",computer vision; power transmission tower; displacement identification; noise; subpixel corner; dilation; erosion; histogram equalization
"Generalizable person Re-Identification (ReID) aims to learn ready-to-use cross-domain representations for direct cross-data evaluation, which has attracted growing attention in the recent computer vision (CV) community. In this work, we construct a structural causal model (SCM) among identity labels, identity-specific factors (clothing/shoes color etc.), and domain-specific factors (background, viewpoints etc.). According to the causal analysis, we propose a novel Domain Invariant Representation Learning for generalizable person Re-Identification (DIR-ReID) framework. Specifically, we propose to disentangle the identity-specific and domain-specific factors into two independent feature spaces, based on which an effective backdoor adjustment approximate implementation is proposed for serving as a causal intervention towards the SCM. Extensive experiments have been conducted, showing that DIR-ReID outperforms state-of-the-art (SOTA) methods on large-scale domain generalization (DG) ReID benchmarks.",Generalizable person re-Identification; disentanglement; backdoor adjustment
"recent years, transformer models have revolutionized natural language processing (NLP) and shown promising performance on computer vision (CV) tasks. Despite their effectiveness, transformers' attention operations are hard to accelerate due to the complicated data movement and quadratic computational complexity, prohibiting the real-time inference on resource-constrained edge-computing platforms. To tackle this challenge, we propose Energon, an algorithm-architecture co design approach that accelerates various transformers using dynamic sparse attention. With the observation that attention results only depend on a few important query-key pairs, we propose a mix-precision multiround filtering (MP-MRF) algorithm to dynamically identify such pairs at runtime. We adopt low bitwidth in each filtering round and only use high-precision tensors in the attention stage to reduce overall complexity. By this means, we significantly mitigate the computational cost with negligible accuracy loss. To enable such an algorithm with lower latency and better energy efficiency, we also propose an Energon co-processor architecture. Elaborated pipelines and specialized optimizations jointly boost the performance and reduce power consumption. Extensive experiments on both NLP and CV benchmarks demonstrate that Energon achieves 168x and 8.7x geo-mean speedup and up to 10(4)x and 10(3)x energy reduction over Intel Xeon 5220 CPU and NVIDIA V100 GPU, respectively, Compared to state-of-the-art attention accelerators SpAtten and A(3), Energon also achieves 1.7xand 1.25x speedup, and 1.6xand 1.5x higher energy efficiency.",Transformers; Task analysis; Heuristic algorithms; Random access memory; Graphics processing units; Computational modeling; Hardware; Computer architecture; deep learning; hardware acceleration; transformers
"In Late-Medieval panel paintings from the Tuscan area, mechanical tools called punches were used to impress repeated motifs on gold foils to create decorative patterns. Such patterns can be used as clues to objectively support the attribution of the paintings, as proposed by art historian Erling S. Skaug in his decades-long study on punches. We investigate the feasibility of employing automatic pattern recognition techniques for accelerating the process of classification of punches by experts working in the field. We propose a system composed of (a) a Convolutional Neural Network for categorizing a punch contained in a frame, and (b) an additional component for uncertainty estimation, aimed at recognizing possible Out-of-Distribution (OOD) samples. After collecting a set of 14(th) century panel paintings from Tuscany, we train a Convolutional Neural Network which achieves very high test-set accuracy. As far as the uncertainty estimation is concerned, we experiment with two techniques, OpenGAN and II-loss, both exhibiting very positive results. The former seems to work better on specific data extracted from images of panel paintings, while the latter showcases a more consistent behavior when considering additional OOD data obtained randomly. These outcomes indicate that an application of our system in support of experts is feasible, although we subsequently show that additional experiments on larger datasets might be required.",Painting; Machine learning; Computer vision; Authentication; Art; Convolutional neural networks; Feature extraction; Pattern recognition; Image classification; Humanities; Digital preservation; computer vision; pattern recognition; image classification; artwork classification; artwork authentication; digital humanities
"Computer vision is now playing a vital role in modern UAV (Unmanned Aerial Vehicle) systems. However, the on-board realtime small object detection for UAVs remains challenging. This paper presents an end-to-end ViT (Vision Transformer) detector, named Sparse ROI-based Deformable DETR (SRDD), to make ViT model available to UAV on-board systems. We embed a scoring network in the transformer T-encoder to selectively prune the redundant tokens, at the same time, introduce ROI-based detection refinement module in the decoder to optimise detection performance while maintaining end-to-end detection pipeline. By using scoring networks, we compress the Transformer encoder/decoder to 1/3-layer structure, which is far slim compared with DETR. With the help of lightweight backbone ResT and dynamic anchor box, we relieve the memory insufficient of on-board SoC. Experiment on UAVDT dataset shows the proposed SRDD method achieved 50.2% mAP (outperforms Deformable DETR at least 7%). In addition, the lightweight version of SRDD achieved 51.08% mAP with 44% Pa rams reduction.",Deep learning; vision transformer; object detection; lightweight neural architecture
"Medical image segmentation is a critical step in many imaging applications. Automatic segmentation has gained extensive concern using a convolutional neural network (CNN). However, the traditional CNN-based methods fail to extract global and long-range contextual information due to local convolution operation. Transformer overcomes the limitation of CNN-based models. Inspired by the success of transformers in computer vision (CV), many researchers focus on designing the transformer-based U-shaped method in medical image segmentation. The transformer-based approach cannot effectively capture the fine-grained details. This paper proposes a dual encoder network with transformer-CNN for multi-organ segmentation. The new segmentation framework takes full advantage of CNN and transformer to enhance the segmentation accuracy. The Swin-transformer encoder extracts global information, and the CNN encoder captures local information. We introduce fusion modules to fuse convolutional features and the sequence of features from the transformer. Feature fusion is concatenated through the skip connection to smooth the decision boundary effectively. We extensively evaluate our method on the synapse multi-organ CT dataset and the automated cardiac diagnosis challenge (ACDC) dataset. The results demonstrate that the proposed method achieves Dice similarity coefficient (DSC) metrics of 80.68% and 91.12% on the synapse multi-organ CT and ACDC datasets, respectively. We perform the ablation studies on the ACDC dataset, demonstrating the effectiveness of critical components of our method. Our results match the ground-truth boundary more consistently than the existing models. Our approach gains more accurate results on challenging 2D images for multi-organ segmentation. Compared with the state-of-the-art methods, our proposed method achieves superior performance in multi-organ segmentation tasks. Graphical AbstractThe key process in medical image segmentation.",Swin-transformer; Convolutional neural network; Image segmentation; Feature fusion
"Leukocytes serve as an important barrier to healthy immunity in the body and play an important role in fighting diseases. Manual morphological examination of leukocytes is the gold standard for the diagnosis of certain diseases but is undoubtedly labour-intensive and requires a high level of expertise. Therefore, conducting research on computer-aided diagnostics is important. With the development of deep learning techniques in computer vision, an increasing number of deep learning-based methods are now being applied in the field of medical imaging. Recently, the detection transformer (DETR) model, which is based on the transformer architecture, has exhibited outstanding performances in object detection tasks and has attracted considerable attention. Our study aims to propose a pure transformer-based end-to-end object detection network based on DETR and apply it to a practical medical scenario of leukocyte detection. First, we introduce the pyramid vision transformer and deformable attention module into the DETR model to improve the model performance and convergence speed. Second, we train the improved model on the challenging Common Objects in Context dataset to obtain the pretrained weights. Third, we perform transfer learning on the modified Raabin leukocyte dataset to obtain the optimal model. The improved DETR shows a mean average precision detection performance of up to 0.961 and is therefore superior to the original DETR and convolutional neural network. The study findings are expected to be useful for the development of a transformer structural model for leukocyte detection.",improved DETR; Deep learning; Leukocyte detection; Convolutional neural network; Transformer
"Abnormal event detection is a popular research direction in the field of intelligent transportation and public safety. The features that characterize abnormal events are extracted from given video sequence through computer vision technology. Then the abnormal events in the video are automatically detected through the classification model. In order to describe the motion characteristics of events more accurately, a new feature based on motion entropy is proposed in this paper. The entropy value of motion pixels in the video frame is calculated as the input feature of the classification model. Motion entropy is suitable to regard as a feature to distinguish normal events from abnormal events due to the big differences between normal and abnormal events. In addition, an abnormal event detection model based on motion entropy and dual support vector data description (ME-DSVDD) is presented to solve the problem of insufficient sample diversity. The standard data set is tested to analyze the performance of the proposed model. The experimental results show that the proposed method can effectively improve the performance of the abnormal event detection model.",Crowd abnormal event detection; motion entropy; support vector data description; computer vision
"Dynamic modern healthcare systems rely heavily on the contributions of computer scientists. The diagnosis process is a team effort involving many people: patients, their families, healthcare providers, researchers, and policymakers. Computer technology plays a crucial role in supporting this effort by providing a number of essential services to all of these groups. In the early stages of many diseases, a diagnosis can be made automatically using a computer-aided system, with some degree of certainty. This paper presents a hybrid optimal deep learning-based model for tuberculosis disease recognition using MRI images. Several deep learning models are combined to extract the most relevant features from MRI images. In particular, we establish a combination between vision transformer (ViTs) and Efficient-Net models in order to maximize classification accuracy. We conducted experiments to investigate the accuracy of the proposed model using the Shenzhen and Montgomery data set, and found that it yielded substantially more accurate and better results than the state of-the-art works.",Deep learning; PSO algorithm; tuberculosis disease classification
"With the development of computer vision technology, many advanced computer vision methods have been successfully applied to animal detection, tracking, recognition and behavior analysis, which is of great help to ecological protection, biodiversity conservation and environmental protection. As existing datasets applied to target tracking contain various kinds of common objects, but rarely focus on wild animals, this paper proposes the first benchmark, named Wild Animal Tracking Benchmark (WATB), to encourage further progress of research and applications of visual object tracking. WATB contains more than 203,000 frames and 206 video sequences, and covers different kinds of animals from land, sea and sky. The average length of the videos is over 980 frames. Each video is manually labelled with thirteen challenge attributes including illumination variation, rotation, deformation, and so on. In the dataset, all frames are annotated with axis-aligned bounding boxes. To reveal the performance of these existing tracking algorithms and provide baseline results for future research on wild animal tracking, we benchmark a total of 38 state-of-the-art trackers and rank them according to tracking accuracy. Evaluation results demonstrate that the trackers based on deep networks perform much better than other trackers like correlation filters. Another finding on the basis of the evaluation results is that wild animals tracking is still a big challenge in computer vision community.",Computer vision; Visual object tracking; Benchmark; Wild animal tracking; Biodiversity conservation
"Vision Transformer (ViT) has fully exhibited the potential of Transformer in computer vision domain. However, the computational complexity is proportional to the input dimension which is a constant value for Transformer. Therefore, training a vision transformer network is extremely memory expensive, where a large number of intermediate activation functions and parameters are involved to compute the gradients during back-propagation. In this paper, we propose Conv-PVT (Convolution blocks + Pyramid Vision Transformer) to improve the overall performance of vision transformer. Especially, we deploy simple convolution blocks in the first layer to reduce the memory footprint by down-sampling the input. Extensive experiments (including image classification, object detection and segmentation) have been carried out on ImageNet-1k, COCO and ADE20k datasets to test the accuracy, training time, memory occupation and robustness of our model. The results demonstrate that Conv-PVT achieves comparable performances with the original PVT and outperforms ResNet and ResNetXt for some downstream vision tasks. But it shortens 60% of the training time and reduces 42% GPU (Graphics Processing Unit) memory occupation, realizing twice the inference speed of PVT.",Attention; Vision transformer; Convolution; Down-stream vision tasks
"3D point cloud registration is a fundamental problem in computer vision (CV) and computer graphics (CG). Recently, a series of learning-based algorithms have been proposed to show the advantages in regis-tration accuracy and inference speed. However, those learning-based methods usually ignore transforma-tions with constrained rotations and translations in registration. In this paper, we propose a novel hybrid optimization method to solve the constrained rotational and translational transformations. A mapping function is introduced to deal with the restrained variables in optimization. Our method achieves supe-rior performance on the Multi-View Partial Point dataset, which won the first place on the registration challenge in ICCV 2021. The method is also validated on the synthetic datasets ModelNet, ICL-NUIM, and the realistic 3DMatch dataset. We demonstrate that the global optimization methods still have great po-tential research for point cloud registration. The code is available at https://github.com/Dizzy-cell/HOUV .(c) 2022 Elsevier Ltd. All rights reserved.",Point cloud registration; Optimization
"Systems subjected to continuous operation are exposed to different failure mechanisms such as fatigue, corrosion, and temperature-related defects, which makes inspection and monitoring their health paramount to prevent a system suffering from severe damage. However, visual inspection strongly depends on a human being's experience, and so its accuracy is influenced by the physical and cognitive state of the inspector. Particularly, civil infrastructures need to be periodically inspected. This is costly, time-consuming, labor-intensive, hazardous, and biased. Advances in Computer Vision (CV) techniques provide the means to develop automated, accurate, non-contact, and non-destructive inspection methods. Hence, this paper compares two different approaches to detecting cracks in images automatically. The first is based on a traditional CV technique, using texture analysis and machine learning methods (TA + ML-based), and the second is based on deep learning (DL), using Convolutional Neural Networks (CNN) models. We analyze both approaches, comparing several ML models and CNN architectures in a real crack database considering six distinct dataset sizes. The results showed that for small-sized datasets, for example, up to 100 images, the DL-based approach achieved a balanced accuracy (BA) of similar to 74%, while the TA + ML-based approach obtained a BA > 95%. For larger datasets, the performances of both approaches present comparable results. For images classified as having crack(s), we also evaluate three metrics to measure the severity of a crack based on a segmented version of the original image, as an additional metric to trigger the appropriate maintenance response.",Automated inspection; crack detection; computer vision; deep learning; texture analysis; machine learning
"Medical image is an essential tool used in quantitative and qualitative evaluation of different diseases. Medical imaging methods such as fluorescein angiography (FA), optical coherence tomography angiography (OCTA), computed tomography (CT), optical coherence tomography (OCT), and X-ray are used for diagnosis. These imaging modalities suffer from low contrast, which leads to deterioration in the image quality. Consequently, this causes limitation in the usage of medical images in clinical routine and hindered its potential by depriving clinicians from assessing useful information that are needed in disease monitoring, treatment, progression, and decision-making. To overcome this limitation, we propose a novel local transfer function for medical image enhancement algorithm using the pixel neighborhood constraint. The proposed algorithm uses block-wise intensity distribution to generate the regional similarity index. The regional similarity index transformed each centered pixel in the block, to generate a new similarity image. An intuitive optimization algorithm is utilized to optimize the proposed algorithm parameters. Experimentation results show that the proposed LTF-NSI performs better than the state-of-the-art methods and improves the interpretability and perception of the medical images, which can provide clinicians and computer vision program with good quantitative and qualitative information.",Transfer function; Computer vision; Image enhancement; Image processing; Medical image analysis
"Human activity recognition (HAR) using drone-mounted cameras has attracted considerable interest from the computer vision research community in recent years. A robust and efficient HAR system has a pivotal role in fields like video surveillance, crowd behavior analysis, sports analysis, and human- computer interaction. What makes it challenging are the complex poses, understanding different viewpoints, and the environmental scenarios where the action is taking place. To address such complexities, in this paper, we propose a novel Sparse Weighted Temporal Attention (SWTA) module to utilize sparsely sampled video frames for obtaining global weighted temporal attention. The proposed SWTA is comprised of two parts. First, temporal segment network that sparsely samples a given set of frames. Second, weighted temporal attention, which incorporates a fusion of attention maps derived from optical flow, with raw RGB images. This is followed by a basenet network, which comprises a convolutional neural network (CNN) module along with fully connected layers that provide us with activity recognition. The SWTA network can be used as a plug-in module to the existing deep CNN architectures, for optimizing them to learn temporal information by eliminating the need for a separate temporal stream. It has been evaluated on three publicly available benchmark datasets, namely Okutama, MOD20, and Drone-Action. The proposed model has received an accuracy of 72.76%, 92.56%, and 78.86% on the respective datasets thereby surpassing the previous state-of-the-art performances by a margin of 25.26%, 18.56%, and 2.94%, respectively.(c) 2022 Elsevier Ltd. All rights reserved.",Action recognition; Sparse weighted temporal attention; Drone vision
"Over the last three decades, computer vision has had a vital role in the healthcare sector by providing soft computing-based robust and intelligent diagnostic solutions. Glaucoma is a critical ophthalmic disease that can trigger irreversible loss of vision. The number of patients with glaucoma is increasing dramatically worldwide. Manual ophthalmic assessment of glaucoma detection is a tedious, error-prone, time-consuming, and subjective task. Therefore, computer-assisted automatic glaucoma diagnosis methods are required to strengthen existing diagnostic methods with their robust performance. Optic disc (OD) and optic cup (OC) segmentation have a key role in glaucoma detection. Accurate segmentation of the OD and OC provides valuable computational and clinical details that can substantially assist in the glaucoma screening process. Retinal fundus images have extensive variations in terms of size, shape, pixel intensity values, and background effects that make segmentation challenging. To mitigate these challenges, we developed two novel networks for accurate OD and OC segmentation. An efficient shallow segmentation network (ESS-Net) is the base network whereas a feature-blending-based shallow segmentation network (FBSS-Net) is the final network of this work. ESS-Net is a shallow architecture with a maximum-depth semantic preservation block for accurate segmentation, while FBSS-Net uses internal and external feature blending to improve overall segmentation performance.To confirm their effectiveness, we evaluated both networks using four publicly available datasets; REFUGE, Drions-DB, Drishti-GS, and Rim-One-r3. The proposed methods exhibited excellent segmen-tation performance, requiring a small number of trainable parameters (3.02 million parameters).(c) 2022 The Author(s). Published by Elsevier B.V. This is an open access article under the CC BY license (http://creativecommons.org/licenses/by/4.0/).",Deep learning; Optic disc and optic cup segmentation; Glaucoma diagnosis; Computer-assisted diagnosis; ESS-Net and FBSS-Net
"It is an indisputable dogma in extremity radiography to acquire x-ray studies in at least two complementary projections, which is also true for distal radius fractures in children. However, there is cautious hope that computer vision could enable breaking with this tradition in minor injuries, clinically lacking malalignment. We trained three different state-of-the-art convolutional neural networks (CNNs) on a dataset of 2,474 images: 1,237 images were posteroanterior (PA) pediatric wrist radiographs containing isolated distal radius torus fractures, and 1,237 images were normal controls without fractures. The task was to classify images into fractured and non-fractured. In total, 200 previously unseen images (100 per class) served as test set. CNN predictions reached area under the curves (AUCs) up to 98% [95% confidence interval (CI) 96.6%-99.5%], consistently exceeding human expert ratings (mean AUC 93.5%, 95% CI 89.9%-97.2%). Following training on larger data sets CNNs might be able to effectively rule out the presence of a distal radius fracture, enabling to consider foregoing the yet inevitable lateral projection in children. Built into the radiography workflow, such an algorithm could contribute to radiation hygiene and patient comfort.",wrist; fracture; radiography; artificial intelligence; radius
"Graph matching is an essential problem in computer science and communications. It can be applied to a variety of issues such as artificial intelligence, computer vision, and communication systems. In this paper, we propose a new Graphics Processing Unit framework written in CUDA C++ specifically dedicated to geometric graph matching but providing new parallel algorithms, with low computational complexity, as the self-organizing map in the plane, and a distributed local search method. Unlike state-of-the-art graph matching algorithms, available from Matlab platform, that most often need at least O(N2) memory size, with N the problem size, our proposals only require O(N) space and allows massively parallel execution. These parallel algorithms are evaluated and compared to the state-of-the-art methods available for graph matching and following the same experimental protocol.",Graph matching; Distributed local search; Self-organizing map; GPU
"The texture is the most fundamental aspect of a picture that contributes to its recognition. Computer vision challenges such as picture identification and segmentation are built on the foundation of texture analysis. Various images of satellite, forestry, medical etc. have been identifiable because of textures. This work aims to offer texture classification models that will outperform previously presented methods. In this work, transfer learning was applied to attain this goal. MobileNetV3 and InceptionV3 are the two pre-trained models employed. Brodatz, Kylberg, and Outex texture datasets were used to evaluate the models. The models achieved excellent results and achieved the objective in most cases. Classification accuracy obtained for the Kylberg dataset were 100% and 99.89%. For the Brodatz dataset, the classification accuracy obtained was 99.83% and 99.94%. For the Outex datasets, the classification accuracy obtained was 99.48% and 99.48%. The model outputs the corresponding label of the texture of the image.",Texture classification; Computer vision; Transfer learning; MobileNetV3; InceptionV3; Deep learning
"The interpretability of the model is a hot issue in the field of computer vision. Score-CAM is a kind of interpretable CAM method with good discrimination and gradient free calculation. It is a representative work in this field. However, it has the disadvantages of long calculation time and incomplete heatmap coverage. Therefore, this paper proposes an improved Score-CAM method named FIMF Score-CAM, which can fast integrate multiple features. Its contribution is reflected in two aspects: The weight of the feature map is directly calculated by using the feature template. Unlike Score-CAM, this model greatly saves computation time because it only requires one convolutional calculation. Another contribution is that the feature map used to generate the heatmap integrates various semantic features of the local space, so that the heatmap of the object of interest can be generated with more complete coverage and better interpretation. The FIMF Score-CAM is superior to the existing mainstream models in interpreting the visual performance and fairness indicators of the decision-making, having more complete explanations of object classes and the advantage of faster calculation speed.",class activation mapping; computer vision; deep network; model interpretation
"Introduction: The seriously degraded fogging image affects the further visual tasks. How to obtain a fog-free image is not only challenging, but also important in computer vision. Recently, the vision transformer (ViT) architecture has achieved very efficient performance in several vision areas. Methods: In this paper, we propose a new transformer-based progressive residual network. Different from the existing single-stage ViT architecture, we recursively call the progressive residual network with the introduction of swin transformer. Specifically, our progressive residual network consists of three main components: the recurrent block, the transformer codecs and the supervise fusion module. First, the recursive block learns the features of the input image, white connecting the original image features of the original iteration. Then, the encoder introduces the swin transformer block to encode the feature representation of the decomposed block, and continuously reduces the feature mapping resolution to extract remote context features. The decoder recursively selects and fuses image features by combining attention mechanism and dense residual blocks. In addition, we add a channel attention mechanism between codecs to focus on the importance of different features. Results and discussion: The experimental results show that the performance of this method outperforms state-of-the-art handcrafted and learning-based methods.",transformer; residual network; image dehazing; progressive recurrent; multiple self-attention
"In this paper, the existing postearthquake performance assessment framework for reinforced concrete (RC) building structures is improved by adding a new feature of the computer vision-based damage detection. In this framework, visible seismic damage is classified and quantified from photographs of damaged RC components using the developed deep convolutional network (CNN) Damage-Net of semantic segmentation, and then the mechanical property degradation factors of components determined from the detected damage states are used to update the numerical model. Pushover analysis of the updated model assesses the residual capacity of the damaged structure. Large-scale shaking table tests of a three-story RC building structure, which was heavily instrumented with sensors and recorded with a large volume of photographs, were used as a case study to demonstrate the improved postearthquake performance assessment framework. The vision-based approach accurately detected multicategory seismic damage of the test structure and effectively estimated the residual crack widths and angles under various lighting, image acquisition, and surface conditions. The updated model, which incorporated the mechanical property degradation of the damaged components, provided accurate estimate on the fundamental vibrational frequencies of the damaged structure after various levels of seismic motion shaking, which matched well with the system identification results. Using the mechanical property reduction factor values recommended by FEMA 306 & Chiu et al., pushover analysis of the updated models provided residual capacity curves that reasonably captured the measured hysteretic responses of the structure. In addition, the damage states of components as estimated by the vision-based methods were also compared with the measured plastic hinge rotation data. The successful implementation of the vision-based assessment in this test case indicates its potential for application in the postearthquake evaluation of buildings.",computer vision; damage detection; postearthquake performance assessment; residual capacity; shaking table test
"Roads can be significant traffic lifelines that can be damaged by collapsed tree branches, landslide rubble, and buildings debris. Thus, road damage detection and evaluation by utilizing High-Resolution Remote Sensing Images (RSI) are highly important to maintain routes in optimal conditions and execute rescue operations. Detecting damaged road areas through high-resolution aerial images could promote faster and effectual disaster management and decision making. Several techniques for the prediction and detection of road damage caused by earthquakes are available. Recently, computer vision (CV) techniques have appeared as an optimal solution for road damage automated inspection. This article presents a new Road Damage Detection modality using the Hunger Games Search with Elman Neural Network (RDD-HGSENN) on High-Resolution RSIs. The presented RDD-HGSENN technique mainly aims to determine road damages using RSIs. In the presented RDD-HGSENN technique, the RetinaNet model was applied for damage detection on a road. In addition, the RDD-HGSENN technique can perform road damage classification using the ENN model. To tune the ENN parameters automatically, the HGS algorithm was exploited in this work. To examine the enhanced outcomes of the presented RDD-HGSENN technique, a comprehensive set of simulations were conducted. The experimental outcomes demonstrated the improved performance of the RDD-HGSENN technique with respect to recent approaches in relation to several measures.",hunger games search algorithm; remote sensing; road damage detection; computer vision; deep learning; object detection
"In this paper, we propose a methodology to accurately evaluate and compare the performance of efficient neural network building blocks for computer vision in a hardware-aware manner. Our comparison uses pareto fronts based on randomly sampled networks from a design space to capture the underlying accuracy/complexity trade-offs. We show that our approach enables matching of information obtained by previous comparison paradigms, but provides more insights into the relationship between hardware cost and accuracy. We use our methodology to analyze different building blocks and evaluate their performance on a range of embedded hardware platforms. This highlights the importance of benchmarking building blocks as a preselection step in the design process of a neural network. We show that choosing the right building block can speed up inference by up to a factor of two on specific hardware ML accelerators.",hardware-aware deep learning; edge computing
"Unmanned Aerial Systems (UAS) are becoming more attractive in diverse applications due to their efficiency in performing tasks with a reduced time execution, covering a larger area, and lowering human risks at harmful tasks. In the context of Oil & Gas (O&G), the scenario is even more attractive for the application of UAS for inspection activities due to the large extension of these facilities and the operational risks involved in the processes. Many authors proposed solutions to detect gas leaks regarding the onshore unburied pipeline structures. However, only a few addressed the navigation and tracking problem for the autonomous navigation of UAS over these structures. Most proposed solutions rely on traditional computer vision strategies for tracking. As a drawback, depending on lighting conditions, the obtained path line may be inaccurate, making a strategy to force the UAS to continue on the path necessary. Therefore, this research describes the potential of an autonomous UAS based on image processing technique and Convolutional Neural Network (CNN) strategy to navigate appropriately in complex unburied pipeline networks contributing to the monitoring procedure of the Oil & Gas Industry structures. A CNN is used to detect the pipe, while image processing techniques such as Canny edge detection and Hough Transform are used to detect the pipe line reference, which is used by a line following algorithm to guide the UAS along the pipe. The framework is assessed by a PX4 flight controller Software-in-The-Loop (SITL) simulations performed with the Robot Operating System (ROS) along with the Gazebo platform to simulate the proposed operational environment and verify the approach's functionality as a proof of concept. Real tests were also conducted. The results showed that the solution is robust and feasible to deploy in this proposed task, achieving 72% of mean average precision on detecting different types of pipes and 0.0111 m of mean squared error on the path following with a drone 2 m away from a tube.",automatic inspection; pipe inspection; unmanned aerial system; computer vision
"Neural networks (NNs) have demonstrated their potential in a variety of domains ranging from computer vision (CV) to natural language processing. Among various NNs, two-dimensional (2-D) and three-dimensional (3-D) convolutional NNs (CNNs) have been widely adopted for a broad spectrum of applications, such as image classification and video recognition, due to their excellent capabilities in extracting 2-D and 3-D features. However, standard 2-D and 3-D CNNs are not able to capture their model uncertainty which is crucial for many safety-critical applications, including healthcare and autonomous driving. In contrast, Bayesian CNNs (BayesCNNs), as a variant of CNNs, have demonstrated their ability to express uncertainty in their prediction via a mathematical grounding. Nevertheless, BayesCNNs have not been widely used in industrial practice due to their compute requirements stemming from sampling and subsequent forward passes through the whole network multiple times. As a result, these requirements significantly increase the amount of computation and memory consumption in comparison to standard CNNs. This article proposes a novel field-programmable gate array (FPGA)-based hardware architecture to accelerate both 2-D and 3-D BayesCNNs based on Monte Carlo dropout (MCD). Compared with other state-of-the-art accelerators for BayesCNNs, the proposed design can achieve up to four times higher energy efficiency and nine times better compute efficiency. An automatic framework capable of supporting partial Bayesian inference is proposed to explore the tradeoff between algorithm and hardware performance. Extensive experiments are conducted to demonstrate that our framework can effectively find the optimal implementations in the design space.",Bayesian convolutional neural network (BayesCNN); deep learning; field-programmable gate array (FPGA); three-dimensional convolutional neural network (3-D CNN)
"In medical and health sciences, the detection of cell injury plays an important role in diagnosis, personal treatment and disease prevention. Despite recent advancements in tools and methods for image classification, it is challenging to classify cell images with higher precision and accuracy. Cell classification based on computer vision offers significant benefits in biomedicine and healthcare. There have been studies reported where cell classification techniques have been complemented by Artificial Intelligence-based classifiers such as Convolutional Neural Networks. These classifiers suffer from the drawback of the scale of computational resources required for training and hence do not offer real-time classification capabilities for an embedded system platform. Field Programmable Gate Arrays (FPGAs) offer the flexibility of hardware reconfiguration and have emerged as a viable platform for algorithm acceleration. Given that the logic resources and on-chip memory available on a single device are still limited, hardware/software co-design is proposed where image pre-processing and network training were performed in software, and trained architectures were mapped onto an FPGA device (Nexys4DDR) for real-time cell classification. This paper demonstrates that the embedded hardware-based cell classifier performs with almost 100% accuracy in detecting different types of damaged kidney cells.",artificial neural networks; cell classification; FPGAs; hardware accelerators; human kidney-damaged cells
"Artificial intelligence as an approach to visual inspection in industrial applications has been considered for decades. Recent successes, driven by advances in deep learning, present a potential paradigm shift and have the potential to facilitate an automated visual inspection, even under complex environmental conditions. Thereby, convolutional neural networks (CNN) have been the de facto standard in deep-learning-based computer vision (CV) for the last 10 years. Recently, attention-based vision transformer architectures emerged and surpassed the performance of CNNs on benchmark datasets, regarding regular CV tasks, such as image classification, object detection, or segmentation. Nevertheless, despite their outstanding results, the application of vision transformers to real world visual inspection is sparse. We suspect that this is likely due to the assumption that they require enormous amounts of data to be effective. In this study, we evaluate this assumption. For this, we perform a systematic comparison of seven widely-used state-of-the-art CNN and transformer based architectures trained in three different use cases in the domain of visual damage assessment for railway freight car maintenance. We show that vision transformer models achieve at least equivalent performance to CNNs in industrial applications with sparse data available, and significantly surpass them in increasingly complex tasks.",deep learning; computer vision; vision transformer; attention mechanism; automated industrial visual inspection; defect detection
"In sand-dust environments, the low quality of images captured outdoors adversely affects many remote-based image processing and computer vision systems, because of severe color casts, low contrast, and poor visibility of sand-dust images. In such cases, conventional color correction methods do not guarantee appropriate performance in outdoor computer vision applications. In this paper, we present a novel color correction and dehazing algorithm for sand-dust image enhancement. First, we propose an effective color correction method that preserves the consistency of the chromatic variances and maintains the coincidence of the chromatic means. Next, a transmission map for image dehazing is estimated using the gamma correction for the enhancement of color-corrected sand-dust images. Finally, a cross-correlation-based chromatic histogram shift algorithm is proposed to reduce the reddish artifacts in the enhanced images. We performed extensive experiments for various sand-dust images and compared the performance of the proposed method to that of several existing state-of-the-art enhancement methods. The simulation results indicated that the proposed enhancement scheme outperforms the existing approaches in terms of both subjective and objective qualities.",sand-dust image enhancement; chromatic variance consistency; dehazing; gamma correction; cross-correlation; color correction
"Deep neural networks have proven to be effective in solving computer vision and natural language processing problems. To fully leverage its power, manually designed network templates, i.e., Residual Networks, are introduced to deal with various vision and natural language tasks. These hand-crafted neural networks rely on a large number of parameters, which are both data-dependent and laborious. On the other hand, architectures suitable for specific tasks have also grown exponentially with their size and topology, which prohibits brute force search. To address these challenges, this paper proposes a quantum dynamic optimization algorithm to find the optimal structure for a candidate network using Quantum Dynamic Neural Architecture Search (QDNAS). Specifically, the proposed quantum dynamics optimization algorithm is used to search for meaningful architectures for vision tasks and dedicated rules to express and explore the search space. The proposed quantum dynamics optimization algorithm treats the iterative evolution process of the optimization over time as a quantum dynamic process. The tunneling effect and potential barrier estimation in quantum mechanics can effectively promote the evolution of the optimization algorithm to the global optimum. Extensive experiments on four benchmarks demonstrate the effectiveness of QDNAS, which is consistently better than all baseline methods in image classification tasks. Furthermore, an in-depth analysis is conducted on the searchable networks that provide inspiration for the design of other image classification networks.",quantum dynamics; global optimization; neural architecture search; image classification
"The development of deep learning technologies inevitably generates new tasks and their solutions in areas, such as computer vision, VR/AR technologies, video analytics, multimodal learning, etc. With increasing availability of high-performance computers, many modern methods and tools for digital data processing become widely applicable, including in personal application research. This tendency can easily be traced in the increasing number of open-source solutions that can easily be executed at well-known resources, such as Google Colab. In this paper, we describe results regarding the development and study of breakthrough technologies of high-quality multimedia synthesis, which have wide applications in tasks, such as face swapping.",face swap; GHOST; one shot; image synthesis; video synthesis
"MotivationImage dehazing, as a key prerequisite of high-level computer vision tasks, has gained extensive attention in recent years. Traditional model-based methods acquire dehazed images via the atmospheric scattering model, which dehazed favorably but often causes artifacts due to the error of parameter estimation. By contrast, recent model-free methods directly restore dehazed images by building an end-to-end network, which achieves better color fidelity. To improve the dehazing effect, we combine the complementary merits of these two categories and propose a physical-model guided self-distillation network for single image dehazing named PMGSDN. Proposed methodFirst, we propose a novel attention guided feature extraction block (AGFEB) and build a deep feature extraction network by it. Second, we propose three early-exit branches and embed the dark channel prior information to the network to merge the merits of model-based methods and model-free methods, and then we adopt self-distillation to transfer the features from the deeper layers (perform as teacher) to shallow early-exit branches (perform as student) to improve the dehazing effect. ResultsFor I-HAZE and O-HAZE datasets, better than the other methods, the proposed method achieves the best values of PSNR and SSIM being 17.41dB, 0.813, 18.48dB, and 0.802. Moreover, for real-world images, the proposed method also obtains high quality dehazed results. ConclusionExperimental results on both synthetic and real-world images demonstrate that the proposed PMGSDN can effectively dehaze images, resulting in dehazed results with clear textures and good color fidelity.",image dehazing; knowledge distillation; attention mechanism; deep learning; computer vision
"Human skin detection is the main task for various human-computer interaction applications. For this, several computer vision-based approaches have been developed in recent years. However, different events and features can interfere in the segmentation process, such as luminosity conditions, skin tones, complex backgrounds, and image capture equipment. In digital imaging, skin segmentation methods can overcome these challenges or at least part of them. However, the images analyzed follow an application-specific pattern. In this paper, we present an approach that uses a set of methods to segment skin and non-skin pixels in images from uncontrolled or unknown environments. Our main result is the ability to segment skin and non-skin pixels in digital images from a non-restrained capture environment. Thus, it overcomes several challenges, such as lighting conditions, compression, and scene complexity. By applying a segmented image examination approach, we determine the proportion of skin pixels present in the image by considering only the objects of interest (i.e., the people). In addition, this segmented analysis can generate independent information regarding each part of the human body. The proposed solution produces a dataset composed of a combination of other datasets present in the literature, which enables the construction of a heterogeneous set of images.",skin segmentation; skin detection; computer vision; digital image processing
"Human action recognition and posture prediction aim to recognize and predict respectively the action and postures of persons in videos. They are both active research topics in computer vision community, which have attracted considerable attention from academia and industry. They are also the precondition for intelligent interaction and human-computer cooperation, and they help the machine perceive the external environment. In the past decade, tremendous progress has been made in the field, especially after the emergence of deep learning technologies. Hence, it is necessary to make a comprehensive review of recent developments. In this paper, firstly, we attempt to present the background, and then discuss research progresses. Secondly, we introduce datasets, various typical feature representation methods, and explore advanced human action recognition and posture prediction algorithms. Finally, facing the challenges in the field, this paper puts forward the research focus, and introduces the importance of action recognition and posture prediction by taking interactive cognition in self-driving vehicle as an example.",Deep learning; Support vector machines; Computer vision; Conferences; Cognition; Pattern recognition; Topology; human action recognition; posture prediction; computer vision; human-computer cooperation; interactive cognition
"Transfer learning enables to re-use knowledge learned on a source task to help learning a target task. A simple form of transfer learning is common in current state-of-the-art computer vision models, i.e., pre-training a model for image classification on the ILSVRC dataset, and then fine-tune on any target task. However, previous systematic studies of transfer learning have been limited and the circumstances in which it is expected to work are not fully understood. In this paper we carry out an extensive experimental exploration of transfer learning across vastly different image domains (consumer photos, autonomous driving, aerial imagery, underwater, indoor scenes, synthetic, close-ups) and task types (semantic segmentation, object detection, depth estimation, keypoint detection). Importantly, these are all complex, structured output tasks types relevant to modern computer vision applications. In total we carry out over 2000 transfer learning experiments, including many where the source and target come from different image domains, task types, or both. We systematically analyze these experiments to understand the impact of image domain, task type, and dataset size on transfer learning performance. Our study leads to several insights and concrete recommendations: (1) for most tasks there exists a source which significantly outperforms ILSVRC'12 pre-training; (2) the image domain is the most important factor for achieving positive transfer; (3) the source dataset should include the image domain of the target dataset to achieve best results; (4) at the same time, we observe only small negative effects when the image domain of the source task is much broader than that of the target; (5) transfer across task types can be beneficial, but its success is heavily dependent on both the source and target task types.",Task analysis; Transfer learning; Semantics; Image segmentation; Computer vision; Training; Object detection; Transfer learning; computer vision
"The global population is aging due to many factors, including longer life expectancy through better healthcare, changing diet, physical activity, etc. We are also witnessing various frequent epidemics as well as pandemics. The existing healthcare system has failed to deliver the care and support needed to our older adults (seniors) during these frequent outbreaks. Sophisticated sensor-based in-home care systems may offer an effective solution to this global crisis. The monitoring system is the key component of any in-home care system. The evidence indicates that they are more useful when implemented in a non-intrusive manner through different visual and audio sensors. Artificial Intelligence (AI) and Computer Vision (CV) techniques may be ideal for this purpose. Since the RGB imagery-based CV technique may compromise privacy, people often hesitate to utilize in-home care systems which use this technology. Depth, thermal, and audio-based CV techniques could be meaningful substitutes here. Due to the need to monitor larger areas, this review article presents a systematic discussion on the state-of-the-art using depth sensors as primary data-capturing techniques. We mainly focused on fall detection and other health-related physical patterns. As gait parameters may help to detect these activities, we also considered depth sensor-based gait parameters separately. The article provides discussions on the topic in relation to the terminology, reviews, a survey of popular datasets, and future scopes.",classification of sensor data; computer vision; depth imagery; fall detection; gait analysis; HAR; smart home; survey
"The study of automated video surveillance systems study using computer vision techniques is a hot research topic and has been deployed in many real-world CCTV environments. The main focus of the current systems is higher accuracy, while the assistance of surveillance experts in effective data analysis and instant decision making using efficient computer vision algorithms need researchers' attentions. In this research, to the best of our knowledge, we are the first to introduce a process control technique: control charts for surveillance video data analysis. The control charts concept is merged with a novel deep learning-based violence detection framework. Different from the existing methods, the proposed technique considers the importance of spatial information, as well as temporal representations of the input video data, to detect human violence. The spatial information are fused with the temporal dimension of the deep learning model using a multi-scale strategy to ensure that the temporal information are properly assisted by the spatial representations at multi-levels. The proposed frameworks' results are kept in the history-maintaining module of the control charts to validate the level of risks involved in the live input surveillance video. The detailed experimental results over the existing datasets and the real-world video data demonstrate that the proposed approach is a prominent solution towards automated surveillance with the pre- and post-analyses of violent events.",anomaly detection; fight detection; video classification; surveillance video analysis
"Recently, the construction industry has been digitizing its production processes, the so-called Construction 4.0, in allusion to the paradigm of the fourth industrial revolution. The application of Deep Learning in computer vision systems has been highlighted in Construction 4.0. Thus, the main contribution of this work is to present a systematic review of Deep Learning for vision systems under Construction 4.0, considering the most cited and most recent journal articles between 2017 and 2021 from Scopus database. For this, a research method selected and analyzed 76 published papers. Six main points were evaluated in the proposed methodology: study area, computer vision applications, Deep Learning methods, hyperparameter tuning, data augmentation, and future work. The following topics stand out as relevant perspectives and directions for continued advancement in this field of research: improving Deep Learning models, increasing the quality of databases, investigating the generality techniques and optimizing processing capacity.",Deep Learning; Computer vision systems; Construction 4; 0
"Bolts, as the basic units of tunnel linings, are crucial to safe tunnel service. Caused by the moist and complex environment in the tunnel, corrosion becomes a significant defect of bolts. Computer vision technology is adopted because manual patrol inspection is inefficient and often misses the corroded bolts. However, most current studies are conducted in a laboratory with good lighting conditions, while their effects in actual practice have yet to be considered, and the accuracy also needs to be improved. In this paper, we put forward an Ensemble Learning approach combining our Improved MultiScale Retinex with Color Restoration (IMSRCR) and You Only Look Once (YOLO) based on truly acquired tunnel image data to detect corroded bolts in the lining. The IMSRCR sharpens and strengthens the features of the lining pictures, weakening the bad effect of a dim environment compared with the existing MSRCR. Furthermore, we combine models with different parameters that show different performance using the ensemble learning method, greatly improving the accuracy. Sufficient comparisons and ablation experiments based on a dataset collected from the tunnel in service are conducted to prove the superiority of our proposed algorithm.",corroded bolt detection; computer vision; color enhancement; ensemble learning
"Innovative concrete structure maintenance now requires automated computer vision inspection. Modern edge computing devices (ECDs), such as smartphones, can serve as sensing and computational platforms and can be integrated with deep learning models to detect on-site damage. Due to the fact that ECDs have limited processing power, model sizes should be reduced to improve efficiency. This study compared and analyzed the performance of five semantic segmentation models that can be used for damage detection. These models are categorized as lightweight (ENet, CGNet, ESNet) and heavyweight (DDRNet-Slim23, DeepLabV3+ (ResNet-50)), based on the number of model parameters. All five models were trained and tested on the concrete structure dataset considering four types of damage: cracks, efflorescence, rebar exposure, and spalling. Overall, based on the performance evaluation and computational cost, CGNet outperformed the other models and was considered effective for the on-site damage detection application of ECDs.",computer vision; edge computing device; deep learning; lightweight models; damage detection
"Computer vision algorithms play a vital role in developing self-sustained autonomous systems. The objective of the present work is to integrate the robotic system with a moving conveyor using a single camera by adopting a Gaussian Mixture Model (GMM) based background subtraction method. In this work, a simple web camera is placed above the work cell to capture the continuous images of the moving objects on the conveyor along with a jointed arm robot connected to a microcontroller through the computer. The position of the object with time and its features are extracted from the captured image frames by subtracting its background using the Gaussian Mixture Model (GMM). The output images of GMM are further processed by image processing techniques to extract the features like shape, color, and center coordinates. The extracted coordinates of objects of interest are used as input parameters to the controller to activate the base rotation of a joint arm robot to perform different manipulations. The developed algorithm is evaluated on an indigenously fabricated work cell integrated with a computer vision setup.",Computer vision; Control; Conveyor; Robot; GMM
"Pedestrian attribute recognition (PAR) and re-identification (ReID) are important works in the area of computer vision, which are widely used in intelligent surveillance and are of great significance to the creation of smart life. The purpose of this article is to focus on organizing a review of ReID based on deep learning and analyze the associations between PAR and ReID. Firstly, we summarize the major ideas of Attribute-Assisted ReID and compare the differences in datasets and algorithmic concerns between the two areas. Secondly, we introduce a wide range of representative ReID methods. By analyzing some cutting-edge researches, we summarize their specific network structure, loss function design, and effective training tricks. Reference methods and solutions are provided for the main challenges of ReID, such as cloth-changing, domain adaptation, occlusion condition, resolution changes, etc. Finally, we conclude the performance and characteristics of the SOTA methods, obtain inspiration and prospects for future research directions, and demonstrate the effectiveness of Attribute-Assisted ReID.",Computer vision; Pedestrian attribute recognition; Person re-identification
"Neural architecture search (NAS) has attracted increasing attention. In recent years, individual search methods have been replaced by weight-sharing search methods for higher search efficiency, but the latter methods often suffer lower instability. This article provides a literature review on these methods and owes this issue to the optimization gap. From this perspective, we summarize existing approaches into several categories according to their efforts in bridging the gap, and we analyze both advantages and disadvantages of these methodologies. Finally, we share our opinions on the future directions of NAS and AutoML. Due to the expertise of the authors, this article mainly focuses on the application of NAS to computer vision problems.",AutoML; neural architecture search; weight-sharing; super-network; optimization gap; computer vision
"Artificial intelligence (AI) provides advanced mathematical frameworks and algorithms for further innovation and vitality of classical civil engineering (CE). Plenty of complex, time-consuming, and laborious workloads of design, construction, and inspection can be enhanced and upgraded by emerging AI techniques. In addition, many unsolved issues and unknown laws in the field of CE can be addressed and discovered by physical machine learning via merging the data paradigm with physical laws. Intelligent science and technology in CE profoundly promote the current level of informatization, digitalization, autonomation, and intellectualization. To this end, this paper provides a systematic review and summarizes the state-of-the-art progress of AI in CE for the entire life cycle of civil structures and infrastructure, including intelligent architectural design, intelligent structural health diagnosis, intelligent disaster prevention and reduction. A series of examples for intelligent architectural art shape design, structural topology optimization, computer-vision-based structural damage recognition, correlation-pattern-based structural condition assessment, machine-learning-enhanced reliability analysis, vision-based earthquake disaster evaluation, and dense displacement monitoring of structures under wind and earthquake, are given. Finally, the prospects of intelligent science and technology in future CE are discussed.",artificial intelligence; machine learning; computer vision; architectural design; structural health diagnosis; seismic disaster evaluation
"Structural health monitoring systems that employ vision data are under constant development. Generating synthetic vision data is an actual issue. It allows, for example, for obtention of additional data for machine learning techniques or predicting the result of observations using a vision system with a reduced number of experiments. A random speckle pattern (RSP) fixed on the surface of the observed structure is usually used in measurements. The determination of displacements of its areas using digital image correlation (DIC) methods allows for extracting the structure's deformation in both static and dynamic cases. An RSP modeling methodology for synthetic image generation is developed within this paper. The proposed approach combines the finite element modeling technique and simulation results with the Blender graphics environment to generate video sequences of the mechanical structure with deformable RSP attached to it. The comparative analysis showed high compliance of the displacement between the synthetic images processed with the DIC method and numerical data.",finite element analysis; vision systems; rendering; random speckle patterns; Blender
"The use of 360 degrees omnidirectional images has occurred widely in areas where comprehensive visual information is required due to their large visual field coverage. However, many extant convolutional neural networks based on 360 degrees omnidirectional images have not performed well in computer vision tasks. This occurs because 360 degrees omnidirectional images are processed into plane images by equirectangular projection, which generates discontinuities at the edges and can result in serious distortion. At present, most methods to alleviate these problems are based on multi-projection and resampling, which can result in huge computational overhead. Therefore, a novel edge continuity distortion-aware block (ECDAB) for 360 degrees omnidirectional images is proposed here, which prevents the discontinuity of edges and distortion by recombining and segmenting features. To further improve the performance of the network, a novel convolutional row-column attention block (CRCAB) is also proposed. CRCAB captures row-to-row and column-to-column dependencies to aggregate global information, enabling stronger representation of the extracted features. Moreover, to reduce the memory overhead of CRCAB, we propose an improved convolutional row-column attention block (ICRCAB), which can adjust the number of vectors in the row-column direction. Finally, to verify the effectiveness of the proposed networks, we conducted experiments on both traditional images and 360 degrees omnidirectional image datasets. The experimental results demonstrated that better performance than for the baseline model was obtained by the network using ECDAB or CRCAB.",computer vision; object detection; 360 degrees omnidirectional images; row-column attention mechanism
"Object detection is a type of application that includes computer vision and image processing technologies, which deal with detecting, tracking, and classifying desired objects in images. Computer vision is a field of artificial intelligence that enables computers and systems to derive information from digital images and take action or suggestions based on that information. CNN is one of the current methods of object detection due to its ease of use and GPU-supported parallel working features. Due to the aim of completing deep learning model training quickly or due to insufficient dataset, many studies using the transfer learning method are carried out in fields such as medicine, agriculture, and weapons. However, there are very few studies that use the fine-tuning method and compare transfer learning in terms of effectiveness. By paying attention to the balanced distribution of the data, approximately 100 images of each chess piece type were included in the analysis and a dataset of at least 1000 images was created. The without transfer learning fine-tune, fine-tuned transfer learning, transfer learning, fully supervised learning (FSL) and weakly supervised learning (WSL) applied models performances compared. Experimental results show that the fine-tuned transfer learning applied YOLO V4 model produces more accurate results than the other models in FSL and the transfer learning applied Faster R-CNN model produces more accurate results than the other models in WSL.",CNN; deep learning; fine-tuned transfer learning
"The 6D pose estimation of an object from an image is a central problem in many domains of Computer Vision (CV) and researchers have struggled with this issue for several years. Traditional pose estimation methods (1) leveraged on geometrical approaches, exploiting manually annotated local features, or (2) relied on 2D object representations from different points of view and their comparisons with the original image. The two methods mentioned above are also known as Feature-based and Template-based, respectively. With the diffusion of Deep Learning (DL), new Learning-based strategies have been introduced to achieve the 6D pose estimation, improving traditional methods by involving Convolutional Neural Networks (CNN). This review analyzed techniques belonging to different research fields and classified them into three main categories: Template-based methods, Feature-based methods, and Learning-Based methods. In recent years, the research mainly focused on Learning-based methods, which allow the training of a neural network tailored for a specific task. For this reason, most of the analyzed methods belong to this category, and they have been in turn classified into three sub-categories: Bounding box prediction and Perspective-n-Point (PnP) algorithm-based methods, Classification-based methods, and Regression-based methods. This review aims to provide a general overview of the latest 6D pose recovery methods to underline the pros and cons and highlight the best-performing techniques for each group. The main goal is to supply the readers with helpful guidelines for the implementation of performing applications even under challenging circumstances such as auto-occlusions, symmetries, occlusions between multiple objects, and bad lighting conditions.",Computer vision; 6D position estimation; Deep learning; RGB Input
"Benefiting from the advanced human visual system, humans naturally classify activities and predict motions in a short time. However, most existing computer vision studies consider those two tasks separately, resulting in an insufficient understanding of human actions. Moreover, the effects of view variations remain challenging for most existing skeleton-based methods, and the existing graph operators cannot fully explore multiscale relationship. In this article, a versatile graph-based model (Vers-GNN) is proposed to deal with those two tasks simultaneously. First, a skeleton representation self-regulated scheme is proposed. It is among the first trials that successfully integrate the idea of view adaptation into a graph-based human activity analysis system. Next, several novel graph operators are proposed to model the positional relationships and learn the abstract dynamics between different human joints and parts. Finally, a practical multitask learning framework and a multiobjective self-supervised learning scheme are proposed to promote both the tasks. The comparative experimental results show that Vers-GNN outperforms the recent state-of-the-art methods for both the tasks, with the to date highest recognition accuracies on the datasets of NTU RGB $+$ D (CV: 97.2%), UWA3D (88.7%), and CMU (1000 ms: 1.13).",Graph neural network; human action recogni-tion; motion prediction; skeleton model
"With the continuous progress of UAV (unmanned aerial vehicle) flight technology, more and more outdoor vision tasks begin to rely on UAV to complete, many of which require computer vision algorithms to analyze the information captured by the camera. However, it is difficult to deploy detectors on embedded devices due to the challenges among energy consumption, accuracy, and speed. In this paper, we propose an end-to-end object detection model running on a UAV platform that is suitable for real-time applications. Through the research of shufflenetv2 and mobilenetv3, a new feature extraction network structure is proposed. In order to improve the detection accuracy without losing the detection efficiency, a multi-scale fusion module based on deconvolution is added. Experiments show when deployed on our onboard Nvidia Jetson TX2 for testing and inference, our model combined with a modified focal loss function, produced a desirable performance of 21.7% mAP for object detection with an inference time of 17 fps.",UAV; Object detection; Lightweight; ShufflenetV2; MobileNetV3
"With the proliferation of cheap sensors and handheld devices, the amount of 3d data has grown exponentially and finds uses in the automated diagnosis of medical images, computer vision, and a host of other applications. Description and identification of geometrical primitives play an important role in computer vision and image processing. In this article, a definition of discrete spheres is given based on the dilation of euclidean spheres with a unit tetrahedron. It is shown in the article that the isothetic covers of spheres are equivalent to our definition of discrete spheres. Analysis of isothetic covers of spheres are presented, particularly its number-theoretic properties, and show that the bounding radius of isothetic cover faces is closely related to the distribution of the square number in integer intervals. Spherical segment recognition algorithms based on the number-theoretic properties of isothetic covers are proposed. Information content of the isothetic covers and computational load of the algorithm can be adjusted as per the requirements of the applications by changing the grid size. The computational complexities of the methods are determined and shows they are competitive to other related methods in the literature. The proposed methods are experimented with a large number of synthetic data to study its behavior and some of the results are presented in the article.",Isothetic cover; Discrete sphere; Sphere recognition; Number theory
"Edge detection is of great importance to the middle and high-level vision task in computer vision, and it is useful to improve its performance. This paper is different from previous edge detection methods designed only for decoding networks. We propose a new edge detection network composed of modulation coding network and decoding network. Among them, modulation coding network is the combination of modulation enhancement network and coding network designed by using the self-attention mechanism in Transformer, which is inspired by the selective attention mechanism of V1, V2, and V4 in biological vision. The modulation enhancement network effectively enhances the feature extraction ability of the encoding network, realizes the selective extraction of the global features of the input image, and improves the performance of the entire model. In addition, we designed a new decoding network based on the function of integrating feature information in the IT layer of the biological vision system. Unlike previous decoding networks, it combines top-down decoding and bottom-up decoding, uses down-sampling decoding to extract more features, and then achieves better performance by fusing up-sampling decoding features. We evaluated the proposed method experimentally on multiple publicly available datasets BSDS500, NYUD-V2, and barcelona images for perceptual edge detection (BIPED). Among them, the best performance is achieved on the NYUD and BIPED datasets, and the second result is achieved on the BSDS500. Experimental results show that this method is highly competitive among all methods.",edge detection; biological vision; visual cortex; self-attention mechanism; convolutional neural network
"Computer vision-based crack analysis for civil infrastructure has become popular to automatically process in-spection imaging data for crack detection, localisation and quantification. Some literature reviews have been conducted, which mostly focus on qualitative damage evaluation or damage segmentation, missing the meth-odology categorisation for applicability-oriented quantitative crack assessment. To fill the gap, this review provides a comprehensive overview of state-of-the-art image-based crack analysis under various conditions in both qualitative and quantitative aspects, particularly focusing on image processing and deep learning-based methodologies from image-level detection to pixel-level segmentation and quantification. The key challenges and research gaps are also discussed as follows, which indicate the importance of future research: (1) developing data model methodologies to resolve the difficulties due to the image data deficiency; (2) building a learning -based model capable of processing data with complex backgrounds; (3) enhancing the scene generalisation on different detection tasks; (4) establishing a lightweight mechanism for real-time crack analysis; (5) constructing learning-based systems that comprehend the local and global contexts during crack evaluation; (6) developing a semi-supervised mechanism for more information capturing and (7) establishing attention-based models for enhanced segmentation performance.",Image processing techniques; Structural health monitoring; Crack detection and quantification; Deep learning; Image segmentation
"To forecast giant panda (Ailuropoda melanoleuca) population dynamics in the wild, it is crucial to comprehend their age distribution. Traditional methods for estimating the age of panda are costly, time-consuming, and inaccurate. Additionally, these methods only forecast an age group rather than a real age, and lack a uniform standard. However, advances in deep learning and computer vision have given rise to fresh approaches to this problem. Classification models can be improved by using ordinal regression, which uses ordinal correlations across ages to reduce the non-stationary nature of aging tasks. In this study, we collected 8002 images from 272 pandas in various environments, whose ages ranged from 0 to 38. We applied a five-fold subject-exclusive (SE) protocol to train seven Convolutional Neural Networks (CNN) based on ordinal regression. Experiments were conducted on the Panda Age Dataset (PAD Full) and the Lite Panda Age Dataset (PAD Lite). The results were very encouraging and achieved a Mean Absolute Error (MAE) of 2.51 and 2.41, respectively. Our findings demonstrate that this new tool can noninvasively predict the age of giant pandas in captivity and the wild. Continued development of computer vision technology will drive progress in ecology and conservation.",Biodiversity; Giant panda; Machine learning; Age estimation; Ordinal regression
"In recent years, computer vision algorithms have become more powerful, which enabled technologies such as autonomous driving to evolve rapidly. However, current algorithms mainly share one limitation: They rely on directly visible objects. This is a significant drawback compared to human behavior, where visual cues caused by objects (e. g., shadows) are already used intuitively to retrieve information or anticipate occurring objects. While driving at night, this performance deficit becomes even more obvious: Humans already process the light artifacts caused by the headlamps of oncoming vehicles to estimate where they appear, whereas current object detection systems require that the oncoming vehicle is directly visible before it can be detected. Based on previous work on this subject, in this paper, we present a complete system that can detect light artifacts caused by the headlights of oncoming vehicles so that it detects that a vehicle is approaching providently (denoted as provident vehicle detection). For that, an entire algorithm architecture is investigated, including the detection in the image space, the three-dimensional localization, and the tracking of light artifacts. To demonstrate the usefulness of such an algorithm, the proposed algorithm is deployed in a test vehicle to use the detected light artifacts to control the glare-free high beam system proactively (react before the oncoming vehicle is directly visible). Using this experimental setting, the provident vehicle detection system's time benefit compared to an in-production computer vision system is quantified. Additionally, the glare-free high beam use case provides a real-time and real-world visualization interface of the detection results by considering the adaptive headlamps as projectors. With this investigation of provident vehicle detection, we want to put awareness on the unconventional sensing task of detecting objects providently (detection based on observable visual cues the objects cause before they are visible) and further close the performance gap between human behavior and computer vision algorithms to bring autonomous and automated driving a step forward.",Vehicle detection; Advanced driver assistance systems; Provident object detection
"Human hand gestures are the most important tools for interacting with the real environment. Capturing hand motion is critical for a wide range of applications in Augmented Reality (AR)/Virtual Reality (VR), Human-computer Interface (HCI), and many other disciplines. This paper presents a 3 module pipeline for effective hand gesture detection in real-time at the speed of 100 frames per second (fps).Various hand gestures can be captured by simple RGB camera and then processed to first detect the palm and then find essential 3D landmarks, which helps in creating skeletal representation of hand. In order to form a 3D mesh around the skeletal hand 2D and 3D annotations of Hand gestures are merged and in the final module 3D animated hand gestures are presented using advanced neural network. 3D representation of hand gestures ensures greater understanding of depth ambiguity problem in monocular pose estimations and can be effectively used in computer vision and graphics applications. The proposed design is compared with several benchmarks to highlight improvements in the results achieved over conventional methods.",Augmented reality; Human-computer interaction; 3D reconstruction; Virtual reality
"Human pose estimation (HPE) has attracted a significant amount of attention from the computer vision community in the past decades. Moreover, HPE has been applied to various domains, such as human-computer interaction, sports analysis, and human tracking via images and videos. Recently, deep learning-based approaches have shown state-of-the-art performance in HPE-based applications. Although deep learning-based approaches have achieved remarkable performance in HPE, a comprehensive review of deep learning-based HPE methods remains lacking in literature. In this article, we provide an up-to-date and in-depth overview of the deep learning approaches in vision-based HPE. We summarize these methods of 2-D and 3-D HPE, and their applications, discuss the challenges and the research trends through bibliometrics, and provide insightful recommendations for future research. This article provides a meaningful overview as introductory material for beginners to deep learning-based HPE, as well as supplementary material for advanced researchers.",Action recognition; bibliometric; deep learning; human performance assessment; human pose estimation (HPE)
"Monitoring of critical infrastructure for Structural Health Monitoring (SHM) is vital for the detection of structural damage (cracks or voids) at an initial stage, thus increasing the structures' serviceable life. The traditional methods of visual inspection to detect damages are time-consuming and less efficient. Sensor based Non-Destructive Techniques (S-NDTs) such as ground-penetration radar, acoustic emission, laser scanning, etc. for detection and analysis are extensively used to monitor structural health but are expensive and time-consuming. Recent advancements in Artificial Intelligence (AI) techniques such as Computer Vision (CV) assisted with Convolutional Neural Network (CNN), Machine Learning (ML) and Deep Learning (DL) in Structural Health Monitoring (SHM) provide more accurate data classification and damage detection systems. This paper provides a state-of-the-art review of the applications of AI-based techniques in SHM. A detailed study on vision data collection, processing techniques, and segmentation (feature, model, and pattern) is discussed, along with their limitations. The application of AI techniques for SHM to detect, isolate, and identify data anomalies, along with biomimetic algorithms are reviewed to assist in future research directions for life critical infrastructure monitoring.",
"Sign languages has extensive applications among differently-abled to communicate with their surroundings. With the development of different sensing technologies, several new human-computer interaction techniques (HCI) have been established to recognize hand gestures. Computer vision-based methods have shown significant utility for such applications. However, these methods are strongly dependent on the lighting conditions. The surface electromyography (sEMG) technique is invariant to lighting conditions and can easily reflect human motion intention. In this work, sEMG based sign language recognition model was developed using an efficient machine learning pipeline. Two sEMG datasets were recorded for predefined hand gestures using wireless sensors. These signals were mainly acquired against 24 manual alphabets (ASL-24) and ten digits(ASL-10) of American Sign Language (ASL). The collected data sets were preprocessed, and around 450 well-established feature was extracted from each sEMG channel. We applied an ensemble feature selection approach combining four diverse filter-based feature selection methods (ANOVA, Chi-square, Mutual Info, ReliefF). A newly proposed feature combiner that exploits feature-feature and feature-class correlation thresholds is used to combine feature subsets formed across the ensemble. The resulting features comprise reduced & most representative feature subsets and are further used in the pipeline for classifying ASL gestures. Using the CatBoost algorithm, the pipeline presented excellent average classification accuracy(99.91% on ASL-24) and other performance parameters for recognizing ASL gestures. The pipeline was also applied and validated on a benchmark dataset (Ninapro database 5, exercise A) and achieved similar outcomes. The result highlights the feasibility of using sEMG based approach as better options to computer-vision-based techniques to build an accurate and robust Sign Language Recognition system (SLRS). Moreover, efforts were made to find the optimal number of sensors and features for recognition task on (ASL-10 dataset) without impacting the overall reliability and accuracy of the system. The experiments results can be used to enhance the performance of various wearable sEMG sensor based HCI applications.",Human-computer interaction; Surface electromyography; American sign language; Feature selection
"In most developing countries, the contribution of agriculture to gross domestic product is significant. Plant disease is one of the major factors that adversely affect crop yield. Traditional plant disease detection techniques are time-consuming, biased, and ineffective. Potato is among the top consumed plants in the world, in general, and in developing countries, in particular. However, potato is affected by different kinds of diseases which minimize their yield and quantity. The advancement in AI and machine learning has paved the way for new methods of tackling plant disease detection. This study presents a comprehensive systematic literature review on the major diseases that harm potato crops. In this effort, computer vision-based techniques are employed to identify potato diseases, and types of machine learning algorithms used are surveyed. In this review, 39 primary studies that have provided useful information about the research questions are chosen. Accordingly, the most common potato diseases are found to be late blight, early blight, and bacterial wilt. Furthermore, the review discovered that deep learning algorithms were more frequently used to detect crop diseases than classical machine learning algorithms. Finally, the review categorized the state-of-the-art algorithms and identifies open research problems in the area.",
"The massive influx of text, images, and videos to the internet has recently increased the challenge of computer vision-based tasks in big data. Integrating visual data with natural language to generate video explanations has been a challenge for decades. However, recent experiments on image/video captioning that employ Long-Short-Term-Memory (LSTM) have piqued the interest of researchers studying its possible application in video captioning. The proposed video captioning architecture combines the bidirectional multilayer LSTM (BiLSTM) encoder and unidirectional decoder. The innovative architecture also considers temporal relations when creating superior global video representations. In contrast to the majority of prior work, the most relevant features of a video are selected and utilized specifically for captioning purposes. Existing methods utilize a single-layer attention mechanism for linking visual input with phrase meaning. This approach employs LSTMs and a multilayer attention mechanism to extract characteristics from movies, construct links between multi-modal (words and visual material) representations, and generate sentences with rich semantic coherence. In addition, we evaluated the performance of the suggested system using a benchmark dataset for video captioning. The obtained results reveal superior performance relative to state-of-the-art works in METEOR and promising performance relative to the BLEU score. In terms of quantitative performance, the proposed approach outperforms most existing methodologies.",Attention; Computer vision; Convolutional Neural Network; LSTM; Video captioning
"Despite the wide use of computer vision methods in plant health monitoring, little attention is paid to segmenting the diseased leaf area at its early stages. It can be explained by the lack of datasets of plant images with annotated disease lesions. We propose a novel methodology to generate fluorescent images of diseased plants with an automated lesion annotation. We demonstrate that a U-Net model aiming to segment disease lesions on fluorescent images of plant leaves can be efficiently trained purely by a synthetically generated dataset. The trained model showed 0.793% recall and 0.723% average precision against an empirical fluorescent test dataset. Creating and using such synthetic data can be a powerful technique to facilitate the application of deep learning methods in precision crop protection. Moreover, our method of generating synthetic fluorescent images is a way to improve the generalization ability of deep learning models.",synthetic data; semantic segmentation; plant disease; precision agriculture; deep learning; computer vision
"Crowd localization is a new computer vision task, evolved from crowd counting. Different from the latter, it provides more precise location information for each instance, not just counting numbers for the whole crowd scene, which brings greater challenges, especially in extremely congested crowd scenes. In this paper, we focus on how to achieve precise instance localization in high-density crowd scenes, and to alle-viate the problem that the feature extraction ability of the traditional model is reduced due to the target occlusion, the image blur, etc. To this end, we propose a Dilated Convolutional Swin Transformer (DCST) for congested crowd scenes. Specifically, a window-based vision transformer is introduced into the crowd localization task, which effectively improves the capacity of representation learning. Then, the well -designed dilated convolutional module is inserted into some different stages of the transformer to enhance the large-range contextual information. Extensive experiments evidence the effectiveness of the proposed methods and achieve the state-of-the-art performance on five popular datasets. Especially, the proposed model achieves F1-measure of 77.5% and MAE of 84.2 in terms of localization and counting performance, respectively. (c) 2022 Elsevier B.V. All rights reserved.",Crowd localization; Vision transformer; Dilated convolution; Contextual information
"Interpreting high-resolution satellite imagery could be an expensive and time-consuming task for human eyes. Computer Vision and Deep Learning techniques can help to solve this major problem by applying detection algorithms, which can ease the task of analysing such images for the benefit of humans. It can help in changing the way we comprehend and anticipate the economic activity around the world. Such techniques help us to observe the urban development in high security areas such as national and international borders. Constant progressions in improving and making satellites deployment, a cost-effective process to strengthen the networks of satellite orbiting the earth is one of the reasons such tasks can be easily solved with the help of high-resolution images. Current computer vision research works have achieved significant milestones in accuracy and speed but, there are still room for improvements. In this paper, we addressed some of these methods to bring them to a combined pipeline and proposed a set of improvements to further improve the speed and the accuracy of the detections. We proposed a unified framework, which combines several object detection algorithms and the state-of-art architecture of YoloV4 along with the TensorFlow object detection API. This framework can detect small and well as large objects with improved speed and accuracy by using two detectors for different scales. Evaluation ran on these high-resolution images yield mAP of 85.6% F1-score of 0.84.",adaptive pipeline inference system (APIS); computer vision; deep learning; satellite imagery; TensorFlow; YoloV4
"On the one hand, the solution of computer vision tasks is associated with the development of various kinds of images or random fields mathematical models, i.e., algorithms, that are called traditional image processing. On the other hand, nowadays, deep learning methods play an important role in image recognition tasks. Such methods are based on convolutional neural networks that perform many matrix multiplication operations with model parameters and local convolutions and pooling operations. However, the modern artificial neural network architectures, such as transformers, came to the field of machine vision from natural language processing. Image transformers operate with embeddings, in the form of mosaic blocks of picture and the links between them. However, the use of graph methods in the design of neural networks can also increase efficiency. In this case, the search for hyperparameters will also include an architectural solution, such as the number of hidden layers and the number of neurons for each layer. The article proposes to use graph structures to develop simple recognition networks on different datasets, including small unbalanced X-ray image datasets, widely known the CIFAR-10 dataset and the Kaggle competition Dogs vs Cats dataset. Graph methods are compared with various known architectures and with networks trained from scratch. In addition, an algorithm for representing an image in the form of graph lattice segments is implemented, for which an appropriate description is created, based on graph data structures. This description provides quite good accuracy and performance of recognition. The effectiveness of this approach based, on the descriptors of the resulting segments, is shown, as well as the graph methods for the architecture search.",computer vision; artificial intelligence; mathematical modeling; pattern recognition; machine learning; deep learning; graphs; transformers; image descriptors
"Road discrepancies such as potholes and road cracks are often present in our day-to-day commuting and travel. The cost of damage repairs caused by potholes has always been a concern for owners of any type of vehicle. Thus, an early detection processes can contribute to the swift response of road maintenance services and the prevention of pothole related accidents. In this paper, automatic detection of potholes is performed using the computer vision model library, You Look Only Once version 3, also known as Yolo v3. Light and weather during driving naturally affect our ability to observe road damage. Such adverse conditions also negatively influence the performance of visual object detectors. The aim of this work was to examine the effect adverse conditions have on pothole detection. The basic design of this study is therefore composed of two main parts: (1) dataset creation and data processing, and (2) dataset experiments using Yolo v3. Additionally, Sparse R-CNN was incorporated into our experiments. For this purpose, a dataset consisting of subsets of images recorded under different light and weather was developed. To the best of our knowledge, there exists no detailed analysis of pothole detection performance under adverse conditions. Despite the existence of newer libraries, Yolo v3 is still a competitive architecture that provides good results with lower hardware requirements.",pothole detection; pavement distress; adverse conditions; Yolo v3
"A semantic understanding of road traffic can help people understand road traffic flow situations and emergencies more accurately and provide a more accurate basis for anomaly detection and traffic prediction. At present, the overview of computer vision in traffic mainly focuses on the static detection of vehicles and pedestrians. There are few in-depth studies on the semantic understanding of road traffic using visual methods. This paper aims to review recent approaches to the semantic understanding of road traffic using vision sensors to bridge this gap. First, this paper classifies all kinds of traffic monitoring analysis methods from the two perspectives of macro traffic flow and micro road behavior. Next, the techniques for each class of methods are reviewed and discussed in detail. Finally, we analyze the existing traffic monitoring challenges and corresponding solutions.",Traffic surveillance analysis; macro-traffic flow; micro-vehicle behaviors; temporal reasoning; computer vision
"In the field of intelligent surface inspection systems, particular attention is paid to decision making problems, based on data from different sensors. The combination of such data helps to make an intelligent decision. In this research, an approach to intelligent decision making based on a data integration strategy to raise awareness of a controlled object is used. In the following article, this approach is considered in the context of reasonable decisions when detecting defects on the surface of welds that arise after the metal pipe welding processes. The main data types were RGB, RGB-D images, and acoustic emission signals. The fusion of such multimodality data, which mimics the eyes and ears of an experienced person through computer vision and digital signal processing, provides more concrete and meaningful information for intelligent decision making. The main results of this study include an overview of the architecture of the system with a detailed description of its parts, methods for acquiring data from various sensors, pseudocodes for data processing algorithms, and an approach to data fusion meant to improve the efficiency of decision making in detecting defects on the surface of various materials.",data fusion; intelligent system; decision making; computer vision; acoustic emission control; surface inspection
"Machine learning and computer vision algorithms can provide a precise and automated interpretation of medical videos. The segmentation of the left ventricle of echocardiography videos plays an essential role in cardiology for carrying out clinical cardiac diagnosis and monitoring the patient's condition. Most of the developed deep learning algorithms for video segmentation require an enormous amount of labeled data to generate accurate results. Thus, there is a need to develop new semi-supervised segmentation methods due to the scarcity and costly labeled data. In recent research, semi-supervised learning approaches based on graph signal processing emerged in computer vision due to their ability to avail the geometrical structure of data. Video object segmentation can be considered as a node classification problem. In this paper, we propose a new approach called GraphECV based on the use of graph signal processing for semi-supervised learning of video object segmentation applied for the segmentation of the left ventricle in echordiography videos. GraphECV includes instance segmentation, extraction of temporal, texture and statistical features to represent the nodes, construction of a graph using K-nearest neighbors, graph sampling to embed the graph with small amount of labeled nodes or graph signals, and finally a semi-supervised learning approach based on the minimization of the Sobolov norm of graph signals. The new algorithm is evaluated using two publicly available echocardiography videos, EchoNet-Dynamic and CAMUS datasets. The proposed approach outperforms other state-of-the-art methods under challenging background conditions.",echocardiography; video object segmentation; deep learning; graph signal processing; semi-supervised learning
"Visual detection and classification of water and waterbodies provide important information needed for managing water resources systems and infrastructure, such as developing flood early warning systems and drought management. But water itself is a challenging object for visual analysis because it is shapeless, colorless, and transparent. Therefore, detecting, tracking, and localizing water in different visual environments are difficult tasks. Computer vision (CV) techniques provide powerful tools for image processing and high-level scene analysis. Despite the complexities associated with water in visual scenes, there are still some physical differences, such as color, turbidity, and turbulence, affected by surrounding settings, which can potentially support CV modeling to cope with the visual processing challenges of water. The goal of this study is to introduce a new image data set, ATLANTIS Texture (ATeX), which represents various water textures of different waterbodies, and evaluate the performance of deep learning (DL) models for classification purposes on ATeX. Experimental results show that among DL models, EffNet-B7, EffNet-B0, GoogLeNet, and ShuffleNet V2 x 1.0 provide the highest precision, recall, and F1 score. However, by considering the training time, total number of parameters, and total memory occupied by these models, ShuffleNet V2 x 1.0 is presented as the most efficient DL network for water classification. Finally, results from this study suggest that ATeX provides a new benchmark to investigate existing challenges in the field of image analysis, in particular for water, which can help both water resources engineers and the computer vision community. (C) 2022 American Society of Civil Engineers.",
"Kinship verification from facial images in the wild based on one-to-one classification has gathered a promising attention by image processing and computer vision researchers. While family classification based on one-to-many classification is relatively the least explored domain in computer vision. This paper first performs family classification on different family-sets based on number of family members. Second, we perform kinship verification on different kinship relations covering parent-child and siblings. We present a new kinship database named KinIndian dedicated for these two tasks of family classification and kinship verification. KinIndian database comprises 1926 images of 813 individuals from 230 unique Indian families with 2-7 members. KinIndian is designed into two levels: the first is family-level for family classification, and the second is photo-level for kinship verification. We propose a novel weighted nearest member metric leaning (WNMML) method to evaluate family classification on different family-sets. Proposed WNMML method is based on minimizing intraclass separation by characterizing compactness for positive families and maximizing interclass separation by pushing members of negative families as far as possible. WNMML achieves competitive accuracy on different family-sets and hence shows that WNMML could be effectively used in real-world scenarios. Furthermore, we also perform kinship verification on KinIndian using baseline multimetric learning methods and achieves promising and encouraging kinship accuracy.",Family classification; Kinship verification; Database; KinIndian
"Cooking at home is a critical survival skill. We propose a new cooking assistance system in which a user only needs to wear an all-in-one augmented reality (AR) headset without having to install any external sensors or devices in the kitchen. Utilizing the built-in camera and cutting-edge computer vision (CV) technology, the user can direct the AR headset to recognize available food ingredients by simply looking at them. Based on the types of the recognized food ingredients, suitable recipes are suggested accordingly. A step-by-step video tutorial providing details of the selected recipe is then displayed with the AR glasses. The user can conveniently interact with the proposed system using eight kinds of natural hand gestures without needing to touch any devices throughout the entire cooking process. Compared with the deep learning models ResNet and ResNeXt, experimental results show that the YOLOv5 achieves lower accuracy for ingredient recognition, but it can locate and classify multiple ingredients in one shot and make the scanning process easier for users. Twenty participants test the prototype system and provide feedback via two questionnaires. Based on the analysis results, 19 of the 20 participants would recommend others to use the proposed system, and all participants are overall satisfied with the prototype system.",augmented reality; Magic Leap One; smart kitchen; AR cooking
"Object detection is a computer vision task that involves localisation and classification of objects in an image. Video data implicitly introduces several challenges, such as blur, occlusion and defocus, making video object detection more challenging in comparison to still image object detection, which is performed on individual and independent images. This paper tackles these challenges by proposing an attention-heavy framework for video object detection that aggregates the disentangled features extracted from individual frames. The proposed framework is a two-stage object detector based on the Faster R-CNN architecture. The disentanglement head integrates scale, spatial and task-aware attention and applies it to the features extracted by the backbone network across all the frames. Subsequently, the aggregation head incorporates temporal attention and improves detection in the target frame by aggregating the features of the support frames. These include the features extracted from the disentanglement network along with the temporal features. We evaluate the proposed framework using the ImageNet VID dataset and achieve a mean Average Precision (mAP) of 49.8 and 52.5 using the backbones of ResNet-50 and ResNet-101, respectively. The improvement in performance over the individual baseline methods validates the efficacy of the proposed approach.",object detection; video object detection; attention; computer vision; deep learning
"With the increasing number of data-driven models in nuclear applications, large volumes of numerical data are requiblack to accurately model and pblackict the health status of a plant component. However, many historical operation logs that contain useful information are not fully utilized due to the lack of a systematic approach of digitization. To overcome this issue, this study proposes an automatic pipeline for extracting information from handwritten tabular documents collected from nuclear power plants. In our pipeline, we first denoise scanned documents with morphological operations, and then extract relevant parts from individual pages using both traditional computer vision and neural network methods. Handwriting recognition is applied to obtain text and numbers. As the most challenging step is how to crop only relevant information, the main focus of our paper is to detect tables and cells from scanned handwritten documents. We evaluate the efficiency and accuracy of our proposed method on handwritten operational reports obtained from a real-world case study. The results demonstrate the high accuracy and practicality of our proposed method.",Nuclear power plants; Computer vision; Image processing; Neural networks; Table detection; Handwritten documents
"With the increase of large camera networks around us, it is becoming more difficult to manually identify vehicles. Computer vision enables us to automate this task. More specifically, vehicle re-identification (ReID) aims to identify cars in a camera network with non-overlapping views. Images captured of vehicles can undergo intense variations of appearance due to illumination, pose, or viewpoint. Furthermore, due to small inter-class similarities and large intra-class differences, feature learning is often enhanced with non-visual cues, such as the topology of camera networks and temporal information. These are, however, not always available or can be resource intensive for the model. Following the success of Transformer baselines in ReID, we propose for the first time an outlook-attention-based vehicle ReID framework using the Vision Outlooker as its backbone, which is able to encode finer-level features. We show that, without embedding any additional side information and using only the visual cues, we can achieve an 80.31% mAP and 97.13% R-1 on the VeRi-776 dataset. Besides documenting our research, this paper also aims to provide a comprehensive walkthrough of vehicle ReID. We aim to provide a starting point for individuals and organisations, as it is difficult to navigate through the myriad of complex research in this field.",vehicle re-identification; Vision Outlooker; explainable AI; secure AI; smart cities
"The performance of a computer vision system depends on the accuracy of visual information extracted by the sensors and the system's visual-processing capabilities. To derive optimum information from the sensed data, the system must be capable of identifying objects of interest (OOIs) and activities in the scene. Active vision systems intend to capture OOIs with the highest possible resolution to extract the optimum visual information by calibrating the configuration spaces of the cameras. As the data processing and reconfiguration of cameras are interdependent, it becomes very challenging for advanced active vision systems to perform in real time. Due to limited computational resources, model-based asymmetric active vision systems only work in known conditions and fail miserably in unforeseen conditions. Symmetric/asymmetric systems employing artificial intelligence, while they manage to tackle unforeseen environments, require iterative training and thus are not reliable for real-time applications. Thus, the contemporary symmetric/asymmetric reconfiguration systems proposed to obtain optimum configuration spaces of sensors for accurate activity tracking and scene understanding may not be adequate to tackle unforeseen conditions in real time. To address this problem, this article presents an adaptive self-reconfiguration (ASR) framework for active vision systems operating co-operatively in a distributed blockchain network. The ASR framework enables active vision systems to share their derived learning about an activity or an unforeseen environment, which learning can be utilized by other active vision systems in the network, thus lowering the time needed for learning and adaptation to new conditions. Further, as the learning duration is reduced, the duration of the reconfiguration of the cameras is also reduced, yielding better performance in terms of understanding of a scene. The ASR framework enables resource and data sharing in a distributed network of active vision systems and outperforms state-of-the-art active vision systems in terms of accuracy and latency, making it ideal for real-time applications.",active vision; self-adaptation; self-reconfiguration; smart camera network
"Citizen science platforms, social media and smart phone applications enable the collection of large amounts of georeferenced images. This provides a huge opportunity in biodiversity and ecological research, but also creates challenges for efficient data handling and processing. Recreational and small-scale fisheries is one of the fields that could be revolutionised by efficient, widely accessible and machine learning-based processing of georeferenced images. Most non-commercial inland and coastal fisheries are considered data poor and are rarely assessed, yet they provide multiple societal benefits and can have substantial ecological impacts. Given that large quantities of georeferenced fish images are being collected by fishers every day, artificial intelligence (AI) and computer vision applications offer a great opportunity to automate their analyses by providing species identification, and potentially also fish size estimation. This would deliver data needed for fisheries management and fisher engagement. To date, however, many AI image analysis applications in fisheries are focused on the commercial sector, limited to specific species or settings, and are not publicly available. In addition, using AI and computer vision tools often requires a strong background in programming. In this study, we aim to facilitate broader use of computer vision tools in fisheries and ecological research by compiling an open-source user friendly and modular framework for large-scale image storage, handling, annotation and automatic classification, using cost- and labour-efficient methodologies. The tool is based on TensorFlow Lite Model Maker library, and includes data augmentation and transfer learning techniques applied to different convolutional neural network models. We demonstrate the potential application of this framework using a small example dataset of fish images taken through a recreational fishing smartphone application. The framework presented here can be used to develop region-specific species identification models, which could potentially be combined into a larger hierarchical model.",recreational fisheries; artisanal fisheries; citizen science; deep learning; fish species identification; image annotation; smart phone applications
"Spatial color algorithms (SCAs) are computer vision procedures widely used for image enhancement and human vision modeling. The main characteristic of SCA family is that they mimic the behavior of the human vision system (HVS), achieving in this way robustness and the capability to adjust their effect according to the image content. Here, we review 35 different, popular Retinex-inspired SCAs discussing and providing a set of measures for their evaluation in terms of image quality. To this purpose, we also introduce SCA-30, a real-world color image dataset made publicly available. The algorithms considered here include and spread from well-known Retinex implementations, Retinex variants, Milano-Retinex and related inspired enhancers, illumination/decomposition approaches, and deep learning-based techniques. Data and code used for the evaluation are made freely available to the community, to pursue further analysis and comparisons. (c) 2022 SPIE and IS&T",Retinex theory; image enhancement; image quality assessment
"Because the pretraining model is not limited by the scale of data annotation and can learn general semantic information, it performs well in tasks related to natural language processing and computer vision. In recent years, more and more attention has been paid to research on the multimodal pretraining model. Many vision-language multimodal datasets and related models have been proposed one after another. In order to better summarize and analyze the development status and future trend of vision-language multimodal pretraining model technology, firstly this paper comprehensively combs the category system and related tasks of vision-language multimodal pretraining. Secondly, research progress on vision-language multimodal pretraining is summarized and analyzed from the two dimensions of image-language and video-language models. Finally, problems with and development trends in vision-language multimodal pretraining are discussed.",vision-language pretraining model; multimodal pretraining model; pretraining techniques; unsupervised learning
"Since Google proposed Transformer in 2017, it has made significant natural language processing (NLP) development. However, the increasing cost is a large amount of calculation and parameters. Previous researchers designed and proposed some accelerator structures for transformer models in field-programmable gate array (FPGA) to deal with NLP tasks efficiently. Now, the development of Transformer has also affected computer vision (CV) and has rapidly surpassed convolution neural networks (CNNs) in various image tasks. And there are apparent differences between the image data used in CV and the sequence data in NLP. The details in the models contained with transformer units in these two fields are also different. The difference in terms of data brings about the problem of the locality. The difference in the model structure brings about the problem of path dependence, which is not noticed in the existing related accelerator design. Therefore, in this work, we propose the ViA, a novel vision transformer (ViT) accelerator architecture based on FPGA, to execute the transformer application efficiently and avoid the cost of these challenges. By analyzing the data structure in the ViT, we design an appropriate partition strategy to reduce the impact of data locality in the image and improve the efficiency of computation and memory access. Meanwhile, by observing the computing flow of the ViT, we use the half-layer mapping and throughput analysis to reduce the impact of path dependence caused by the shortcut mechanism and fully utilize hardware resources to execute the Transformer efficiently. Based on optimization strategies, we design two reuse processing engines with the internal stream, different from the previous overlap or stream design patterns. In the stage of the experiment, we implement the ViA architecture in Xilinx Alveo U50 FPGA and finally achieved similar to 5.2 times improvement of energy efficiency compared with NVIDIA Tesla V100, and 4-10 times improvement of performance compared with related accelerators based on FPGA, that obtained nearly 309.6 GOP/s computing performance in the peek.",Accelerator; field-programmable gate array (FPGA); vision transformer (ViT)
"Computer vision (CV) combined with a deep convolutional neural network (CNN) has emerged as a reliable analytical method to effectively characterize and quantify high-throughput phenotyping of different grain crops, including rice, wheat, corn, and soybean. In addition to the ability to rapidly obtain information on plant organs and abiotic stresses, and the ability to segment crops from weeds, such techniques have been used to detect pests and plant diseases and to identify grain varieties. The development of corresponding imaging systems to assess the phenotypic parameters, yield, and quality of crop plants will increase the confidence of stakeholders in grain crop cultivation, thereby bringing technical and economic benefits to advanced agriculture. Therefore, this paper provides a comprehensive review of CNNs in computer vision for grain crop phenotyping. It is meaningful to provide a review as a roadmap for future research in such a thriving research area. The CNN models (e.g., VGG, YOLO, and Faster R-CNN) used CV tasks including image classification, object detection, semantic segmentation, and instance segmentation, and the main results of recent studies on crop phenotype detection are discussed and summarized. Additionally, the challenges and future trends of the phenotyping techniques in grain crops are presented.",grain crops; convolutional neural network; computer vision; phenotype detection
"Target recognition and localization are essential in computer vision and pattern recognition in robotics. The artificial extraction of features was omitted with the emergence of deep convolutional neural networks, reducing the influence of human factors on the results. The single-shot multibox detector (SSD) network has achieved excellent recognition by high precision and fast speed in target recognition and positioning. However, some small real-time systems are difficult to implement because of their demanding hardware and extended training time. Based on the digital normalization and residual network structure, the depth-wise separable convolution is proposed to replace the traditional convolution. The improved SSD network structure was used for identification and positioning in our work. The speed of training and testing increased without a decline in the accuracy, thus reducing the dependence on hardware. The method has achieved good results on the PASCAL VOC dataset after testing. It can also be applied to the field of intelligent inspection robots and intelligent security robots. (c) 2022 SPIE and IS&T",identification and positioning; deep convolutional neural network; single-shot multibox detection; PASCAL VOC; computer vision; robotics
"Despite of its tremendous popularity and success in computer vision (CV) and natural language processing, deep learning is inherently vulnerable to adversarial attacks in which adversarial examples (AEs) are carefully crafted by imposing imperceptible perturbations on the clean examples to deceive the target deep neural networks (DNNs). Many defense solutions in CV have been proposed. However, most of them, e.g., adversarial training, suffer from a low generality due to the reliance on limited AEs. Moreover, some solutions even have a non-negligible negative impact on the classification accuracy of clean examples. Last but not least, they are impotent against the unconstrained attacks in which the attackers optimize the perturbation direction and size by additionally taking the defense methods into accounts. In this article, we propose GRIP-GAN to learn a general robust inverse perturbation (GRIP), which is not only able to offset any potential adversarial perturbations but also strengthen the target class-related features, purely from the clean images via a generative adversarial network (GAN). By feeding a random noise, GRIP-GAN is able to generate a dynamic GRIP for each input image to defend against unconstrained attacks. To further improve the defense performance, we also enable GRIP-GAN to generate a GRIP tailored to each input image via feeding input image specific noise to GRIP-GAN. Extensive experiments are carried out on MNIST, CIFAR10, and ImageNet datasets against 17 adversarial attacks. The results show that GRIP-GAN outperforms all the baselines. We further share insights on the success of GRIP-GAN and provide visualized proofs.",Perturbation methods; Training; Deep learning; Predictive models; Noise reduction; Generative adversarial networks; Computational modeling; Attack-free defense; general robust inverse perturbation; class-related features; generative adversarial network
"With Deep Learning (DL) outperforming previous Machine Learning (ML) techniques in classifying images, the remote sensing community has recently shown an increased interest in using these algorithms to classify Land Use and Land Cover (LULC) using multispectral and hyperspectral data. Land Use (LU) and Land Cover (LC) are two types of cartographic data that are used to develop smart cities and monitor the environment. LULC clas-sification can benefit greatly from successfully applying remote sensing Image Classification (IC) using high spatial resolution data. The acquisition of spatiotemporal data for LULC classification has been made more accessible because of recent improvements in spatial analysis and Deep Learning (DL) technology. Considering the quality of Deep Neural Networks (DNN) in related Computer Vision (CV) tasks and the enormous volume of remotely sensed data accessible, DL methods appear to be particularly promising for modelling many remote sensing problems. However, there are several issues with ground-truth, resolution, and the nature of data that have a significant impact on categorization performance. We propose a Reversible Residual Network (RAVNet), a hybrid residual attention sensitive segmentation approach, to precisely categorize LULC in this study. The suggested network is based on the VNet model, which extracts relevant information by mixing low-level and high-level Feature Maps (FM). The attention-aware features change adaptively to the integration of residual modules. Our system was tested on the National Agriculture Imagery Program (NAIP) dataset, and the findings demonstrate that our architecture is competitive against other learning models.",Land use and land cover; Classification; Machine learning; Deep learning; RAVNet; Spatial analysis; Accuracy
"Rain impairs the performance of outdoor vision systems, such as automated driving systems and outdoor surveillance systems. Therefore, as an image preprocessing technique, image deraining has great potential for application. Defects of convolutional neural networks (small receptive field and non-adaptive to input content) limit the further improvement of deraining model performance. Recently, a novel neural network, transformer, has demonstrated impressive performance on natural language processing and vision tasks. However, using transformer for image deraining still has some issues: Although transformers have powerful long-range computing capabilities, it lacks the ability to model local features, which is critical for image deraining. In addition, transformer uses fixed-size patches to process images, which leads to pixels at the edges of the patches that cannot use the local features of neighboring pixels to restore rain-free images. In this paper, we propose a novel pyramid transformer for image deraining. To address the first issue, we design a residual-Dconv feed-forward network (RDFN), where depth-wise convolution improves the capability of modeling local features. To address the second issue, we introduce multi-resolution features into the transformer, which allows the transformer to obtain patches with different scales, thus enabling the boundary pixels to utilize local features. Furthermore, we propose a novel multi-scale fusion bridge (MSFB) to effectively integrate the extracted multi-scale features and capture the correlation between different scales. Extensive experiments on synthetic and real-world images demonstrate that the proposed deraining model achieves superior performance, especially the PSNR value achieves 47.55 dB on the SPA-Data dataset. We also further validate the effectiveness of the proposed model on subsequent high-level computer vision tasks.",Single image deraining; Vision transformer; Image restoration; Neural networks; Deep learning
"Flattening shapes without distortion is a problem that has been intriguing scientists for centuries. It is a fundamental problem of high importance in computer vision as many approaches may greatly benefit from its implementation. This paper introduces a new approach that allows flattening without distortion, by transforming the shape from Riemannian geometry to Weitzenbock geometry. This transformation is obtained by calculating the Cholesky frame associated with the Riemannian metric. In the Weitzenbock space, the Riemann tensor is identically zero which means that the Weitzenbock space is entirely flat. The teleparallel equation, which determines distances in the Weitzenbock space, and the geodesic equation, which determines distances in its Riemannian counterpart, are equivalent. The end result is that there is no distortion when passing from Riemannian geometry to Weitzenbock geometry. Given the importance of the heat kernel in computer vision, an analytic solution of the heat kernel in Weitzenbock space is presented.",Computer vision; Flattening; Distortion; Riemannian; Weitzenb?ck; Teleparallel; Heat kernel; Fokker-Planck; Schr?dinger
"Early and precise detection of diabetic retinopathy prevents vision impairments through computer-aided clinical procedures. Identifying the symptoms and processing those by using sophisticated clinical procedures reduces hemorrhage kind of risks. The input diabetic retinopathy images are influenced by using computer vision-based processes for segmentation and classification through feature extractions. In this article, a delimiting segmentation using knowledge learning (DS-KL) is introduced for classifying and detecting exudate regions by using varying histograms. The input image is identified for its histogram changes from the feature-dependent segmentation process. Depending on the training knowledge from multiple inputs with different exudate regions, the segmentation is performed. This segmentation identifies infected and noninfected regions across the delimiting pixel boundaries. The knowledge-learning process stores the newly identified exudate region for training and pixel correlation. The recurrent training improves the segmentation accuracy with precise detection and limited errors.",
"With the extensive application of deep learning (DL) algorithms in recent years, e.g., for detecting Android malware or vulnerable source code, artificial intelligence (AI) and machine learning (ML) are increasingly becoming essential in the development of cybersecurity solutions. However, sharing the same fundamental limitation with other DL application domains, such as computer vision (CV) and natural language processing (NLP), AI-based cybersecurity solutions are incapable of justifying the results (ranging from detection and prediction to reasoning and decision-making) and making them understandable to humans. Consequently, explainable AI (XAI) has emerged as a paramount topic addressing the related challenges of making AI models explainable or interpretable to human users. It is particularly relevant in cybersecurity domain, in that XAI may allow security operators, who are overwhelmed with tens of thousands of security alerts per day (most of which are false positives), to better assess the potential threats and reduce alert fatigue. We conduct an extensive literature review on the intersection between XAI and cybersecurity. Particularly, we investigate the existing literature from two perspectives: the applications of XAI to cybersecurity (e.g., intrusion detection, malware classification), and the security of XAI (e.g., attacks on XAI pipelines, potential countermeasures). We characterize the security of XAI with several security properties that have been discussed in the literature. We also formulate open questions that are either unanswered or insufficiently addressed in the literature, and discuss future directions of research.",Cybersecurity; Explainable AI; Machine learning
"The fatality of road accidents in this era is alarming. According to WHO, approximately 1.30 million people die each year in road accidents. Road accidents result in significant socioeconomic losses for people, their families, and the country. The integration of modern technologies into automobiles can help to reduce the number of people killed or injured in road accidents. Most of the study and police reports claim that fatigued driving is one of the deadliest factors behind many road accidents. This paper presents a complete embedded system to detect fatigue driving using deep learning, computer vision, and heart rate monitoring with Nvidia Jetson Nano developer kit, Arduino Uno, and AD8232 heart rate module. The proposed system can monitor the driver's real-time situations, then analyze the situation to detect any fatigue conditions and act accordingly. The onboard camera module constantly monitors the driver. The frames are retrieved and analyzed by the core system that uses deep learning and computer vision techniques to verify the situation with Nvidia Jetson Nano. The driver's states are identified using eye and mouth localization approaches from 68 distinct facial landmarks. Experimentally driven threshold data is employed to classify the states. The onboard heart rate module constantly measures the heart rates and detects any fluctuation in BPM related to the drowsiness. This system uses a convolutional neural network-based deep learning framework to include additional face mask detection to cope with the current pandemic situation. The heart rate module works parallelly where the other modules work in a conditional sequential manner to ensure uninterrupted detection. It will detect any sign of drowsiness in real-time and generate the alarm. The system successfully passed the initial lab tests and some actual situation experiments with 97.44% accuracy in fatigue detection and 97.90% accuracy in face mask identification. The automatic device was able to analyze different situations of drivers (different distances of driver from the camera, various lighting conditions, wearing eyeglasses, oblique projection) more precisely and generate an alarm before the accident happened.",Convolutional neural network; Drowsiness detection; Embedded system; Eye aspect ratio; Eye tracking; Fatigue; Nvidia Jetson Nano; Percentage of eye closure
"While there have been a considerable number of studies on computer vision (CV)-based crack detection on concrete/asphalt public facilities, such as sewers and tunnels, masonry-related structures have received less attention. This research seeks to implement an automated crack segmentation and a real-life crack length measurement of masonry walls using CV techniques and deep learning. The main contributions include (1) a large dataset of manually labelled images about various types of Korea masonry walls; (2) a careful performance evaluation of various deep learning-based crack segmentation models, including U-Net, DeepLabV3+, and FPN; and (3) a novel algorithm to extract real-life crack length measurement by detecting the brick units. The experimental results showed that deep learning-based masonry crack segmentation performed significantly better than previous approaches and could provide a real-life crack measurement. Therefore, it has a huge po-tential for motivating masonry-based structure investigation.",Masonry building; Crack segmentation; Deep learning; Measurement; Image processing
"Object detection is the most important problem in computer vision tasks. After AlexNet proposed, based on Convolutional Neural Network (CNN) methods have become mainstream in the computer vision field, many researches on neural networks and different transformations of algorithm structures have appeared. In order to achieve fast and accurate detection effects, it is necessary to jump out of the existing CNN framework and has great challenges. Transformer's relatively mature theoretical support and technological development in the field of Natural Language Processing have brought it into the researcher's sight, and it has been proved that Transformer's method can be used for computer vision tasks, and proved that it exceeds the existing CNN method in some tasks. In order to enable more researchers to better understand the development process of object detection methods, existing methods, different frameworks, challenging problems and development trends, paper introduced historical classic methods of object detection used CNN, discusses the highlights, advantages and disadvantages of these algorithms. By consulting a large amount of paper, the paper compared different CNN detection methods and Transformer detection methods. Vertically under fair conditions, 13 different detection methods that have a broad impact on the field and are the most mainstream and promising are selected for comparison. The comparative data gives us confidence in the development of Transformer and the convergence between different methods. It also presents the recent innovative approaches to using Transformer in computer vision tasks. In the end, the challenges, opportunities and future prospects of this field are summarized.",Computer vision; Object detection; Real-time system; CNN; Transformer
"Emotion recognition from facial images is an important and active area of research. Facial features are widely used in computer vision for emotion interpretation, cognitive science, and social interaction. To obtain accurate analysis of facial expressions (happy, angry, sad, surprised, disgusted, fearful, and neutral), a complex method based on human-computer interaction and data is required. It is still difficult to develop an effective and computationally simple mechanism for feature selection and emotion classification. In this paper, an emotion recognition model using adaptive neuro-fuzzy inference system optimized with particle swarm optimization is proposed. The proposed model was compared with many classification algorithms (ANNs, SVMs, and k-Nearest Neighbor (k-NN) and their subcomponents). The confusion matrix was used to evaluate the performance of these classifiers. The proposed model was evaluated using the MUG database. The model achieved a prediction accuracy of 99.6%.",Facial expression; Emotion recognition (ER); Adaptive neuro-fuzzy inference system (ANFIS); Machine learning (ML); Particle swarm optimization (PSO)
"Research of visual neural networks (VNNs) is one of the most important topics in deep learning and has received wide attention from industry and academia for their promising performance. The applications of VNNs range from image classification and target detection to scene segmentation in various fields such as transportation, healthcare and finance. In general, VNNs can be divided into two types: Convolutional neural networks (CNNs) and Transformer networks. In the last decade, CNNs have dominated the research of vision tasks. Recently, Transformer networks are successfully used in the fields of natural language processing and computer vision, and have achieved remarkable performance in many vision tasks. In this paper, the basic architectures and current trends of these two types of VNNs are first introduced. Then, three major challenges of VNNs are pointed out: scalability, robustness and interpretability. Next, the lightweight, robust and interpretable solutions are summarized and analyzed. Finally, the future opportunities of VNNs are presented.",Visual Transformer; Convolutional neural networks (CNNs); Deep learning; Scalability; Robustness; Interpretability
"The understanding of human-object interactions is fundamental in First Person Vision (FPV). Visual tracking algorithms which follow the objects manipulated by the camera wearer can provide useful information to effectively model such interactions. In the last years, the computer vision community has significantly improved the performance of tracking algorithms for a large variety of target objects and scenarios. Despite a few previous attempts to exploit trackers in the FPV domain, a methodical analysis of the performance of state-of-the-art trackers is still missing. This research gap raises the question of whether current solutions can be used off-the-shelf or more domain-specific investigations should be carried out. This paper aims to provide answers to such questions. We present the first systematic investigation of single object tracking in FPV. Our study extensively analyses the performance of 42 algorithms including generic object trackers and baseline FPV-specific trackers. The analysis is carried out by focusing on different aspects of the FPV setting, introducing new performance measures, and in relation to FPV-specific tasks. The study is made possible through the introduction of TREK-150, a novel benchmark dataset composed of 150 densely annotated video sequences. Our results show that object tracking in FPV poses new challenges to current visual trackers. We highlight the factors causing such behavior and point out possible research directions. Despite their difficulties, we prove that trackers bring benefits to FPV downstream tasks requiring short-term object tracking. We expect that generic object tracking will gain popularity in FPV as new and FPV-specific methodologies are investigated.",First person vision; Egocentric vision; Visual object tracking; Single object tracking
"The detection of anomalies is at the basis of any 3D printing control. In this paper, we propose a methodology for detection of anomalies based on computer vision. This methodology is composed of three modules: (1) image acquisition, (2) interlayer line and layer segmentation and (3) characterization of the local geometry and texture of the layers and detection of anomalies. The image acquisition is performed with a camera fixed to the printing nozzle. The proposed layer segmentation method recognizes and locates the lines separating the printed layers (F-score = 91%). The third module - taking as input the segmentation and the original image - evaluates the geometry of the layers and the texture of the material. The results are used to detect geometry anomalies when the values are outside the expected range. The material texture is classified into four classes of quality (macro-averaged F-score = 94%). We present the results and show the suitability of our methodology for automatic detection and localization of anomalies on images acquired during a printing session.",Automatic monitoring; 3D concrete printing; Image processing; Deep learning
"A large number of demands for space on-orbit services to ensure the on-orbit system completes its specified tasks are foreseeable, and the efficiency and the security are the most significant factors when we carry out an on-orbit mission. And it can improve human-computer interaction efficiency in operations with proper gesture recognition solutions. In actual situations, the operations are complex and changeable, so the gestures used in interaction are also difficult to predict in advance due to the compounding of multiple consecutive gestures. To recognize such gestures based on computer vision (CV) requires complex models trained by a large amount of datasets, it is often unable to obtain enough gesture samples for training a complex model in real tasks, and the cost of labeling the collected gesture samples is quite expensive. Aiming at the problems mentioned above, we propose a few-shot continuous gesture recognition scheme based on RGB video. The scheme uses Mediapipe to detect the key points of each frame in the video stream, decomposes the basic components of gesture features based on certain human palm structure, and then extracts and combines the above basic gesture features by a lightweight autoencoder network. Our scheme can achieve 89.73% recognition accuracy on the 5-way 1-shot gesture recognition task which randomly selected 142 gesture instances of 5 categories from the RWTH German fingerspelling dataset.",
"Non-rigid point set registration has been used in a wide range of computer vision applications such as human movement tracking, medical image analysis, three dimensional (3D) object reconstruction and is a very challenging task. It has two fundamental tasks. One is to find correspondences between two or more point sets and another is to transform a point set so that it aligns with other point sets. There has been significant progress in the past two decades in the non-rigid registration field but it still has major challenges and is an active research area in the computer vision and pattern recognition community. In this review, we present a survey of non-rigid point set registration. Unlike recent surveys, we focus on the mathematical foundations of non-rigid registration methods, categorize the methods from several perspectives, and discuss open challenges. We categorize the methods according to correspondence models, motivations, and challenges such as deformation, data degradation, computational efficiency, and different constraints used in the methods to achieve accurate registration results. We present the publicly available data sets and different evaluation techniques employed in the methods. Further, we discuss open challenges, recent trends, and potential directions for future work in this area.",Registration; Point Set; Point Cloud; Optimization; Deformation
"Glaucoma is a condition that causes lifelong visual loss, although it can be avoided if caught early. Computer vision-based techniques can effectively be applied to classify glaucoma stages with Machine Learning (ML) and Artificial Intelligence (AI) techniques. One of the most important elements in glaucoma diagnosis is the ratio of the optic disc to the cup. However, proper disc and cup segmentation remain a difficulty. In this work, new optic disc segmentation and classification techniques are proposed using deep learning and pattern classification neural networks. To perform optical disc segmentation, level set segmentation is used in the first stage in the resized input image. Further, AlexNet is used to perform classification for normal and glaucoma classes. Glaucoma images are further fed to a pattern recognition neural network to classify initial, moderate, or severe classes. Various statistical features and Cup-to-Disc Ratio (CDR) are used to train the neural network. This work is executed with DRISHTI-GS, LAG, and RIM-ONE databases. To validate the performance, sensitivity analysis is performed with different testing and training ratios. Metrics such as Accuracy, Sensitivity, Specificity, Precision, F1 score, and Kappa values are calculated. This work produced Accuracy, Sensitivity, and Specificity of 98.42, 97.6, and 97.5 respectively.",Computer vision; Glaucoma classification; Machine learning (ML); Artificial intelligence (AI); Cup-to-disc ratio (Cdr); AlexNet; Sensitivity analysis
"Incremental learning is one of the most important abilities of human beings. In the age of artificial intelligence, it is the key task to make neural network models as powerful as human beings, to achieve the ability to continuously acquire, fine-tune, and accumulate knowledge while simultaneously avoid catastrophic forgetting. In recent years, by virtue of deep neural networks, incremental learning has been attracting a great deal of attention in the field of computer vision. In this paper, we systematically review the current development of incremental learning and give the overall taxonomy of the incremental learning methods. Specifically, three kinds of mainstream methods, i.e., parameter regularization-based approaches, knowledge distillation-based approaches, and dynamic architecture-based approaches, are surveyed, summarized, and discussed in detail. Furthermore, we comprehensively analyze the performance of data-permuted incremental learning, class-incremental learning, and multi-modal incremental learning on widely used datasets, covering a broad of incremental learning scenarios for image classification and semantic segmentation. Lastly, we point out some possible research directions and inspiring suggestions for incremental learning in the field of computer vision.",Incremental learning; Catastrophic forgetting; Parameter regularization; Knowledge distillation; Dynamic architecture; Computer vision
"Automatic human activity recognition is one of the milestones of smart city surveillance projects. Human activity detection and recognition aim to identify the activities based on the observations that are being performed by the subject. Hence, vision-based human activity recognition systems have a wide scope in video surveillance, health care systems, and human-computer interaction. Currently, the world is moving towards a smart and safe city concept. Automatic human activity recognition is the major challenge of smart city surveillance. The proposed research work employed fine-tuned YOLO-v4 for activity detection, whereas for classification purposes, 3D-CNN has been implemented. Besides the classification, the presented research model also leverages human-object interaction with the help of intersection over union (IOU). An Internet of Things (IoT) based architecture is implemented to take efficient and real-time decisions. The dataset of exploit classes has been taken from the UCF-Crime dataset for activity recognition. At the same time, the dataset extracted from MS-COCO for suspicious object detection is involved in human-object interaction. This research is also applied to human activity detection and recognition in the university premises for real-time suspicious activity detection and automatic alerts. The experiments have exhibited that the proposed multimodal approach achieves remarkable activity detection and recognition accuracy.",
"A growing interest in applying Natural Language Processing (NLP) models to computer vision problems has recently emerged. This interest is motivated by the success of NLP models in tasks such as translation and text summarization. In this paper, we propose a new method for applying NLP to image classification problems. We aim to represent the visual patterns of objects by using a sequence of alphabet symbols and then train a Gated Recurrent Unit (GRU), Long Short-Term Memory (LSTM), or Transformer using these sequences to classify objects. An extensive experimental evaluation using a limited number of images for training has been conducted to compare our method with the ResNet-50 deep learning architecture. The results obtained by the proposed method outperform ResNet-50 in all test scenarios. In one test, the method achieved an average accuracy of 95.3% compared to 89.9% of ResNet-50. The source code ( http:// git.inovisao.ucdb.br/inovisao/applying-npl-to-image-classification) and dataset ( https://doi.org/10. 6084/m9.figshare.20055602.v1) are publicly available. (c) 2022 Elsevier B.V. All rights reserved.",Syntactic pattern recognition; Recurrent neural network; Visual word; Computer vision
"Handwritten character recognition is a computer-vision-system problem that is still critical and challenging in many computer-vision tasks. With the increased interest in handwriting recognition as well as the developments in machine-learning and deep-learning algorithms, researchers have made significant improvements and advances in developing English-handwriting-recognition methodologies; however, Arabic handwriting recognition has not yet received enough interest. In this work, several deep-learning and hybrid models were created. The methodology of the current study took advantage of machine learning in classification and deep learning in feature extraction to create hybrid models. Among the standalone deep-learning models trained on the two datasets used in the experiments performed, the best results were obtained with the transfer-learning model on the MNIST dataset, with 0.9967 accuracy achieved. The results for the hybrid models using the MNIST dataset were good, with accuracy measures exceeding 0.9 for all the hybrid models; however, the results for the hybrid models using the Arabic character dataset were inferior.",classification; convolutional neural network; recurrent neural networks; MNIST; model selection
"Hyperspectral imaging opens up new opportunities for masked face recognition via discrimination of the spectral information obtained by hyperspectral sensors. In this work, we present a novel algorithm to extract facial spectral-features from different regions of interests by performing computer vision techniques over the hyperspectral images, particularly Histogram of Oriented Gradients. We have applied this algorithm over the UWA-HSFD dataset to extract the facial spectral-features and then a set of parallel Support Vector Machines with custom kernels, based on the cosine similarity and Euclidean distance, have been trained on fly to classify unknown subjects/faces according to the distance of the visible facial spectral-features, i.e., the regions that are not concealed by a face mask or scarf. The results draw up an optimal trade-off between recognition accuracy and compression ratio in accordance with the facial regions that are not occluded.",facial recognition; hyperspectral compression; hyperspectral imaging; biometrics; SVM; computer vision
"One-class learning is the classic problem of fitting a model to the data for which annotations are available only for a single class. In this paper, we explore novel objectives for one-class learning, which we collectively refer to as Generalized One-class Discriminative Subspaces (GODS). Our key idea is to learn a pair of complementary classifiers to flexibly bound the one-class data distribution, where the data belongs to the positive half-space of one of the classifiers in the complementary pair and to the negative half-space of the other. To avoid redundancy while allowing non-linearity in the classifier decision surfaces, we propose to design each classifier as an orthonormal frame and seek to learn these frames via jointly optimizing for two conflicting objectives, namely: i) to minimize the distance between the two frames, and ii) to maximize the margin between the frames and the data. The learned orthonormal frames will thus characterize a piecewise linear decision surface that allows for efficient inference, while our objectives seek to bound the data within a minimal volume that maximizes the decision margin, thereby robustly capturing the data distribution. We explore several variants of our formulation under different constraints on the constituent classifiers, including kernelized feature maps. We demonstrate the empirical benefits of our approach via experiments on data from several applications in computer vision, such as anomaly detection in video sequences, human poses, and human activities. We also explore the generality and effectiveness of GODS for non-vision tasks via experiments on several UCI datasets, demonstrating state-of-the-art results.",Anomaly detection; Data models; Task analysis; Kernel; Computer vision; Support vector machines; Manifolds; One-class classification; subspace learning; kernelized subspaces; Riemannian optimization
"Generative adversarial models with convolutional neural network (CNN) backbones have recently been established as state-of-the-art in numerous medical image synthesis tasks. However, CNNs are designed to perform local processing with compact filters, and this inductive bias compromises learning of contextual features. Here, we propose a novel generative adversarial approach for medical image synthesis, ResViT, that leverages the contextual sensitivity of vision transformers along with the precision of convolution operators and realism of adversarial learning. ResViT's generator employs a central bottleneck comprising novel aggregated residual transformer (ART) blocks that synergistically combine residual convolutional and transformer modules. Residual connections in ART blocks promote diversity in captured representations, while a channel compression module distills task-relevant information. A weight sharing strategy is introduced among ART blocks to mitigate computational burden. A unified implementation is introduced to avoid the need to rebuild separate synthesis models for varying source-target modality configurations. Comprehensive demonstrations are performed for synthesizing missing sequences in multi-contrast MRI, and CT images from MRI. Our results indicate superiority of ResViT against competing CNN- and transformer-based methods in terms of qualitative observations and quantitative metrics.",Transformers; Biomedical imaging; Subspace constraints; Task analysis; Image synthesis; Magnetic resonance imaging; Computer architecture; Medical image synthesis; transformer; residual; vision; adversarial; generative; unified
"Massively parallel systolic arrays and resource-efficient depthwise separable convolutions are two promising hardware and software techniques to accelerate DNN inference on the edge. Interestingly, their combination is inefficient: Computational patterns of depthwise separable convolutions do not exhibit a rhythmic systolic flow and lack sufficient data reuse to saturate systolic arrays. In this article, we formally analyse this inefficiency and propose an efficient operator, an optimal hardware dataflow, and a superior training methodology towards alleviating this. The efficient operator, called Fully-Separable Convolutions (FuSeConv),(1) is a drop-in replacement for depthwise-separable convolutions. FuSeConv generalizes factorization of convolution fully along their spatial and depth dimensions. The resultant computation is systolic and efficiently maps to systolic arrays. The optimal hardware dataflow, called Spatial-Tiled Output Stationary (ST-OS), maximizes the efficiency of FuSeConv on systolic arrays. It maps independent convolutions to rows of the systolic array to maximise resource-utilization with negligible VLSI overheads. Neural Operator Scaffolding (NOS) scaffolds the training of FuSeConv operators by distilling knowledge from the more expensive depthwise separable convolution operation. This bridges the accuracy gap between FuSeConv networks and networks with depthwise-separable convolutions. Additionally, NOS can be combined with Neural Architecture Search (NAS) to trade off latency and accuracy. The hardware-software co-design of FuSeConv with ST-OS achieves a significant speedup of 4.1 - 9.25x with state-of-the-art efficient networks for the ImageNet dataset. The parameter efficiency of FuSeConv and its significant superiority over depthwise-separable convolutions on systolic arrays illustrates their promise as a strong solution on the edge. Training FuSeConv networks with NOS achieves accuracy comparable to the depthwise-separable convolution baselines. Further, by combining NOS with NAS, we design networks that define state-of-the-art models improving on both accuracy and latency for computer vision on systolic arrays.",Deep neural networks; hardware-software co-design; systems for deep learning; efficient networks; efficient hardware; computer vision; edge computing
"Convolutional neural networks (CNNs) can be generally regarded as learning-based visual systems for computer vision tasks. By imitating the operating mechanism of the human visual system (HVS), CNNs can even achieve better results than human beings in some visual tasks. However, they are primary when compared to the HVS for the reason that the HVS has the ability of active vision to promptly analyze and adapt to specific tasks. In this article, a new unified pooling framework is proposed and a series of pooling methods are designed based on the framework to implement active vision to CNNs. In addition, an active selection pooling (ASP) is put forward to reorganize the existing and newly proposed pooling methods. The CNN models with an ASP tend to have a behavior of focus selection according to tasks during the training process, which acts extremely similar to the HVS.",Visual systems; Task analysis; Visualization; Training; Convolutional neural networks; Informatics; Image color analysis; Active vision; deep convolutional neural networks (CNNs); deep visual learning; human visual system (HVS); pooling framework
"Recently, transformers have been widely adopted for various computer vision tasks and show promising results due to their ability to encode long-range spatial dependencies in an image effectively. However, very few studies on adopting transformers in self-supervised depth estimation have been conducted. When replacing the CNN architecture with the transformer in self-supervised learning of depth, we encounter several problems such as problematic multi-scale photometric loss function when used with transformers and, insufficient ability to capture local details. In this letter, we propose an attention-based decoder module, Pixel-Wise Skip Attention (PWSA), to enhance fine details in feature maps while keeping global context from transformers. In addition, we propose utilizing self-distillation loss with single-scale photometric loss to alleviate the instability of transformer training by using correct training signals. We demonstrate that the proposed model performs accurate predictions on large objects and thin structures that require global context and local details. Our model achieves state-of-the-art performance among the self-supervised monocular depth estimation methods on KITTI and DDAD benchmarks.",Deep learning for visual perception; computer vision for transportation; visual learning
"In the field of computer vision and robotics, scholars use object tracking technology to track objects of interest in various video streams and extend practical applications, such as unmanned vehicles, self-driving cars, robotics, drones, and security surveillance. Object tracking is a mature technology in the field of computer vision and robotics; however, there is still no one object tracking algorithm that can comprehensively and simultaneously solve the four problems encountered by tracking objects, namely deformation, illumination variation, motion blur, and occlusion. We propose an algorithm called an adaptive dynamic multi-template correlation filter (ADMTCF) which can simultaneously solve the above four difficulties encountered in tracking moving objects. The ADMTCF encodes local binary pattern (LBP) features in the HSV color space, so the encoded features can resist the pollution of the tracking image caused by illumination variation. The ADMTCF has four templates that can be adaptively and dynamically resized to maintain tracking accuracy to combat tracking problems such as deformation, motion blur, and occlusion. In this paper, we experimented with our ADMTCF algorithm and various state-of-the-art tracking algorithms in scenarios such as deformation, illumination variation, motion blur, and occlusion. Experimental results show that our proposed ADMTCF exhibits excellent performance, stability, and robustness in various scenarios.",adaptive-adjustment; local binary patterns; multi-template
"Regular scaffolding quality inspection is an essential part of construction safety. However, current evaluation methods and quality requirements for temporary structures are based on subjective visual inspection by safety managers. Accordingly, the assessment process and results depend on an inspector's competence, experience, and human factors, making objective analysis complex. The safety inspections performed by specialized services bring additional costs and increase evaluation times. Therefore, a temporary structure quality and safety evaluation system based on experts' experience and independent of the human factor is the relevant solution in intelligent construction. This study aimed to present a quality evaluation system prototype for scaffolding parts based on computer vision. The main steps of the proposed system development are preparing a dataset, designing a neural network (NN) model, and training and evaluating the model. Since traditional methods of preparing a dataset are very laborious and time-consuming, this work used mixed real and synthetic datasets modeled in Blender. Further, the resulting datasets were processed using artificial intelligence algorithms to obtain information about defect type, size, and location. Finally, the tested parts' quality classes were calculated based on the obtained defect values.",computer vison; semantic segmentation; scaffolding; synthetic dataset; AI
"Food preparation is one of the essential tasks in daily life and involves a large number of physical interactions between hands, utensils, ingredients, etc. The fundamental unit in the food preparation activity is the concept of a recipe. The recipe describes the cooking process-the way to make a dish in a sequential order of cooking steps. Frequently, following these steps can be an extremely complicated process, which requires coordination, monitoring and execution of multiple tasks simultaneously. This work introduces a cooking assistance system powered by Computer Vision techniques that provide the user with guidance in the accomplishment of a cooking activity in terms of a recipe and its correct execution. The system can provide the user with guidance for carrying out a recipe through the appropriate messages, which appear in a panel specifically designed for the user. Throughout the process, the system can validate the correctness of each step by (a) detection and motion estimation of the ingredients and utensils in the scene and (b) spatial arrangement of them in terms of where each one is located to another. The system was first evaluated on individual algorithmic steps and on the end-to-end execution of two recipes with promising results.",cooking assistance; recipe preparation; intelligent kitchen; object detection; object tracking; computer vision; machine learning
"In this letter, we tackle the problem of active robotic 3D reconstruction of an object. In particular, we study howa mobile robot with an arm-held camera can select a favorable number of views to recover an object's 3D shape efficiently. Contrary to the existing solution to this problem, we leverage the popular neural radiance fields-based object representation, which has recently shown impressive results for various computer vision tasks. However, it is not straightforward to directly reason about an object's explicit 3D geometric details using such a representation, making the next-best-view selection problem for dense 3D reconstruction challenging. This paper introduces a ray-based volumetric uncertainty estimator, which computes the entropy of the weight distribution of the color samples along each ray of the object's implicit neural representation. We show that it is possible to infer the uncertainty of the underlying 3D geometry given a novel view with the proposed estimator. We then present a next-best-view selection policy guided by the ray-based volumetric uncertainty in neural radiance fields-based representations. Encouraging experimental results on synthetic and real-world data suggest that the approach presented in this paper can enable a new research direction of using an implicit 3D object representation for the next-best-view problem in robot vision applications, distinguishing our approach from the existing approaches that rely on explicit 3D geometric modeling.",Active 3D reconstruction; robot vision; neural radiance fields; next-best-view selection; uncertainty estimation
"With the development of deep learning technology and people's demand for intelligent security, human-computer interaction, shopping guide and other technologies, computer vision technology for pedestrian identification shows great application value. In this paper, pedestrian identification method based on multi-scale feature learning in surveillance video images is studied. Firstly, the deep residual network ResNet and densely connected convolutional network DenseNet are introduced as baseline networks. A model is constructed based on hybrid hourglass network module, enhanced weighted feature pyramid fusion network module and post-processing module. The loss function is designed, which is unified with other traditional models, and the optimization objective of the loss function is respectively corresponding to three parts, namely, the prediction error of corresponding center point, the prediction error of offset and the prediction error of bounding box size. The experimental results verify the effectiveness of the proposed model.",multi-scale feature learning; surveillance video; pedestrian identification
"Finger vein recognition has been widely studied due to its advantages, such as high security, convenience, and living body recognition. At present, the performance of the most advanced finger vein recognition methods largely depends on the quality of finger vein images. However, when collecting finger vein images, due to the possible deviation of finger position, ambient lighting and other factors, the quality of the captured images is often relatively low, which directly affects the performance of finger vein recognition. In this study, we proposed a new model for finger vein recognition that combined the vision transformer architecture with the capsule network (ViT-Cap). The model can explore finger vein image information based on global and local attention and selectively focus on the important finger vein feature information. First, we split-finger vein images into patches and then linearly embedded each of the patches. Second, the resulting vector sequence was fed into a transformer encoder to extract the finger vein features. Third, the feature vectors generated by the vision transformer module were fed into the capsule module for further training. We tested the proposed method on four publicly available finger vein databases. Experimental results showed that the average recognition accuracy of the algorithm based on the proposed model was above 96%, which was better than the original vision transformer, capsule network, and other advanced finger vein recognition algorithms. Moreover, the equal error rate (EER) of our model achieved state-of-the-art performance, especially reaching less than 0.3% under the test of FV-USM datasets which proved the effectiveness and reliability of the proposed model in finger vein recognition.",finger vein; biometrics; computer vision; deep learning
"Reflection is common when we see through a glass window, which not only is a visual disturbance but also influences the performance of computer vision algorithms. Removing the reflection from a single image, however, is highly ill-posed since the color at each pixel needs to be separated into two values belonging to the clear background and the reflection, respectively. To solve this, existing methods use additional priors such as reflection layer smoothness, double reflection effect, and color consistency to distinguish the two layers. However, these low-level priors may not be consistently valid in real cases. In this paper, inspired by the fact that human beings can separate the two layers easily by recognizing the objects and understanding the scene, we propose to use the object semantic cue, which is high-level information, as the guidance to help reflection removal. Based on the data analysis, we develop a multi-task end-to-end deep learning method with a semantic guidance component, to solve reflection removal and semantic segmentation jointly. Extensive experiments on different datasets show significant performance gain when using high-level object-oriented information. We also demonstrate the application of our method to other computer vision tasks.",Reflection removal; semantic segmentation; multi-task learning; highlevel guidance; deep learning
"Deep learning has been widely applied in various fields such as computer vision, natural language processing, and data mining. Although deep learning has achieved significant success in solving complex problems, it has been shown that deep neural networks are vulnerable to adversarial attacks, resulting in models that fail to perform their tasks properly, which limits the application of deep learning in security-critical areas. In this paper, we first review some of the classical and latest representative adversarial attacks based on a reasonable taxonomy of adversarial attacks. Then, we construct a knowledge graph based on the citation relationship relying on the software VOSviewer, visualize and analyze the subject development in this field based on the information of 5923 articles from Scopus. In the end, possible research directions for the development about adversarial attacks are proposed based on the trends deduced by keywords detection analysis. All the data used for visualization are available at: https://github.com/NanyunLengmu/Adversarial-Attack-Visualization. (C) 2022 Elsevier Ltd. All rights reserved.",Deep learning; Adversarial attack; Black-box attack; White-box attack; Robustness; Visualization analysis
"With the development of semiconductor assembly technology, the continuous requirement for the improvement of chip quality caused an increasing pressure on the assembly manufacturing process. The defects of chip pin had been mostly verified by manual inspection, which has low efficiency, high cost, and low reliability. In this paper, we propose a vision measurement method to detect the chip pin defects, such as the pin warping and collapse that heavily influence the quality of chip assembly. This task is performed by extracting the corner feature of the chip pins, computing the corresponding point pairs in the binocular sequence images, and reconstructing the target features of the chip. In the corner feature step, the corner detection of the pins using the gradient correlation matrices (GCM), and the feature point extraction of the chip package body surface using the crossing points of the fitting lines are introduced, respectively. After obtaining the corresponding point pairs, the feature points are utilized to reconstruct the three dimensional (3D) coordinate information in the binocular vision measurement system, and the key geometry dimension of the pins is computed, which reflects whether the quality of the chip pins is up to the standard. The proposed method is evaluated on the chip data, and the effectiveness is also verified by the comparison experiments.",3D reconstruction; chip pin; defect detection; feature extraction; computer vision
"In the past years, deep neural networks (DNNs) have become popular in many disciplines such as computer vision (CV). One of the most important challenges in the CV area is Medical Image Analysis (MIA). However, adversarial attacks (AdAs) have proven to be an important threat to vision systems by significantly reducing the performance of the models. This paper proposes a new black-box adversarial attack, which is based omicron n orthogonal image moments named Mb-AdA. Additionally, a corresponding defensive method of adversarial training using Mb-AdA adversarial examples is also investigated, with encouraging results. The proposed attack was applied in classification and segmentation tasks with six state-of-the-art Deep Learning (DL) models in X-ray, histopathology and nuclei cell images. The main advantage of Mb-AdA is that it does not destroy the structure of images like other attacks, as instead of adding noise it removes specific image information, which is critical for medical models' decisions. The proposed attack is more effective than compared ones and achieved degradation up to 65% and 18% in terms of accuracy and IoU for classification and segmentation tasks, respectively, by also presenting relatively high SSIM. At the same time, it was proved that Mb-AdA adversarial examples can enhance the robustness of the model.",adversarial attack; medical image analysis; computer vision; deep learning; adversarial training; robustness; image moments
,
"Action quality assessment (AQA) is an important problem in computer vision applications. During human AQA, differences in body size or changes in position relative to the sensor may cause unwanted effects. We propose a motion registration method based on self-coordination (SC) and self-referential normalization (SRN). By establishing a coordinate system on the human body and using a part of the human body as a normalized reference standard to process the raw data, the standardization and distinguishability of the raw data are improved. To demonstrate the effectiveness of our method, we conducted experiments on KTH datasets. The experimental results show that the method improved the classification accuracy of the KNN-DTW network for KTH-5 from 82.46% to 87.72% and for KTH-4 from 89.47% to 94.74%, and it improved the classification accuracy of the tsai-MiniRocket network for KTH-5 from 91.29% to 93.86% and for KTH-4 from 94.74% to 97.90%. The results show that our method can reduce the above effects and improve the action classification accuracy of the action classification network. This study provides a new method and idea for improving the accuracy of AQA-related algorithms.",computer vision; AQA; motion registration; action feature fusion method; SC; SRN
"Modern design codes ensure a large displacement capacity and prevent total collapse for bridges under earth-quakes. However, this performance objective is usually attained at the cost of damage to target ductile members. For conventional reinforced concrete (RC) bridges, columns are usually the main source of ductility during an earthquake in which concrete cover, core, and reinforcement may damage, and the column may experience a large permanent lateral deformation. Visual inspection is currently the preferred method of bridge assessment after an event. Nevertheless, sending inspectors to all affected bridges is time consuming and logistically chal-lenging. An alternative method may save time, costs, and lives. The main goal of the present study was to accelerate post-earthquake assessment of standard RC bridge columns using computer vision and seismic ana-lyses. Standard columns are those that are designed following seismic requirements. To achieve the project goal, a new quantitative definition was proposed for RC bridge column damage states suited for computer program-ming, the most comprehensive experimental database of standard RC bridge columns consisting of 290 speci-mens was compiled, then the database was statistically analyzed to relate the proposed column damage states to displacement demands. Furthermore, an artificial intelligence enabled software was developed to quickly detect RC bridge column damages, to comment on the column damage state, and to tag the column. A framework was proposed to assess the serviceability of standard RC bridge columns after earthquakes.",Post -Earthquake Conditions; Damage Assessment; Serviceability Assessment; RC Bridge Columns; Computer Vision; Artificial Intelligence; Seismic Analysis
"Convolutional neural networks (CNNs) have gained a massive impression in the fields of computer vision and especially in the embedded applications because of their high accuracy and performance. However, high computational complexity and power consumption due to convolution operations causes a high demand for low-power accelerators. A 3D geometric optimization strategy is proposed to alleviate the area and power requirements of Multiply Accumulate operations prevalent in all spatial CNNs. The proposed technique is generic and may be easily scaled for accelerators performing spatial 2D convolution.",Convolutional neural networks; approximate computing; hardware accelerator; embedded platforms
"While the deep learning-based image deraining methods have made great progress in recent years, there are two major shortcomings in their application in real-world situations. Firstly, the gap between the low-level vision task represented by rain removal and the high-level vision task represented by object detection is significant, and the low-level vision task can hardly contribute to the high-level vision task. Secondly, the quality of the deraining dataset needs to be improved. In fact, the rain lines in many baselines have a large gap with the real rain lines, and the resolution of the deraining dataset images is generally not ideal. Meanwhile, there are few common datasets for both the low-level vision task and the high-level vision task. In this letter, we explore the combination of the low-level vision task with the high-level vision task. Specifically, we propose an end-to-end object detection network for reducing the impact of rainfall, which consists of two cascaded networks, an improved image deraining network and an object detection network, respectively. We also design the components of the loss function to accommodate the characteristics of the different sub-networks. We then propose a dataset based on the KITTI dataset for rainfall removal and object detection, on which our network surpasses the state-of-the-art with a significant improvement in metrics. Besides, our proposed network is measured on driving videos collected by self-driving vehicles and shows positive results for rain removal and object detection.",Deep learning for visual perception; computer vision for transportation; visual tracking; image deraining
"Video captioning is a challenging task as it needs to accurately transform visual understanding into natural language description. To date, state-of-the-art methods inadequately model global-local vision representation for sentence generation, leaving plenty of room for improvement. In this work, we approach the video captioning task from a new perspective and propose a GLR framework, namely a global-local representation granularity. Our GLR demonstrates three advantages over the prior efforts. First, we propose a simple solution, which exploits extensive vision representations from different video ranges to improve linguistic expression. Second, we devise a novel global-local encoder, which encodes different video representations including long-range, short-range and local-keyframe, to produce rich semantic vocabulary for obtaining a descriptive granularity of video contents across frames. Finally, we introduce the progressive training strategy which can effectively organize feature learning to incur optimal captioning behavior. Evaluated on the MSR-VTT and MSVD dataset, we outperform recent state-of-the-art methods including a well-tuned SA-LSTM baseline by a significant margin, with shorter training schedules. Because of its simplicity and efficacy, we hope that our GLR could serve as a strong baseline for many video understanding tasks besides video captioning. Code will be available.",Training; Task analysis; Visualization; Vocabulary; Semantics; Decoding; Correlation; Computer vision; video captioning; video representation; natural language processing; visual analysis
"The reality of the ray tracing technology that leads to its rendering effect is becoming increasingly apparent in computer vision and industrial applications. However, designing efficient ray tracing hardware is challenging due to memory access issues, divergent branches, and daunting computation intensity. This article presents a novel architecture, a RT engine (Ray Tracing engine), that accelerates ray tracing. First, we set up multiple stacks to store information for each ray so that the RT engine can process many rays parallel in the system. The information in these stacks can effectively improve the performance of the system. Second, we choose the three-phase break method during the triangle intersection test, which can make the loop break earlier. Third, the reciprocal unit adopts the approximation method, which combines Parabolic Synthesis and Second-Degree interpolation. Combined with these strategies, we implement our system at RTL level with agile chip development. Simulation and experimental results show that our architecture achieves a performance per area which is 2.4 x greater than the best reported results for ray tracing on dedicated hardware.",machine vision; computer graphics; hardware architecture; rendering; ray tracing; graphics accelerators
"The quality of images captured in rainy days is severely degraded, which affects the accuracy of subsequent computer vision tasks. Recently, many deep learning-based methods have demonstrated superior performance for single image deraining. However, there are still many issues left. Since real -world rain images and their corresponding ground truths are difficult to collect, models trained on limited data may lead to overfitting. Meanwhile, although many methods can remove part of the rain streaks, most of them cannot reconstruct precise edges and textures. For the first issue, we use the transfer learning approach. Loading pre-trained parameters trained on the ImageNet enables the network to have robust feature representation, which improves the generalization of the network. For the second issue, we restore clear details by making full use of the frequency domain information of the image. Specifically, we design a novel frequency domain residual block (FRDB) and use an efficient fusion strategy in FRDB to fuse spatial and frequency domain features. Then, we propose a frequency domain reconstruction loss function (FDR loss) to restore details by reducing the differences in high-frequency space. Finally, a simple detail enhancement attention module (DEAM) is used to further enhance the image details. Extensive experimental results demonstrate that our DPNet has superior performance on both synthetic and real data. Furthermore, we verify the effectiveness of our method on downstream computer vision tasks. The source codes will be open at https://github .com /noxsine /DPNet.(c) 2022 Elsevier Inc. All rights reserved.",Single image deraining; Frequency domain; Transfer learning; Image restoration; Neural networks
"Slow detection of redundant objects and low accuracy in assembly lines, particularly in the setting of civil aircraft assembly, are tough and challenging problems. To address these issues, a redundant object detection method based on computer vision and augmented reality (AR) smart glasses is proposed in this paper. The method uses AR glasses as the image collection hardware and takes the live image collected by the camera as the input of the proposed deep learning machine vision model. The proposed model, the Feature Pyramid Networks-CenterNet, is inspired by CenterNet and combined with multi-scale feature fusion to solve the problem of low detection accuracy of small-scale redundant targets. The weight factor of the loss function was set according to the proportion of small targets in the dataset, which solves the problem of an unbalanced proportion of large and small targets in the training samples. The proposed network model was validated on the PASCAL Visual Object Classes public dataset and the self-built redundant object dataset. The results showed that the new method can detect seven redundant objects with a mean accuracy of 74.49% within the visible range of smart glasses within 200 ms. The research provides a new reference for the quality process management of civil aircraft assembly.",machine vision; smart glasses; civil aircraft assembly; redundant objects detection
"Cracks are the main damages of concrete structures. Since cracks may occur in areas that are difficult to reach, non-contact measurement technology is required to accurately measure the width of cracks. This study presents an innovative computer vision system combining a camera and laser rangefinder to measure crack width from any angle and at a long distance. To solve the problem of pixel distortion caused by non-vertical photographing, geometric transformation formulas that can calculate the unit pixel length of the image captured at any angle are proposed. The complexity of crack edge calculation and the imbalance of data in the image are other problems that affect measurement accuracy, and a combination of the improved U-net convolutional networks algorithm and Canny edge detection method is adopted to accurately extract the cracks. The measurement results on the different concrete wall indicate that the proposed system can measure the crack in a non-vertical position, and the proposed algorithm can extract the crack from different background images. Although the proposed system cannot achieve fully automated measurement, the results also confirm the ability to obtain the crack width accurately and conveniently.",computer vision; crack measurement; U-net; convolutional neural network; structural health monitoring; artificial intelligence
"Since the rise of convolutional neural networks (CNN), deep learning-based computer vision has been a dynamic field of research. Nevertheless, modern CNN architectures have not given sufficient consideration to real-time applications within limited computation settings and always compromise speed and accuracy. To this end, a novel approach to CNN design, based on the emerging technology of compressive sensing (CS), is proposed. For instance, CS networks function in a compression-reconstruction approach as an encoder-decoder neural network. This approach transforms the computer vision problem into a multioutput learning problem by incorporating the CS network into a recognition network for joint training. As to the deployment phase, images are obtained from a CS-acquisition device and fed directly, without reconstruction, to the new recognition network. Following such an approach considerably improves transmission bandwidth and reduces the computational burden. Furthermore, the redesigned CNN holds fewer parameters than its original counterpart, thus reducing model complexity. To validate our findings, object detection using the Single-Shot Detector (SSD) network was redesigned to operate in our CS-based ecosystem using different datasets. The results show that the lightweight CS network offers good performance at a faster running speed. For instance, the number of FLOPS was reduced by 57% compared to the SSD baseline. Furthermore, the proposed CS_SSD achieves a compelling accuracy while being 30% faster than its original counterpart on small GPUs. Code is available at: littps://github.com/Bouderbal-Imene/CS-SSD.",Computer vision; Real-time; Convolutional neural network; Compressive sensing; SSD; Embedded systems; Robots
"Computer vision is the science that enables computers and machines to see and perceive image content on a semantic level. It combines concepts, techniques, and ideas from various fields such as digital image processing, pattern matching, artificial intelligence, and computer graphics. A computer vision system is designed to model the human visual system on a functional basis as closely as possible. Deep learning and Convolutional Neural Networks (CNNs) in particular which are biologically inspired have significantly contributed to computer vision studies. This research develops a computer vision system that uses CNNs and handcrafted filters from Log-Gabor filters to identify medicinal plants based on their leaf textural features in an ensemble manner. The system was tested on a dataset developed from the Centre of Plant Medicine Research, Ghana (MyDataset) consisting of forty-nine (49) plant species. Using the concept of transfer learning, ten pretrained networks including Alexnet, GoogLeNet, DenseNet201, Inceptionv3, Mobilenetv2, Restnet18, Resnet50, Resnet101, vgg16, and vgg19 were used as feature extractors. The DenseNet201 architecture resulted with the best outcome of 87% accuracy and GoogLeNet with 79% preforming the worse averaged across six supervised learning algorithms. The proposed model (OTAMNet), created by fusing a Log-Gabor layer into the transition layers of the DenseNet201 architecture achieved 98% accuracy when tested on MyDataset. OTAMNet was tested on other benchmark datasets; Flavia, Swedish Leaf, MD2020, and the Folio dataset. The Flavia dataset achieved 99%, Swedish Leaf 100%, MD2020 99%, and the Folio dataset 97%. A false-positive rate of less than 0.1% was achieved in all cases.",
"As an indispensable part in the field of computer vision, target tracking has been widely used in intelligent transportation, missile guidance, unmanned aerial vehicle (UAV) tracking, and many other fields. It has become one of the hot directions in computer vision in recent years, while occlusion problem has always been a great difficulty and challenge in the process of target tracking. In this article, the problem of occlusion interference in target tracking is described, and the solution of occlusion problem is proposed based on different occlusion conditions. Due to the disadvantages of feature point center weighting, multiparticle template matching, and Kalman filter trajectory prediction algorithms in different cases, some algorithms with higher robustness and stability are developed to solve the occlusion problem. In the analysis of the anti-occlusion model, it is found that some tracking errors caused by occlusion can be solved by improving the quality of negative training samples and enriching the diversity of positive sample sets. According to the different training characteristics of online and offline tracking algorithms, the anti-occlusion model suitable for an active learning algorithm under different tracking conditions is found, and the tracking algorithm and characteristics of the active learning algorithm are listed, which is helpful to select the suitable tracking model in different scenarios. Finally, the future development of occlusion problem in target tracking is prospected.",
"Image caption is a popular research direction in computer vision. It is a task that enables machines to convey the computer's perception and cognition of vision to the outside world in the form of human language. Currently, the most dominant models are Transformer-based architectures which achieve the cutting-edge performance. Inspired by the distinguished meshed-memory transformer model which uses a mesh-like connectivity at decoding stage. It let us see more possibilities in the Transformer model. With the aim to explore more possible connectivity schemas in Transformer, we propose the input enhanced asymmetric transformer (IEAT) model. It improves the connectivity between encoder layers and optimizes the generation effect of the captions. To better evaluate the final effect of our model, we conducted extensive experiments (offline evaluation, online evaluation and ablation study) on the MS-COCO benchmark and the Karpathy test split. And the results show that IEAT outperforms the previously proposed models to generate satisfactory image captions.",Image caption; Adaptive sparse attention; Vision; Language pretraining
"Automatically understanding the content of medical images and delivering accurate descriptions is an emerging field of artificial intelligence that combines skills in both computer vision and natural language processing fields. Medical image captioning is involved in various applications related to diagnosis, treatment, report generation and computer-aided diagnosis to facilitate the decision making and clinical workflows. Unlike generic image captioning, medical image captioning highlights the relationships between image objects and clinical findings, which makes it a very challenging task. Although few review papers have already been published in this field, their coverage is still quite limited and only particular problems are addressed. This motivates the current paper where a rapid review protocol was adopted to review the latest achievements in automatic medical image captioning from the medical domain perspective. We aim through this review to provide the reader with an up-to-date literature in this field by summarizing the key findings and approaches in this field, including the related datasets, applications and limitations as well as highlighting the main competitions, challenges and future directions.",Automatic image captioning; Caption; Diagnosis generation; Medical images; Rapid review; Report generation; PRISMA
"After a major earthquake, rapid community recovery is conditional on ensuring buildings are safe to reoccupy. Prior studies have developed statistical and machine learning-based classifiers to characterize a building's collapse capacity to resist an aftershock given mainshock responses of the building. However, for rapid safety assessment, such a method must be coupled with an automated inspection methodology to collect damage information. Furthermore, probabilistic models of expected building performance must be updated based on the distribution of observed damage. This paper presents a method for rapidly assessing the safety of a building by incorporating damage that has been identified and localized using unmanned aerial vehicle images of the building. Probabilistic models of earthquake demands on exterior components are directly updated using observed damage and Bayes' Theorem. Updated demand models on interior components are then inferred using a machine learning-based surrogate for the analysis model. Both sets of updated models are used to determine if the building is safe to occupy. Results show that predictions of building demands are improved when considering the observed damage. When combined with automated image collection and processing, the proposed methodology will enable rapid, automated safety assessment of earthquake-affected buildings.",earthquake engineering; structural health monitoring; machine learning; computer vision
"In the past decade, Deep Convolutional Neural Network (DCNN) achieved the state-of-the-art performance in computer vision tasks. However, DCNN is usually treated as a black box'', whose internal working principle is hard to understand. This drawback significantly limits its usage in real-world applications, e.g., vision-based Structural Health Monitoring (SHM), where wrong predictions may lead to catastrophic consequences. To resolve this problem, a framework for the interpretation of the Deep Learning (DL) results called Structural Image Guided Map Analysis Box (SIGMA-Box or Sigma-Box) is proposed. In the. Sigma-Box, visual interpretation results (saliency maps) are produced and used for model quality evaluation along with human experts' domain knowledge. In this study, the use of the. Sigma-Box is explored in vision-based SHM applications. Firstly, understanding trained DCNN's performance in concrete cover spalling detection is investigated. Besides, learning procedure at different epochs, learned feature from different network depths, influence of training techniques, and level of semantic abstraction are studied. The experiments demonstrate the good interpretable performance of the. Sigma-Box which facilitates the understanding of the DCNN models' recognition capabilities, preferences, and limitations. In conclusion, this study sheds light on the high potential of interpreting the trained DCNN in vision-based SHM, providing confidence to the engineers for practical engineering applications involving DL.",Deep learning; Explainable artificial intelligence; Structural health monitoring; Guided map analysis; Visual interpretation
"Extraction of chemical formulas from images was not in the top priority of Computer Vision tasks for a while. The complexity both on the input and prediction sides has made this task challenging for the conventional Artificial Intelligence and Machine Learning problems. A binary input image which might seem trivial for convolutional analysis was not easy to classify, since the provided sample was not representative of the given molecule: to describe the same formula, a variety of graphical representations which do not resemble each other can be used. Considering the variety of molecules, the problem shifted from classification to that of formula generation, which makes Natural Language Processing (NLP) a good candidate for an effective solution. This paper describes the evolution of approaches from rule-based structure analyses to complex statistical models, and compares the efficiency of models and methodologies used in the recent years. Although the latest achievements deliver ideal results on particular datasets, the authors mention possible problems for various scenarios and provide suggestions for further development.",InChI; Chemical images; Molecules; Computer vision; Transformers; ViT; LSTM; Attention; Augmentation
"With the importance people attach to the harmony of ecosystem in modern society, the concept of ecological design has been adopted in some industries and is gradually being valued and recognized by people. In real life, advertising design and production is an important field of art design. With the rapid development of society and the continuous updating of network information technology, network advertising and visual communication design inevitably collide. As a new form of advertising, network advertising had become a part of people's network life. This paper analyzes the technical characteristics of image transformation, image enhancement, image restoration, image coding, and image recognition in computer vision processing. Through the image processing algorithm and program of computer, the advertising image can be processed to realize the functions of recognition, restoration, coding, enhancement, and transformation of advertising image. This thesis had studied the visual communication design in modern online advertising under the concept of ecological design. Using computer vision processing technology, this thesis had studied how to process the ecological concept and advertising visual communication content through computer vision technology, which will help to better improve the configuration of online advertising and create a more pleasant visual environment. It is of great significance to promote the rapid development of online advertising industry.",
"Advances in microscopy, computer vision and open source software are converging to usher in a new era of microscopes that control themselves.",
"Intrinsic image decomposition is the decomposition of an image into its reflectance and shading components. The intrinsic image decomposition problem is inherently ill-posed, since there can be multiple solutions to compute the intrinsic components forming the same image. In this paper, we explore the use of physics-based priors. We also propose a new architecture that separates the learning components in a stacked manner. We explore various ways of integrating such priors into a deep learning system. Our method is trained and tested on a large synthetic garden dataset to assess its performance. It is evaluated and compared to state-of-the-art methods using two standard intrinsic datasets. Finally, the pre-trained network is tested on real world images to show the generalisation capabilities of the network.",Computer vision; Physics based vision; Intrinsics image decomposition; Deep learning
"Computer vision models are currently making great strides in object detection with the rapid development of deep convolutional detectors. However, generating a large number of anchors is an indispensable step in the object detection models for locating targets, which inevitably leads to redundant detections and low computational efficiency. Detecting contours in an image is a fundamental cognitive ability in human vision system, which offers effective evidences for object detection. This paper proposes a novel and simple method by utilizing the distribution of line segments to facilitate the Non-Maximum Suppression (NMS) for the object detection models. Multiple differentiated metrics are designed for the overlap measure between bounding boxes. As a post -processing technique, the proposed segment-based NMS can be easily applied by various models. Furthermore, the proposed method is verified on multiple benchmarks and extensive experiments have been implemented to illustrate its effectiveness. (C) 2022 Published by Elsevier B.V.",Object detection pipelines; Line -segment -based metrics; Non maximum regression; Post -processing technique; Duplicated detection elimination
"Pests cause heavy crop losses, so it is vital to conduct early pest management and control in precision agriculture. In general, pest monitoring is a foundation for early pest management and control. Conventional pest monitoring using manual sampling and detection is time consuming and labour intensive. Therefore, many studies have explored how to achieve automatic pest monitoring. However, few works have focused on automatic monitoring of flying vegetable insect pests. To close this gap, this study developed an automatic monitoring scheme for flying vegetable insect pests based on two hypotheses: (1) yellow sticky traps could provide reliable information to assess population density of flying vegetable insect pests, and (2) a computer-vision-based detector could accurately detect pests in images. Specifically, yellow sticky traps were exploited to sample flying vegetable insect pests, and an RGB camera was adopted to capture yellow-sticky-trap images; and a computer-vision-based detector called YOLO for Small Insect Pests (YOLO-SIP) was used to detect pests in captured images. The hypotheses were tested by using the Heuristics engineering method, installing yellow sticky traps and RGB cameras in vegetable fields, constructing a manually labelled image dataset, and applying YOLO-SIP to the constructed dataset with the mean average precision (mAP), average mean absolute error (aMAE), and average mean square error (aMSE) metrics. Experiments showed that the proposed scheme captured yellow-sticky-trap images automatically and obtained an mAP of 84.22%, an aMAE of 0.422, and an aMSE of 1.126. Thus, the proposed scheme is promising for the automatic monitoring of flying vegetable insect pests.",Early pest management and control; Insect pest monitoring; Automatic; Yellow sticky traps; Computer-vision-based detector
"The increasingly mature computer vision (CV) technology represented by convolutional neural networks (CNN) and available high-resolution remote sensing images (HR-RSIs) provide opportunities to accurately measure the evolution of natural and artificial environments on Earth at a large scale. Based on the advanced CNN method high-resolution net (HRNet) and multi-temporal HR-RSIs, a framework is proposed for monitoring a green evolution of courtyard buildings characterized by their courtyards being roofed (CBR). The proposed framework consists of an expert module focusing on scenes analysis, a CV module for automatic detection, an evaluation module containing thresholds, and an output module for data analysis. Based on this, the changes in the adoption of different CBR technologies (CBRTs), including light-translucent CBRTs (LT-CBRTs) and non-light-translucent CBRTs (NLT-CBRTs), in 24 villages in southern Hebei were identified from 2007 to 2021. The evolution of CBRTs was featured as an inverse S-curve, and differences were found in their evolution stage, adoption ratio, and development speed for different villages. LT-CBRTs are the dominant type but are being replaced and surpassed by NLT-CBRTs in some villages, characterizing different preferences for the technology type of villages. The proposed research framework provides a reference for the evolution monitoring of vernacular buildings, and the identified evolution laws enable to trace and predict the adoption of different CBRTs in a particular village. This work lays a foundation for future exploration of the occurrence and development mechanism of the CBR phenomenon and provides an important reference for the optimization and promotion of CBRTs.",courtyard buildings; evolution; deep learning; high-resolution network; remote sensing images
"Detection of regions of interest (ROIs) in whole slide images (WSIs) in a clinical setting is a highly subjective and a labor-intensive task. In this work, recent developments in machine learning and computer vision algorithms are presented to assess their possible usage and performance to enhance and accelerate clinical pathology procedures, such as ROI detection in WSIs. In this context, a state-of-the-art deep learning framework (Detectron2) was trained on two cases linked to the TUPAC16 dataset for object detection and on the JPATHOL dataset for instance segmentation. The predictions were evaluated against competing models and further possible improvements are discussed.",machine learning; computer vision; digital pathology; object detection; instance segmentation; breast cancer
"Convolutional Neural Networks (CNNs) are currently widely used in various fields, particularly for computer vision applications. Edge platforms have drawn tremendous attention from academia and industry due to their ability to improve execution time and preserve privacy. However, edge platforms struggle to satisfy CNNs' needs due to their computation and energy constraints. Thus, it is challenging to find the most efficient CNN that respects accuracy, time, energy, and memory footprint constraints for a target edge platform. Furthermore, given the size of the design space of CNNs and hardware platforms, performance evaluation of CNNs entails several efforts. Consequently, designers need tools to quickly explore large design space and select the CNN that offers the best performance trade-off for a set of hardware platforms. This article proposes a Machine Learning (ML)-based modeling approach for CNN performances on edge GPU-based platforms for vision applications. We implement and compare five of the most successful ML algorithms for accurate and rapid CNN performance predictions on three different edge GPUs in image classification. Experimental results demonstrate the robustness and usefulness of our proposed methodology. For three of the five ML algorithms - XGBoost, Random Forest, and Ridge Polynomial regression - average errors of 11%, 6%, and 8% have been obtained for CNN inference execution time, power consumption, and memory usage, respectively.",Performance modeling; CNN; edge GPU; execution time; power consumption; memory usage; machine learning; regression analysis
"Obstacle detection is the basis for the Advanced Driving Assistance System (ADAS) to take obstacle avoidance measures. However, it is a very essential and challenging task to detect unexpected obstacles on the road. To this end, an unexpected obstacle detection method based on computer vision is proposed. We first present two independent methods for the detection of unexpected obstacles: a semantic segmentation method that can highlight the contextual information of unexpected obstacles on the road and an open-set recognition algorithm that can distinguish known and unknown classes according to the uncertainty degree. Then, the detection results of the two methods are input into the Bayesian framework in the form of probabilities for the final decision. Since there is a big difference between semantic and uncertainty information, the fusion results reflect the respective advantages of the two methods. The proposed method is tested on the Lost and Found dataset and evaluated by comparing it with the various obstacle detection methods and fusion strategies. The results show that our method improves the detection rate while maintaining a relatively low false-positive rate. Especially when detecting unexpected long-distance obstacles, the fusion method outperforms the independent methods and keeps a high detection rate.",unexpected obstacle detection; computer vision; semantic segmentation; open-set recognition algorithm; uncertainty degree; Bayesian fusion
"In the desert region of northwest China, the frequency of wind-sand disasters is high. All types of concrete buildings built in this area face severe wind erosion due to high wind speed, resulting in varying degrees of wind erosion damage to concrete. To accomplish intelligent identification of concrete wind-erosion damage, a concrete wind erosion experiment was conducted in the laboratory, and a concrete wind-erosion damage dataset was generated under the interference of water stains, scratches, shooting distance, and background noise. This paper combined with transformer theory to improve YOLO-v4 and proposed an object detection algorithm called MHSA-YOLOv4 suitable for wind-erosion damage of concrete. The results demonstrate that MHSA-YOLOv4 exhibits improved object detection performance than YOLO-v3, improved YOLO-v3, and YOLO-v4. On the test set, ACC, Precision, Recall, and mAP of MHSA-YOLOv4 are 91.30%, 91.52%, 92.31%, and 0.89, respectively. MHSA-YOLOv4 can accurately identify wind-erosion damage of concrete images under different test conditions, which reflects strong robustness. The applicability of computer vision technology to the intelligent identification of wind-erosion damage on concrete has been verified.",Deep learning; Wind-erosion damage; Computer vision; Concrete
"Recent advances in convolutional neural networks and vision transformers have brought about a revolution in the area of computer vision. Studies have shown that the performance of deep learning-based models is sensitive to image quality. The human visual system is trained to infer semantic information from poor quality images, but deep learning algorithms may find it challenging to perform this task. In this paper, we study the effect of image quality and color parameters on deep learning models trained for the task of semantic segmentation. One of the major challenges in benchmarking robust deep learning-based computer vision models is lack of challenging data covering different quality and colour parameters. In this paper, we have generated data using the subset of the standard benchmark semantic segmentation dataset (ADE20K) with the goal of studying the effect of different quality and colour parameters for the semantic segmentation task. To the best of our knowledge, this is one of the first attempts to benchmark semantic segmentation algorithms under different colour and quality parameters, and this study will motivate further research in this direction. (c) 2022 Society for Imaging Science and Technology.",
"Social distancing measures are proposed as the primary strategy to curb the spread of the COVID-19 pandemic. Therefore, identifying situations where these protocols are violated has implications for curtailing the spread of the disease and promoting a sustainable lifestyle. This paper proposes a novel computer vision-based system to analyze CCTV footage to provide a threat level assessment of COVID-19 spread. The system strives to holistically interpret the information in CCTV footage spanning multiple frames to recognize instances of various violations of social distancing protocols, across time and space, as well as identification of group behaviors. This functionality is achieved primarily by utilizing a temporal graph-based structure to represent the information of the CCTV footage and a strategy to holistically interpret the graph and quantify the threat level of the given scene. The individual components are evaluated in a range of scenarios, and the complete system is tested against human expert opinion. The results reflect the dependence of the threat level on people, their physical proximity, interactions, protective clothing, and group dynamics, with a system performance of 76% accuracy.",COVID-19; computer vision; surveillance; artificial intelligence
"One of the most important and challenging research subjects in computer vision is visual object tracking. The information obtained from the first frame consists of limited and insufficient information to represent an object. If prior information about robust representation that can represent an object well is not sufficient, object tracking fails when not robustly responding to changes in features of the target object according to various factors, namely shape, illumination variation, and scene distortion. In this paper, a real-time single object tracking algorithm is proposed based on a Siamese network to solve this problem. For the object feature extraction, we designed a fully convolutional neural network that removes a fully connected layer and configured a convolution block consisting of a bottleneck structure that preserves the information in a previous layer. This network was designed as a Siamese network, while a regional proposal network was combined at the end of the network for object tracking. The ImageNet Large-Scale Visual Recognition Challenge 2017 dataset was used to train the network in the pre-training phase. Then, in the experimental phase, the object tracking benchmark dataset was used to quantitatively evaluate the network. The experimental results revealed that the proposed tracking algorithm produced more competitive results compared to other tracking algorithms.",object tracking; convolution neural network; AI; siamese network; image similarity; CUDA; Python; PyTorch; computer vision
"Smart tourism is a developing industry, and numerous nations are planning to establish smart cities in which technology is employed to make life easier and link nearly everything. Many researchers have created object detectors; however, there is a demand for lightweight versions that can fit into smartphones and other edge devices. The goal of this research is to demonstrate the notion of employing a mobile application that can detect statues efficiently on mobile applications, and also improve the performance of the models by employing the Gaussian Smoothing Filter (GSF). In this study, three object detection models, EfficientDet-D0, EfficientDet-D2 and EfficientDet-D4, were trained on original and smoothened images; moreover, their performance was compared to find a model efficient detection score that is easy to run on a mobile phone. EfficientDet-D4, trained on smoothened images, achieves a Mean Average Precision (mAP) of 0.811, an mAP-50 of 1 and an mAP-75 of 0.90.",smart cities; computer vision; object detection; mobile application
"With the continuous development of computer vision technology, moving object detection technology has been paid enough attention and made great progress. Many new methods and new equipment have been developed. As an important part of computer vision, it has important applications in battlefield reconnaissance, video surveillance, image compression and retrieval, human-computer interaction and other research fields, moving object detection and tracking algorithm has always been a research hotspot. We mainly study the panoramic multitarget real-time detection based on machine vision and deep learning. By studying the principle of multitarget real-time detection based on machine vision and deep learning, the panoramic multitarget real-time detection model based on machine vision and deep learning is determined. The principle and correction effect of existing image distortion correction algorithm are analyzed, and the existing problems are summarized. Aiming at the problem that the existing image distortion correction effect is not good, a new method based on machine vision and deep learning is proposed. A real-time panoramic multitarget detection method based on degree learning is proposed. The experimental results show that when the target is moving at medium speed and slow speed, the success rate of tracking is 97% and 95%, respectively; the probability of successful target detection is 100% and 97%, respectively. Experimental results show that the improved method can solve the problem of particle degradation and improve the accuracy of real-time detection. (c) 2022 SPIE and IS&T",deep learning; image processing; machine vision; multitarget real-time detection
"Human detection is a special application of object recognition and is considered one of the greatest challenges in computer vision. It is the starting point of a number of applications, including public safety and security surveillance around the world. Human detection technologies have advanced significantly in recent years due to the rapid development of deep learning techniques. Despite recent advances, we still need to adopt the best network-design practices that enable compact sizes, deep designs, and fast training times while maintaining high accuracies. In this article, we propose ReSTiNet, a novel compressed convolutional neural network that addresses the issues of size, detection speed, and accuracy. Following SqueezeNet, ReSTiNet adopts the fire modules by examining the number of fire modules and their placement within the model to reduce the number of parameters and thus the model size. The residual connections within the fire modules in ReSTiNet are interpolated and finely constructed to improve feature propagation and ensure the largest possible information flow in the model, with the goal of further improving the proposed ReSTiNet in terms of detection speed and accuracy. The proposed algorithm downsizes the previously popular Tiny-YOLO model and improves the following features: (1) faster detection speed; (2) compact model size; (3) solving the overfitting problems; and (4) superior performance than other lightweight models such as MobileNet and SqueezeNet in terms of mAP. The proposed model was trained and tested using MS COCO and Pascal VOC datasets. The resulting ReSTiNet model is 10.7 MB in size (almost five times smaller than Tiny-YOLO), but it achieves an mAP of 63.74% on PASCAL VOC and 27.3% on MS COCO datasets using Tesla k80 GPU.",computer vision; object detection; human detection; convolutional neural networks
"Object detection is a common application within the computer vision area. Its tasks include the classic challenges of object localization and classification. As a consequence, object detection is a challenging task. Furthermore, this technique is crucial for maritime applications since situational awareness can bring various benefits to surveillance systems. The literature presents various models to improve automatic target recognition and tracking capabilities that can be applied to and leverage maritime surveillance systems. Therefore, this paper reviews the available models focused on localization, classification, and detection. Moreover, it analyzes several works that apply the discussed models to the maritime surveillance scenario. Finally, it highlights the main opportunities and challenges, encouraging new research in this area.",maritime surveillance; classification; localization; detection; artificial intelligence; neural networks
"With the successful development in computer vision, building a deep convolutional neural network (CNNs) has been mainstream, considering the character of shared parameters in a convolutional layer. Stacking convolutional layers into a deep structure improves performance, but over-stacking also ramps up the needed resources for GPUs. Seeing another surge of Transformers in computer vision, the issue has aroused severely. A resource-hungry model is hardly implemented for limited hardware or single-customers-based GPU. Therefore, this work focuses on these concerns and proposes an efficient but robust backbone, which equips with channel and spatial direction attentions, so the attentions help to expand receptive fields in shallow convolutional layers and pass the information to every layer. An attention-boosted network based on already efficient CNNs, Universal Pixel Attention Networks (UPANets), is proposed. Through a series of experiments, UPANets fulfil the purposes of learning global information with less needed resources and outshine many existing SOTAs in CIFAR-{10, 100}.",computer vision; image classification; CNN; attention
"For decades, co-relating different data domains to attain the maximum potential of machines has driven research, especially in neural networks. Similarly, text and visual data (images and videos) are two distinct data domains with extensive research in the past. Recently, using natural language to process 2D or 3D images and videos with the immense power of neural nets has witnessed a promising future. Despite the diverse range of remarkable work in this field, notably in the past few years, rapid improvements have also solved future challenges for researchers. Moreover, the connection between these two domains is mainly subjected to GAN, thus limiting the horizons of this field. This review analyzes Text-to-Image (T2I) synthesis as a broader picture, Text-guided Visual-output (T2Vo), with the primary goal being to highlight the gaps by proposing a more comprehensive taxonomy. We broadly categorize text-guided visual output into three main divisions and meaningful subdivisions by critically examining an extensive body of literature from top-tier computer vision venues and closely related fields, such as machine learning and human-computer interaction, aiming at state-of-the-art models with a comparative analysis. This study successively follows previous surveys on T2I, adding value by analogously evaluating the diverse range of existing methods, including different generative models, several types of visual output, critical examination of various approaches, and highlighting the shortcomings, suggesting the future direction of research.",Text-to-Image; Text-to-Visual; computer vision; neural networks
"The strengthening of concrete structures with laminates of Carbon-Fiber-Reinforced Polymers (CFRP) is a widely adopted technique. retained The application is more effective if pre-stressed CFRP laminates are adopted. The measurement of the strain level during the pre-stress application usually involves laborious and time-consuming applications of instrumentation. Thus, the development of expedited approaches to accurately measure the pre-stressed application in the laminates represents an important contribution to the field. This paper proposes and benchmarks contact-free architecture for measuring the strain level of CFRP laminate based on computer vision. The main objective is to provide a solution that might be economically feasible, automated, easy to use, and accurate. The architecture is fed by digitally deformed synthetic images, generated based on a low-resolution camera. The adopted methods range from traditional machine learning to deep learning. Furthermore, dropout and cross-validation methods for quantifying traditional machine learning algorithms and neural networks are used to efficiently provide uncertainty estimates. ResNet34 deep learning architecture provided the most accurate results, reaching a root mean square error (RMSE) of 0.057 parts per thousand for strain prediction. Finally, it is important to highlight that the architecture presented is contact-free, automatic, cost-effective, and measures directly on the laminate surfaces, which allows them to be widely used in the application of pre-stressed laminates.",machine learning; deep learning; computer vision; CFRP laminates; strengthening RC; strain monitoring
"Large-scale synthetic traffic image datasets have been widely used to make compensate for the insufficient data in real world. However, the mismatch in domain distribution between synthetic datasets and real datasets hinders the application of the synthetic dataset in the actual vision system of intelligent vehicles. In this paper, we propose a novel synthetic-to-real domain adaptation method to settle the mismatch domain distribution from two aspects, i.e., data level and knowledge level. On the data level, a Style-Content Discriminated Data Recombination (SCD-DR) module is proposed, which decouples the style from content and recombines style and content from different domains to generate a hybrid domain as a transition between synthetic and real domains. On the knowledge level, a novel Iterative Cross-Domain Knowledge Transferring (ICD-KT) module including source knowledge learning, knowledge transferring and knowledge refining is designed, which achieves not only effective domain-invariant feature extraction, but also transfers the knowledge from labeled synthetic images to unlabeled actual images. Comprehensive experiments on public virtual and real dataset pairs demonstrate the effectiveness of our proposed synthetic-to-real domain adaptation approach in object detection of traffic scenes.",Object detection; Feature extraction; Data models; Training; Knowledge engineering; Detectors; Computational modeling; Computer vision; Unsupervised Domain Adaptation; Teacher-student learning; Traffic object detection
"Semantic scene segmentation has become an important application in computer vision and is an essential part of intelligent transportation systems for complete scene understanding of the surrounding environment. Several methods based on convolutional neural networks have emerged, but they have some problems, including small-scale target loss, inaccurate detailed region segmentation, and boundary category confusion. Using shallow features, we exploit the capabilities of global context information according to the theory of pyramids. A weighted pyramid feature fusion module is constructed to fuse the feature maps of different scales generated by the backbone network, and the proportion of feature fusion is dynamically updated by trainable parameters. After that, a self-attention mechanism is introduced to discover information about spatial channel interdependencies. Finally, the atrous spatial pyramid pooling module of the DeepLabv3+ network is improved by connecting the atrous convolution with different dilation rates at the receptive field. The experimental results show 4.1% mean pixel accuracy and 3.92% mean intersection over union improvements in the proposed method compared with the DeepLabv3+, and the result of semantic segmentation is more accurate. (c) 2022 SPIE and IS&T",image segmentation; computer vision technology; intelligent transportation systems; attention mechanism; multi-scale feature; convolutional neural networks; feature extraction
"Due to lack of data, overfitting ubiquitously exists in real-world applications of deep neural networks (DNNs). We propose advanced dropout, a model-free methodology, to mitigate overfitting and improve the performance of DNNs. The advanced dropout technique applies a model-free and easily implemented distribution with parametric prior, and adaptively adjusts dropout rate. Specifically, the distribution parameters are optimized by stochastic gradient variational Bayes in order to carry out an end-to-end training. We evaluate the effectiveness of the advanced dropout against nine dropout techniques on seven computer vision datasets (five small-scale datasets and two large-scale datasets) with various base models. The advanced dropout outperforms all the referred techniques on all the datasets. We further compare the effectiveness ratios and find that advanced dropout achieves the highest one on most cases. Next, we conduct a set of analysis of dropout rate characteristics, including convergence of the adaptive dropout rate, the learned distributions of dropout masks, and a comparison with dropout rate generation without an explicit distribution. In addition, the ability of overfitting prevention is evaluated and confirmed. Finally, we extend the application of the advanced dropout to uncertainty inference, network pruning, text classification, and regression. The proposed advanced dropout is also superior to the corresponding referred methods. Codes are available at https://github.com/PRIS-CV/AdvancedDropout.",Training; Bayes methods; Standards; Gaussian distribution; Adaptation models; Stochastic processes; Neural networks; Deep neural network; dropout; model-free distribution; Bayesian approximation; stochastic gradient variational Bayes
"Pixel-level 2D object semantic understanding is an important topic in computer vision and could help machine deeply understand objects (e.g., functionality and affordance) in our daily life. However, most previous methods directly train on correspondences in 2D images, which is end-to-end but loses plenty of information in 3D spaces. In this paper, we propose a new method on predicting image corresponding semantics in 3D domain and then projecting them back onto 2D images to achieve pixel-level understanding. In order to obtain reliable 3D semantic labels that are absent in current image datasets, we build a large scale keypoint knowledge engine called KeypointNet, which contains 103,450 keypoints and 8,234 3D models from 16 object categories. Our method leverages the advantages in 3D vision and can explicitly reason about objects self-occlusion and visibility. We show that our method gives comparative and even superior results on standard semantic benchmarks.",Semantics; Three-dimensional displays; Engines; Annotations; Solid modeling; Pipelines; Training; Keypoint; dataset; point cloud; object analysis; semantic understanding
"As an emerging and challenging problem in the computer vision community, weakly supervised object localization and detection plays an important role for developing new generation computer vision systems and has received significant attention in the past decade. As methods have been proposed, a comprehensive survey of these topics is of great importance. In this work, we review (1) classic models, (2) approaches with feature representations from off-the-shelf deep networks, (3) approaches solely based on deep learning, and (4) publicly available datasets and standard evaluation metrics that are widely used in this field. We also discuss the key challenges in this field, development history of this field, advantages/disadvantages of the methods in each category, the relationships between methods in different categories, applications of the weakly supervised object localization and detection methods, and potential future directions to further promote the development of this research field.",Location awareness; Annotations; Training; Task analysis; Detectors; Supervised learning; Computer vision; Weakly supervised learning; object localization; object detection
"Logistics migration and movement require precise information updates for traceability and visibility of goods through E-commerce platforms. Computer vision and digital image processing techniques are used for visual identification and tracking through different warehouses and delivery points. In this article, an incessant visualized tracking scheme (IVTS) is designed for identifying and tracking E-commerce logistics throughout the migration points. This scheme endorsed computer vision technology for logistics recognition and labelled data detection. In this scheme, the labelled logistics data is verified for its similarity in different migrating locations and to the endpoint. Based on the dimensional features and regional-pixel similarity factor, it is verified using the deep neural network. This learning process identifies dimensional variations due to logistics displacement and position suppressing the similarity variations. It is performed based on the migration and information available to prevent tracking errors. For the varying locations and logistics displacement, the error pixel regions are identified and trained for possible similarity detection. The proposed scheme effectively improves visual accuracy, tracking maximization, and logistics detection by reducing dimensional errors.",
"In southeastern North America, Indigenous potters and woodworkers carved complex, primarily abstract, designs into wooden pottery paddles, which were subsequently used to thin the walls of hand-built, clay vessels. Original paddle designs carry rich historical and cultural information, but pottery paddles from ancient times have not survived. Archaeologists have studied design fragments stamped on sherds to reconstruct complete or nearly complete designs, which is extremely laborious and time-consuming. In Snowvision, we aim to develop computer vision methods to assist archaeologists to accomplish this goal more efficiently and effectively. For this purpose, we identify and study three computer vision tasks: (1) extracting curve structures stamped on pottery sherds; (2) matching sherds to known designs; (3) clustering sherds with unknown designs. Due to the noisy, highly fragmented, composite-curve patterns, each task poses unique challenges to existing methods. To solve them, we propose (1) a weakly-supervised CNN-based curve structure segmentation method that takes only curve skeleton labels to predict full curve masks; (2) a patch-based curve pattern matching method to address the problem of partial matching in terms of noisy binary images; (3) a curve pattern clustering method consisting of pairwise curve matching, graph partitioning and sherd stitching. We evaluate the proposed methods on a set of collected sherds and extensive experimental results show the effectiveness of the proposed algorithms.",Curve-pattern segmentation; Design identification; Curve-pattern matching; Curve-pattern clustering; Swift creek complicated stamped pottery
"Over the past few years, the computer vision domain has evolved and made a revolutionary transition from human-engineered features to automated features to address challenging tasks. Computer vision is an ever-evolving domain with its roots deeply correlated with neuroscience; any new findings that trigger a more intuitive understanding and working of the human visual system generally impact the design strategy of computer vision algorithms. The convolutional neural network is one such algorithm that is currently the de facto standard for most computer vision tasks such as image classification, object detection, image segmentation, etc. As convolutional neural networks are associated with inherent con-straints such as the requirement for an immense amount of labeled data and an inefficient data routing policy, capsule networks could be a viable alternative. Upheld by the backpropagation and the dynamic routing algorithm, the capsule network has set the new paradigm for developing reliable computer vision algorithms. Despite the phenomenal theoretical backing from neuroscience and the groundbreaking per-formance on benchmark datasets, the lack of information concerning the conception and working of cap-sule networks becomes the major impediment to adopting them for computer vision algorithms. This paper presents a concise overview of capsule network-based classification architectures, routing algo-rithms, performance analysis, limitations, and future scope, helping the research community to adopt capsule networks at the forefront of modern computer vision research.(c) 2022 Elsevier B.V. All rights reserved.",Computer vision; Convolutional neural networks; Capsule network; Neuroscience
"Object detection is a hot topic in computer vision (CV), and it has many applications in various security fields. However, many works have demonstrated that neural network-based object detection is vulnerable to adversarial attacks. In this paper, we study adversarial attacks on object detectors in the real world and propose a new adversarial attack called Misleading Attention and Classification Attack (MACA), which can generate adversarial patches to mislead the object detectors. Specifically, we propose a new scheme to generate adversarial patches to fool the object detector. Our scheme restricts the noise of the adversarial patches and aims to ensure that the generated adversarial patches are visually similar to natural images. The attack simulates the complex external physical environment and the 3D transformations of non-rigid objects to increase the robustness of adversarial patches. We attack the up-to-date object detectors (e.g., Yolo-V5), and we prove that our technique has strong transferability among different detectors. Extensive experiments show that it is feasible to transfer the digital adversarial patches to the real world while maintaining the transferability of adversarial patches among different models and the success rate of adversarial attacks.(c) 2022 Elsevier Ltd. All rights reserved.",Computer vision; Model security; Adversarial attack; Object detection
"Understanding human activity and behavior, particularly real-time understanding in video feeds, is one of the most active areas of research in Computer Vision (CV) and Artificial Intelligence (AI) nowadays. To advance the topic of integrating learning engagement research with university teaching practice, accurate and efficient assessment, and analysis of students' classroom learning behavior engagement is very important. The recently proposed classroom behavior recognition algorithms have some limitations, such as the inability to quickly and accurately identify students' classroom behaviors because they do not consider the motion information of students between consecutive frames. In recent years, action recognition algorithms based on Convolutional Neural Networks (CNN) have improved significantly. To address the limitations of existing algorithms, in this study, a 3D-CNN is selected as a network model for classroom student behavior recognition, which increases information multisourcing and classroom student localization with high accuracy and robustness. For better analysis of human behavior in videos, the 3D convolution extends the 2D convolution to the spatial-temporal domain. In the proposed system, first of all, a real-time picture stream of each student is obtained by combining real-time target detection and tracking. Then, a deep spatiotemporal residual CNN is used to learn the spatiotemporal features of each student's behavior, so, as to achieve real-time recognition of classroom behaviors for multistudent targets in classroom teaching scenarios. To verify the effectiveness of the proposed model, different experiments are conducted using the labeled classroom behavior dataset. The experimental results demonstrate that the proposed model exhibits better performance in classroom behavior recognition. The accurate recognition of classroom behaviors can assist the teachers and students to understand the classroom learning situation and help to promote the development of smart classroom.",
"Person reidentification (re-ID) is an important topic in computer vision. This paper studies an unsupervised approach to re-ID, which does not require any labeled information and is thus possible to deploy in real-world scenarios. State-of-the-art unsupervised re-ID methods usually use a memory bank to store the instance feature vectors, generate pseudolabels with a clustering algorithm, and compare the query instances to the centroid of the clusters for contrastive learning. However, because hard negative or noisy samples exist, the centroid generated by unsupervised learning may not be a perfect prototype. Forcing the wrong images to get closer to the centroid would result in accumulated errors and deteriorated overfitting. To solve this problem, we propose a quantitative random selection strategy to form the cluster feature representation. Specifically, in each iteration, the cluster algorithm executes on instance-level feature vectors to generate pseudolabels. Then, we shuffle all the instance vectors belonging to the same cluster and select samples within the same cluster in a certain proportion to form the cluster-level memory. During network training, the query instances are used to update the cluster-level memory for contrastive learning. Extensive experiments show that our proposed method produces state-of-the-art performance in unsupervised person re-ID tasks.",Computer vision; Unsupervised learning; Person reidentification
"Video-based computer vision tasks can benefit from estimation of the salient regions and interactions between those regions. Traditionally, this has been done by identifying the object regions in the images by utilizing pre-trained models to perform object detection, object segmentation and/or object pose estimation. Although using pre-trained models is a viable approach, it has several limitations in the need for an exhaustive annotation of object categories, a possible domain gap between datasets and a bias that is typically present in pre-trained models. In this work, we propose to utilize the common rationale that a sequence of video frames capture a set of common objects and interactions between them, thus a notion of co-segmentation between the video frame features may equip the model with the ability to automatically focus on task-specific salient regions and improve the underlying task's performance in an end-to-end manner. In this regard, we propose a generic module called Co-Segmentation inspired Attention Module(COSAM) that can be plugged in to any CNN model to promote the notion of co-segmentation based attention among a sequence of video frame features. We show the application of COSAM in three video-based tasks namely: (1) Video-based person re-ID, (2) Video captioning, & (3) Video action classification and demonstrate that COSAM is able to capture the task-specific salient regions in video frames, thus leading to notable performance improvements along with interpretable attention maps for a variety of video-based vision tasks, with possible application to other video-based vision tasks as well.",Attention; Co-segmentation; Person re-ID; Video-captioning; Video classification
"Albeit Deep neural networks (DNNs) are widely used in computer vision, natural language processing and speech recognition, they have been discovered to be fragile to adversarial attacks. Specifically, in computer vision, an attacker can easily deceive DNNs by contaminating an input image with perturbations imperceptible to humans. As one of the important vision tasks, face verification is also subject to adversarial attack. Thus, in this paper, we focus on defending against the adversarial attack for face verification to mitigate the potential risk. We learn a network via an implementation of stacked residual blocks, namely adversarial perturbations alleviation network (ApaNet), to alleviate latent adversarial perturbations hidden in the input facial image. During the supervised learning of ApaNet, only the Labeled Faces in the Wild (LFW) is used as the training set, and the legitimate examples and corresponding adversarial examples produced by projected gradient descent algorithm compose supervision and inputs respectively. By leveraging the middle and high layer's activation of FaceNet, the discrepancy between an image output by ApaNet and the supervision is calculated as the loss function to optimize ApaNet. Empirical experiment results on the LFW, YouTube Faces DB and CASIA-FaceV5 confirm the effectiveness of the proposed defender against some representative white-box and black-box adversarial attacks. Also, experimental results show the superiority performance of the ApaNet as comparing with several currently available techniques.",Deep neural network; Face verification; Adversarial example; Adversarial perturbations alleviation network
"Vascular biomarkers allow for non-invasive assessment of vascular structure and function and have been shown to be surrogates for cardiovascular (CV) outcome in adults. They reflect the cumulative risk of a plethora of single CV risk factors, such as obesity and hypertension, on the arterial wall. The process of atherosclerosis oftentimes has its origin in childhood and tracks into adulthood. Obesity-related CV risk in childhood is a main determinant of manifest CV disease and adverse outcome in adulthood. To date, prevention strategies are directed toward the detection and reduction of CV disease in adulthood. This review updates and puts into perspective the potential use of vascular biomarkers in children. With reference to the concept of early vascular aging in adults, it elaborates on the role of vascular biomarkers for CV risk stratification in children. The concept of primordial vascular aging implies that young children be screened for vascular health, in an attempt to timely detect subclinical atherosclerosis and initiate treatment strategies to reverse vascular damage in a period of life with high probability for risk regression. The evidence for the validity of macro- and microvascular candidate biomarkers as screening tools of CV risk in children is reviewed, and limitations as well as remaining research gaps are highlighted. Furthermore, an overview on the effects of exercise treatment on vascular biomarkers is given. Vascular biomarkers susceptible to lifestyle or drug treatment have the potential to qualify as monitoring tools to guide clinicians. This review discusses evidence for vascular biomarkers to optimize screening of childhood CV risk from initial concepts to potential future clinical implementation in cardiovascular prevention.",children and adolescents; cardiovascular risk; primary prevention; vascular aging; biomarkers
"Computer vision systems perform based on their design and parameter setting. In computer vision systems that use grayscale conversion, the conversion of RGB images to a grayscale format influences performance of the systems in terms of both results quality and computational costs. Appropriate setting of the weights for the weighted means grayscale conversion, co-estimated with other parameters used in the computer vision system, helps to approach the desired performance of a system or its subsystem at the cost of a negligible or no increase in its time-complexity. However, parameter space of the system and subsystem as extended by the grayscale conversion weights can contain substandard settings. These settings show strong sensitivity of the system and subsystem to small changes in the distribution of data in a color space of the processed images. We developed a methodology for Tuning of the Grayscale computer Vision systems (TGV) that exploits the advantages while compensating for the disadvantages of the weighted means grayscale conversion. We show that the TGV tuning improves computer vision system performance by up to 16% in the tested case studies. The methodology provides a universally applicable solution that merges the utility of a fine-tuned computer vision system with the robustness of its performance against variable input data.",Computer vision; Parameter optimization; Performance evaluation; WECIA graph; Weighted means grayscale conversion
"As computer vision and human-computer interaction technology mature, vision-based auxiliary text reading has become the mainstream method to optimize the learning and reading experience. Most of the existing auxiliary text reading methods use scene text recognition combined with human gesture recognition to complete the task in multiple stages. However, these methods cannot accurately and effectively extract the textual information that readers are interested in complex and varied reading scenarios. To improve the text reading experience, we propose a human-centered fast auxiliary text reading method. It utilizes a hand-text hybrid object detection (HTD) model to instantly locate text of interest to readers, a font-consistent prior text image superresolution network (FCSRN) to recover low-resolution text images to enhance the accuracy of text recognition, and a convolutional recurrent neural network (CRNN) text recognition operator to obtain the content of the text, that is, interesting to readers. To verify the effectiveness of the proposed method, we tested the performance of the text localization module on a homemade HTD dataset and the performance of the FCSRN on the public text image superresolution dataset called TextZoom. Quantitative experiments on the overall performance of the fast auxiliary reading system, called reading what you are interested in (RWYI), were designed. The experiments indicate that the proposed method can meet the needs of human-computer interactive auxiliary reading in text reading scenarios and optimize the reading experience.",
"This paper takes the broad topic of geometrical surface imperfections on manufactured surfaces and provides an overview of how they affect component functionality and how they may be detected and classified as defects or not. The presented overview considers both human visual inspection and machine vision-based approaches along with their evolving roles. Of note is that the paper takes a highly granular field consisting of customized solutions for customized applications and frames the discussion around fundamental considerations for each of the tasks; search/acquisition, sensing/detection, processing, classification and decision. Future trends and areas still requiring attention are highlighted.(c) 2022 Published by Elsevier Ltd on behalf of CIRP.",Defect; Surface; Machine Vision; Table 1
"Computer vision has established a foothold in the online fashion retail industry. Main product detection is a crucial step of vision-based fashion product feed parsing pipelines, focused on identifying the bounding boxes that contain the product being sold in the gallery of images of the product page. The current state-of-the-art approach does not leverage the relations between regions in the image, and treats images of the same product independently, therefore not fully exploiting visual and product contextual information. In this paper, we propose a model that incorporates Graph Convolutional Networks (GCN) that jointly represent all detected bounding boxes in the gallery as nodes. We show that the proposed method is better than the state-of-the-art, especially, when we consider the scenario where title-input is missing at inference time and for cross-dataset evaluation, our method outperforms previous approaches by a large margin.",Main product detection; Graph networks; Fashion
"Scene recognition plays an important role in many computer vision tasks. However, the recognition performance hardly meets the development of computer vision, since scene images show large variations in spatial position, illumination, and scale. To address this issue, a joint global metric learning and local manifold preservation (JGML-LMP) approach is proposed. First, we formulate a new global metric learning problem based on the cluster centers of each specific class, allowing to capture the global discriminative information with more informative samples. Second, in order to exploit the local manifold structure, we introduce an adaptive nearest neighbors constraint through which the local intrinsic relationships can be preserved in the new metric space instead of the Euclidean space. Third, through performing global metric learning and local manifold preservation jointly within a unified optimization framework, our approach can take advantage of both global and local information, and hence produces more discriminative and robust feature repre-sentations for scene recognition. Extensive experiments on four benchmark scene datasets demonstrate the superiority of the proposed method over state-of-the-art methods.(c) 2022 Elsevier Inc. All rights reserved.",Global metric learning; Local manifold preservation; Scene recognition
"With the development of computer vision technology, human action pose recognition has gradually become a popular research direction, but there are still some problems in the application research based on pose recognition in sports action assisted evaluation. In this paper, the human motion pose recognition technology based on deep learning is introduced into this field to realize the intelligence of sports-assisted training. Firstly, we analyze the advantages and limitations of the state-of-the-art human motion pose recognition algorithms in computer vision in specific fields. On this basis, a human motion space recognition method based on periscope neural network is proposed. Firstly, the classical radar signal processing method is used to preprocess the echo signal of human spatial position and generate the frequency image in the process of human spatial position. Then, the periscope neural network (CNN) is constructed, and the time-frequency image is used as the input data of CNN to train the network parameters. Finally, the method is tested by using the open dataset in the network. The experimental results show that the designed CNN can accurately identify four different types of physical motion, and the accuracy coefficient is at least 97%.",
"The human pose estimation is a significant issue that has been taken into consideration in the computer vision network for recent decades. It is a vital advance toward understanding individuals in videos and still images. In simple terms, a human pose estimation model takes in an image or video and estimates the position of a person's skeletal joints in either 2D or 3D space. Several studies on human posture estimation can be found in the literature, however, they center around a specific class; for instance, model-based methodologies or human movement investigation, and so on. Later, various Deep Learning (DL) algorithms came into existence to overcome the difficulties which were there in the earlier approaches. In this study, an exhaustive review of human pose estimation (HPE), including milestone work and recent advancements is carried out. This survey discusses the different two-dimensional (2D) and three-dimensional human (3D) pose estimation techniques along with their classical and deep learning approaches which provide the solution to the various computer vision problems. Moreover, the paper also considers the different deep learning models used in pose estimation, and the analysis of 2D and 3D datasets is done. Some of the evaluation metrics used for estimating human poses are also discussed here. By knowing the direction of the individuals, HPE opens a road for a few real-life applications some of which are talked about in this study.",Human pose estimation; Deep learning; 2D; 3D pose estimation; Activity recognition
"Point cloud completion is a generation and estimation issue derived from the partial point clouds, which plays a vital role in the applications of 3D computer vision. The progress of deep learning (DL) has impressively improved the capability and robustness of point cloud completion. However, the quality of completed point clouds is still needed to be further enhanced to meet the practical utilization. Therefore, this work aims to conduct a comprehensive survey on various methods, including point-based, view-based, convolution-based, graph-based, generative model-based, transformer-based approaches, etc. And this survey summarizes the comparisons among these methods to provoke further research insights. Besides, this review sums up the commonly used datasets and illustrates the applications of point cloud completion. Eventually, we also discussed possible research trends in this promptly expanding field.",Point cloud compression; Three-dimensional displays; Solid modeling; Shape; Task analysis; Laser radar; Deep learning; Deep learning; point cloud; completion; 3D vision
"Artistic graphic design is the aesthetic result of the designer's fusion of various elements, with a high degree of independence. Considering the lack of significant visual design scope and aesthetic indicators of graphic design, our research aims to build an upgraded network model that can categorize different types of artistic graphics with labels and realize the free combination of graphic solutions. We realize the scheme reorganization of artistic graphic design from the perspective of computer vision and propose the artistic graphic design method based on memory neural network. We built a computer vision environment and reconstructed the computer vision network to set up an independent deep camera vision range calculation law. Considering the artistic graphic region segmentation problem, we propose the self-attentive mechanism, which can quantitatively segment different artistic graphic regions according to temporal features, before arranging them in a sequence to obtain the graphic region feature vector. We also add the LSTM structure based on the attention mechanism to match with the self-attention features of the graphical region segmentation module and pass the matched attention feature vector to the LSTM network to extract the labeled text feature information of the graphs. To test the effectiveness of our method, we build a database of artistic graphics and set up an adaptive training process. We also compared deep learning methods of the same type, and the experimental results proved that our method outperformed other deep methods in artistic graphic design by keeping the scheme reorganization accuracy and quantitative evaluation of artistic models above 90%.",
"Monocular sensors depth prediction has received continuous attention in recent years because of its wide application in autonomous driving, intelligent system navigation and other fields. Convolutional neural networks have dominated monocular depth prediction for a long time, and the recent introduction of Transformer-based and MLP-based architectures in the field of computer vision has provided some new ideas for monocular depth prediction. However, they all have a series of problems such as high computational complexity and excessive parameters. In this paper, we propose MLP-Depth, which is a lightweight monocular depth prediction method based on hierarchical multi-stage MLP, and utilizes depth-wise convolution to improve local modeling capabilities and reduce parameters and computational costs. In addition, we also design a multi-scale inverse attention mechanism to implicitly improve the global expressiveness of MLP-Depth. Our method effectively reduces the number of parameters of monocular depth prediction network using transformer-like architectures, and extensive experiments show that MLP-Depth can achieve competitive results with fewer parameters in challenging outdoor and indoor datasets.",Computer architecture; Transformers; Task analysis; Computer vision; Convolutional neural networks; Sensors; Convolution; Monocular sensors depth prediction; hierarchical multi-stage MLP; multi-scale inverse attention
"Facial Expression recognition is a computer vision problem that took relevant benefit from the research in deep learning. Recent deep neural networks achieved superior results, demonstrating the feasibility of recognizing the expression of a user from a single picture or a video recording the face dynamics. Research studies reveal that the most discriminating portions of the face surfaces that contribute to the recognition of facial expressions are located on the mouth and the eyes. The restrictions for COVID pandemic reasons have also revealed that state-of-the-art solutions for the analysis of the face can severely fail due to the occlusions of using the facial masks. This study explores to what extend expression recognition can deal with occluded faces in presence of masks. To a fairer comparison, the analysis is performed in different occluded scenarios to effectively assess if the facial masks can really imply a decrease in the recognition accuracy. The experiments performed on two public datasets show that some famous top deep classifiers expose a significant reduction in accuracy in presence of masks up to half of the accuracy achieved in non-occluded conditions. Moreover, a relevant decrease in performance is also reported also in the case of occluded eyes but the overall drop in performance is not as severe as in presence of the facial masks, thus confirming that, like happens for face biometric recognition, occluded faces by facial mask still represent a challenging limitation for computer vision solutions.",Expression recognition; Masked face analysis; Deep learning
"Image segmentation and computer vision are becoming more important in computer-aided design. A computer algorithm extracts image borders, colours, and textures. It also depletes resources. Technical knowledge is required to extract information about distinctive features. There is currently no medical picture segmentation or recognition software available. The proposed model has 13 layers and uses dilated convolution and max-pooling to extract small features. Ghost model deletes the duplicated features, makes the process easier, and reduces the complexity. The Convolution Neural Network (CNN) generates a feature vector map and improves the accuracy of area or bounding box proposals. Restructuring is required for healing. As a result, convolutional neural networks segment medical images. It is possible to acquire the beginning region of a segmented medical image. The proposed model gives better results as compared to the traditional models, it gives an accuracy of 96.05, Precision 98.2, and recall 95.78. The first findings are improved by thickening and categorising the image's pixels. Morphological techniques may be used to segment medical images. Experiments demonstrate that the recommended segmentation strategy is effective. This study rethinks medical image segmentation methods.",
"The importance of measuring the complexity of shapes can be seen by the wide range of its application such as computer vision, robotics, cognitive studies, eye tracking, and psychology. However, it is very challenging to define an accurate and precise metric to measure the complexity of the shapes. In this paper, we explore different notions of shape complexity, drawing from established work in mathematics, computer science, and computer vision. We integrate results from user studies with quantitative analyses to identify three measures that capture important axes of shape complexity, out of a list of almost 300 measures previously considered in the literature. We then explore the connection between specific measures and the types of complexity that each one can elucidate. Finally, we contribute a dataset of both abstract and meaningful shapes with designated complexity levels both to support our findings and to share with other researchers.",Shape complexity; Complexity measures; 2D shapes
"Human activity recognition (HAR) is a very active yet challenging and demanding area of computer science. Due to the articulated nature of human motion, it is not trivial to detect human activity with high accuracy for all applications. Generally, activities are recognized from a series of actions performed by the human through vision-based sensors or non-vision-based sensors. HAR's application areas span from health, sports, smart home-based, and other diverse areas. Moreover, detecting human activity is also needed to automate systems to monitor ambient and detect suspicious activity while performing surveillance. Besides, providing appropriate information about individuals is a necessary task in pervasive computing. However, identifying human activities and actions is challenging due to the complexity of activities, speed of action, dynamic recording, and diverse application areas. Besides that, all the actions and activities are performed in distinct situations and backgrounds. There is a lot of work done in HAR; finding a suitable algorithm and sensors for a certain application area is still challenging. While some surveys are already conducted in HAR, the comprehensive survey to investigate algorithms and sensors concerning diverse applications is not done yet. This survey investigates the best and optimal machine learning algorithms and techniques to recognize human activities in the field of HAR. It provides an in-depth analysis of which algorithms might be suitable for a certain application area. It also investigates which vision-based or non-vision-based acquisition devices are mostly employed in the literature and are suitable for a specific HAR application.",Machine learning; Human activity recognition; Activities of daily living; Sensors; Videos
"This survey reviews explainability methods for vision-based self-driving systems trained with behavior cloning. The concept of explainability has several facets and the need for explainability is strong in driving, a safety-critical application. Gathering contributions from several research fields, namely computer vision, deep learning, autonomous driving, explainable AI (X-AI), this survey tackles several points. First, it discusses definitions, context, and motivation for gaining more interpretability and explainability from self-driving systems, as well as the challenges that are specific to this application. Second, methods providing explanations to a black-box self-driving system in a post-hoc fashion are comprehensively organized and detailed. Third, approaches from the literature that aim at building more interpretable self-driving systems by design are presented and discussed in detail. Finally, remaining open-challenges and potential future research directions are identified and examined.",Autonomous driving; Explainability; Interpretability; Black-box; Post-hoc interpretabililty
"Transformers have demonstrated impressive expressiveness and transfer capability in computer vision fields. Dense prediction is a fundamental problem in computer vision that is more challenging to solve than general image-level prediction tasks. The inherent properties of transformers enable them to process feature representations with stable and relatively high resolution, which precisely satisfies the demands of dense prediction tasks for finer-grained and more globally coherent predictions. Furthermore, compared to convolutional networks, transformer methods require minimal inductive bias and permit long-range information interaction. These strengths have contributed to exciting advancements in dense prediction tasks that apply transformer networks. This survey aims to provide a comprehensive overview of transformer models with a specific focus on dense prediction. In this survey, we provide a well-rounded view of state-of-the-art transformer-based approaches, explicitly emphasizing pixel-level prediction tasks. We generally consider transformer variants from the network architecture perspective. We further propose a novel taxonomy to organize these models according to their constructions. Subsequently, we examine various specific optimization strategies to tackle certain bottleneck problems in dense prediction tasks. We explore the commonalities and differences among these works and provide multiple horizontal comparisons from the experimental point of view. Finally, we summarize several stubborn problems that continue to impact visual transformers and outline some possible development directions. (C) 2022 Elsevier B.V. All rights reserved.",Deep learning; Transformer; Dense prediction; Computer vision
"In order to solve the problems of low brightness contrast of a color image, hiding a large amount of detail information, and deviation of color information in the process of image acquisition, an optimization method of plane image color enhancement processing based on computer vision virtual reality is proposed. In this method, the input RGB image is converted into the image represented by the HSI color model, and its adaptive brightness is adjusted to improve the overall brightness of the image. For the local detail enhancement of the color image, the three-dimensional Gaussian model perceived by retinal neurons is introduced into the illuminance image estimation of the MSR algorithm to enhance the image color. The results are as follows: from the perspective of objective parameter evaluation, the mean, standard deviation, information entropy, and average gradient of example images 1 and 2 are improved by about 70%; this algorithm not only enhances the brightness and contrast of the image but also maintains the detailed edge information of the image and the color characteristics of the object itself. The average enhancement rate is the highest among various algorithms, up to 95%. The algorithm proposed in this paper maintains the edge detail information of the image, optimizes the defects of the combination of traditional bilateral filtering and Retinex algorithm, and the color is also well restored, which makes the monitoring image easier to identify, more conducive to criminal investigation and solving cases, and lays a foundation for subsequent image processing.",
"In aquaculture breeding or production programmes, counting juvenile fish represents a considerable cost in terms of the human hours needed. In this study, we explored the use of two state-of-the-art machine learning architectures (Single Shot Detection, hereafter SSD and Faster Regions with convolutional neural networks, hereafter Faster R-CNN) to augment a manual image-based juvenile fish counting method for the Australasian snapper (Chrysophrys auratus) bred at The New Zealand Institute for Plant and Food Research Limited. We tested model accuracy after tuning for confidence thresholds and non-maximal suppression overlap parameters, and implementing a bias correction using a Poisson regression model. Validation of image data showed that after tuning, bias-corrected SSD and Faster R-CNN models had mean absolute percent errors (MAPE) of less than 10%, with SSD having MAPE of less than 5%. Comparison of the results with those from manual counts showed that, while manual counts are slightly more accurate (MAPE = 1.56), the machine learning methods allow for more rapid assessment of counts and thus facilitating a higher throughput. This work represents a first step for deploying machine learning applications to an existing real-life aquaculture scenario and provides a useful starting point for further developments, such as real-time counting of fish or collecting additional phenotypic data from the source images.",Computer vision; object detection; Chrysophrys auratus; aquaculture; imaging
"Face synthesis is an important problem in computer vision with many applications. In this work, we describe a new method, namely LandmarkGAN, to synthesize faces based on facial landmarks as input. Facial landmarks are a natural, intuitive, and effective representation for facial expressions and orienta-tions, which are independent from the target's texture or color and background scene. Our method is able to transform a set of facial landmarks into new faces of different subjects, while retains the same facial expression and orientation. Experimental results on face synthesis and reenactments demonstrate the effectiveness of our method.(c) 2022 Elsevier B.V. All rights reserved.",Face synthesis; GAN; Face editing
"Using an attention mechanism based on the convolutional neural networks (CNNs) improves the performance of computer vision tasks by enhancing the representation of the features. The existing attention methods enhance the expression of the features by modeling the internal information of the features. However, due to the limited information flow of the previous features, these methods are difficult to calibrate features more completely. In this paper, we propose a Coupled Attention Framework (CAF) that is a simple attention framework for improving the performance of the existing attention methods. In the CAF, a coupling branch is added to an existing attention method to generate the input attention maps and enhance the input features of the convolution. The input attention is then spread to the output features through coupling between the input attention maps and convolution, the output features. The final result is the experimental results on various visual tasks. The results show that applying CAF to most of the existing attention methods can improve the performance with fewer parameters.",
"Neuromorphic vision sensors (NVSs) are key enablers of energy savings in Internet of Things (IoT)-based traffic monitoring and surveillance systems that exploit the temporal redundancy in video streams. However, for these scenarios, an object typically occupies a fraction of the full image frame leading to a significant spatial redundancy in the active image. Hence, there is a need for energy-efficient, dedicated hardware to detect the region of interests (RoI) to exploit spatial redundancy in the valid frames and reduce computations in the succeeding recognition modules. This article proposes a 9T-SRAM in-memory computing (IMC)-based region proposal (RP) network for event-based binary image (EBBI) frames from a NVS. The proposed 9T-SRAM cell enables a 1-D projection of objects on the horizontal and vertical axes of an image. An iterative and selective search (ISS) of the rising and falling edges of 1-D projection yields the coordinates of a bounding box encapsulating an object. To demonstrate the energy-saving and effectiveness of the algorithm, we fabricated the proposed architecture, RP integrated circuit (RPIC) in a 65 nm CMOS process. Tested with the video recordings from a Dynamic and Active-pixel Vision Sensor (DAVIS), the RPIC achieves a peak throughput of 1259 ft/s at 1 Meps event rate. Moreover, the proposed RP architecture achieves a high energy efficiency of 389 TOPS/W due to in-memory operation.",Random access memory; Computer architecture; Vision sensors; Redundancy; Proposals; X-ray scattering; Streaming media; In-memory computing (IMC); low-power; neuromorphic vision sensors (NVSs); object detection; region proposal (RP) network
"In agricultural image analysis, optimal model performance is keenly pursued for better fulfilling visual recognition tasks (e.g., image classification, segmentation, object detection and localization), in the presence of challenges with biological variability and unstructured environments. Large-scale, balanced and ground-truthed image datasets are tremendously beneficial but most often difficult to obtain to fuel the development of highly performant models. As artificial intelligence through deep learning is impacting analysis and modeling of agricultural images, image augmentation plays a crucial role in boosting model performance while reducing manual efforts for image collection and labelling, by algorithmically creating and expanding datasets. Beyond traditional data augmentation techniques, generative adversarial network (GAN) invented in 2014 in the computer vision community, provides a suite of novel approaches that can learn good data representations and generate highly realistic samples. Since 2017, there has been a growth of research into GANs for image augmentation or synthesis in agriculture for improved model performance. This paper presents an overview of the evolution of GAN architectures followed by a first systematic review of various applications in agriculture and food systems (https://github.com/Derekabc/GANs-Agriculture), involving a diversity of visual recognition tasks for plant health conditions, weeds, fruits (preharvest), aquaculture, animal farming, plant phenotyping as well as postharvest detection of fruit defects. Challenges and opportunities of GANs are discussed for future research.",GAN; Image Augmentation; Agriculture; Deep Learning; Computer Vision
"Computer vision and image processing techniques have been extensively used in various fields and a wide range of applications, as well as recently in surface treatment to determine the quality of metal processing. Accordingly, digital image evaluation and processing are carried out to perform image segmentation, identification, and classification to ensure the quality of metal surfaces. In this work, a novel method is developed to effectively determine the quality of metal surface processing using computer vision techniques in real time, according to the average size of irregularities and caverns of captured metal surface images. The presented literature review focuses on classifying images into treated and untreated areas. The high computation burden to process a given image frame makes it unsuitable for real-time system applications. In addition, the considered current methods do not provide a quantitative assessment of the properties of the treated surfaces. The markup, processed, and untreated surfaces are explored based on the entropy criterion of information showing the randomness disorder of an already treated surface. However, the absence of an explicit indication of the magnitude of the irregularities carries a dependence on the lighting conditions, not allowing to explicitly specify such characteristics in the system. Moreover, due to the requirement of the mandatory use of specific area data, regarding the size of the cavities, the work is challenging in evaluating the average frequency of these cavities. Therefore, an algorithm is developed for finding the period of determining the quality of metal surface treatment, taking into account the porous matrix, and the complexities of calculating the surface tensor. Experimentally, the results of this work make it possible to effectively evaluate the quality of the treated surface, according to the criterion of the size of the resulting irregularities, with a frame processing time of 20 ms, closely meeting the real-time requirements.",computer vision; processing methods; image segmentation; texture analysis; quantitative characterization; real-time analysis
"Traditional grain size determination in materials characterization involves microscopy images and a laborious process requiring significant manual input and human expertise. In recent years, the development of computer vision (CV) has provided an alternative approach to microstructural characterization with preliminary implementations greatly simplifying the grain size determination process. Here, an end-to-end workflow to measure grain size in microscopy images without any manual input is presented. Following the ASTM standards for grain size determination, results from the line intercept (Heyn's method) and planimetric (Saltykov's method) approaches are used as the baseline. A pre-trained holistically nested edge detection (HED) model is used for CV-based edge detection, and the results are further compared to the classic Canny edge detection method. Post-processing was performed using open-source image processing packages to extract the grain size. In optical microscope images, the pre-trained HED model achieves much higher accuracy than the Canny edge detection method while reducing the image processing time by one to two orders of magnitude compared to traditional methods. The effects of morphological operations on the predicted grain size accuracy are also explored. Overall, the proposed end-to-end convolutional neural network (CNN)-based workflow can significantly reduce the processing time while maintaining the same accuracy as the traditional manual method.",deep learning; convolutional neural network; pre-trained model; edge detection; grain size
"Most building structures that are built today are built from concrete, owing to its various favorable properties. Compressive strength is one of the mechanical properties of concrete that is directly related to the safety of the structures. Therefore, predicting the compressive strength can facilitate the early planning of material quality management. A series of deep learning (DL) models that suit computer vision tasks, namely the convolutional neural networks (CNNs), are used to predict the compressive strength of ready-mixed concrete. To demonstrate the efficacy of computer vision-based prediction, its effectiveness using imaging numerical data was compared with that of the deep neural networks (DNNs) technique that uses conventional numerical data. Various DL prediction models were compared and the best ones were identified with the relevant concrete datasets. The best DL models were then optimized by fine-tuning their hyperparameters using a newly developed bio-inspired metaheuristic algorithm, called jellyfish search optimizer, to enhance the accuracy and reliability. Analytical experiments indicate that the computer vision-based CNNs outperform the numerical data-based DNNs in all evaluation metrics except the training time. Thus, the bio-inspired optimization of computer vision-based convolutional neural networks is potentially a promising approach to predict the compressive strength of ready-mixed concrete.",
"Gesture recognition has been studied for a while within the fields of computer vision and pattern recognition. A gesture can be defined as a meaningful physical movement of the fingers, hands, arms, or other parts of the body with the purpose to convey information for the environment interaction. For instance, hand gesture recognition (HGR) can be used to recognize sign language which is the primary means of communication by the deaf and mute. Vision-based HGR is critical in its application; however, there are challenges that will need to be overcome such as variations in the background, illuminations, hand orientation and size and similarities among gestures. The traditional machine learning approach has been widely used in vision-based HGR in recent years but the complexity of its processing has been a major challenge-especially on the handcrafted feature extraction. The effectiveness of the handcrafted feature extraction technique was not proven across various datasets in comparison to deep learning techniques. Therefore, a hybrid network architecture dubbed as Lightweight VGG16 and Random Forest (Lightweight VGG16-RF) is proposed for vision-based hand gesture recognition. The proposed model adopts feature extraction techniques via the convolutional neural network (CNN) while using the machine learning method to perform classification. Experiments were carried out on publicly available datasets such as American Sign Language (ASL), ASL Digits and NUS Hand Posture dataset. The experimental results demonstrate that the proposed model, a combination of lightweight VGG16 and random forest, outperforms other methods.",sign language recognition; hand gesture recognition; convolutional neural network (CNN); ensemble classifier; lightweight VGG16; random forest; transfer learning
"Worker safety at construction sites is a growing concern for many construction industries. Wearing safety helmets can reduce injuries to workers at construction sites, but due to various reasons, safety helmets are not always worn properly. Hence, a computer vision-based automatic safety helmet detection system is extremely important. Many researchers have developed machine and deep learning-based helmet detection systems, but few have focused on helmet detection at construction sites. This paper presents a You Only Look Once (YOLO)-based real-time computer vision-based automatic safety helmet detection system at a construction site. YOLO architecture is high-speed and can process 45 frames per second, making YOLO-based architectures feasible to use in real-time safety helmet detection. A benchmark dataset containing 5000 images of hard hats was used in this study, which was further divided in a ratio of 60:20:20 (%) for training, testing, and validation, respectively. The experimental results showed that the YOLOv5x architecture achieved the best mean average precision (mAP) of 92.44%, thereby showing excellent results in detecting safety helmets even in low-light conditions.",computer vision; safety helmet detection; You Only Look Once (YOLO); deep learning
"During the steel pipeline installation, special attention is paid to the butt weld control performed by fusion welding. The operation of the currently popular automated X-ray and ultrasonic testing complexes is associated with high resource and monetary costs. In this regard, this work is devoted to the development of alternative and cost-effective means of preliminary quality control of the work performed based on the visual testing method. To achieve this goal, a hardware platform based on a single board Raspberry Pi4 minicomputer and a set of available modules and expansion cards is proposed, and software whose main functionality is implemented based on the systemic application of computer vision algorithms and machine learning methods. The YOLOv5 object detection algorithm and the random forest machine learning model were used as a defect detection and classification system. The mean average precision (mAP) of the trained YOLOv5 algorithm based on extracted weld contours is 86.9%. A copy of YOLOv5 trained on the images of control objects showed a mAP result of 96.8%. Random forest identifying of the defect precursor based on the point clouds of the weld surface achieved a mAP of 87.5%.",computer vision; machine learning; nondestructive testing; technologies and systems; material surface reconstruction
"The underground mine environment is dangerous and harsh, tracking and detecting humans based on computer vision is of great significance for mine safety monitoring, which will also greatly facilitate identification of humans using the symmetrical image features of human organs. However, existing methods have difficulty solving the problems of accurate identification of humans and background, unstable human appearance characteristics, and humans occluded or lost. For these reasons, an improved aberrance repressed correlation filter (IARCF) tracker for human tracking in underground mines based on infrared videos is proposed. Firstly, the preprocess operations of edge sharpening, contrast adjustment, and denoising are used to enhance the image features of original videos. Secondly, the response map characteristics of peak shape and peak to side lobe ratio (PSLR) are analyzed to identify abnormal human locations in each frame, and the method of calculating the image similarity by generating virtual tracking boxes is used to accurately relocate the human. Finally, using the value of PSLR and the highest peak point of the response map, the appearance model is adaptively updated to further improve the robustness of the tracker. Experimental results show that the average precision and success rate of the IARCF tracker in the five underground scenarios reach 0.8985 and 0.7183, respectively, and the improvement of human tracking in difficult scenes is excellent. The IARCF tracker can effectively track underground human targets, especially occluded humans in complex scenes.",computer vision; human tracking; infrared videos; response map; appearance model
"Deep learning networks have recently demonstrated yielded impressive progress for multi-exposure image fusion. However, how to restore realistic texture details while correcting color distortion is still a challenging problem to be solved. To alleviate the aforementioned issues, in this paper, we propose an attention-guided global-local adversarial learning network for fusing extreme exposure images in a coarse-to-fine manner. Firstly, the coarse fusion result is generated under the guidance of attention weight maps, which acquires the essential region of interest from both sides. Secondly, we formulate an edge loss function, along with a spatial feature transform layer, for refining the fusion process. So that it can take full use of the edge information to deal with blurry edges. Moreover, by incorporating global-local learning, our method can balance pixel intensity distribution and correct the color distortion on spatially varying source images from both image/patch perspectives. Such a global-local discriminator ensures all the local patches of the fused images align with realistic normal-exposure ones. Extensive experimental results on two publicly available datasets show that our method drastically outperforms state-of-the-art methods in visual inspection and objective analysis. Furthermore, sufficient ablation experiments prove that our method has significant advantages in generating high-quality fused results with appealing details, clear targets, and faithful color. Source code will be available at https://github.com/JinyuanLiu-CV/AGAL.",Image edge detection; Task analysis; Image fusion; Image color analysis; Feature extraction; Deep learning; Computer vision; Image fusion; multi-exposure image; attention learning; illumination correction; adversarial learning
"Disentangled representation learning has been proposed as an approach to learning general representations even in the absence of, or with limited, supervision. A good general representation can be finetuned for new target tasks using modest amounts of data, or used directly in unseen domains achieving remarkable performance in the corresponding task. This alleviation of the data and annotation requirements offers tantalising prospects for applications in computer vision and healthcare. In this tutorial paper, we motivate the need for disentangled representations, revisit key concepts, and describe practical building blocks and criteria for learning such representations. We survey applications in medical imaging emphasising choices made in exemplar key works, and then discuss links to computer vision applications. We conclude by presenting limitations, challenges, and opportunities. (C) 2022 The Authors. Published by Elsevier B.V.",Disentangled representation; Content-style; Applications; Tutorial; Medical imaging; Computer vision
"Image captioning refers to automatic generation of descriptive texts according to the visual content of images. It is a technique integrating multiple disciplines including the computer vision (CV), natural language processing (NLP) and artificial intelligence. In recent years, substantial research efforts have been devoted to generate image caption with impressive progress. To summarize the recent advances in image captioning, we present a comprehensive review on image captioning, covering both traditional methods and recent deep learning-based techniques. Specifically, we first briefly review the early traditional works based on the retrieval and template. Then deep learning-based image captioning researches are focused, which is categorized into the encoder-decoder framework, attention mechanism and training strategies on the basis of model structures and training manners for a detailed introduction. After that, we summarize the publicly available datasets, evaluation metrics and those proposed for specific requirements, and then compare the state of the art methods on the MS COCO dataset. Finally, we provide some discussions on open challenges and future research directions.",Artificial intelligence; attention mechanism; encoder-decoder framework; image captioning; multi-modal understanding; training strategies
"Active learning is a label-efficient machine learning method that actively selects the most valuable unlabeled samples to annotate. Active learning focuses on achieving the best possible performance while using as few, high-quality sample annotations as possible. Recently, active learning achieved promotion combined with deep learning-based methods, which are named deep active learning methods in this paper. Deep active learning plays a crucial role in computer vision tasks, especially in label-insensitive scenarios, such as hard-to-label tasks (medical images analysis) and time-consuming tasks (autonomous driving). However, deep active learning still has some challenges, such as unstable performance and dirty data, which are future research trends. Compared with other reviews on deep active learning, our work introduced the deep active learning from computer vision-related methodologies and corresponding applications. The expected audience of this vision-friendly survey are researchers who are working in computer vision but willing to utilize deep active learning methods to solve vision problems. Specifically, this review systematically focuses on the details of methods, applications, and challenges in vision tasks, and we also introduce the classic theories, strategies, and scenarios of active learning in brief.",deep learning; active learning; computer vision; artificial intelligence
"Most computer vision applications demand input images to meet their specific requirements. To complete different vision tasks, e.g., object detection, object recognition, and object retrieval, low-light images must be enhanced by different methods to achieve different processing effects. The existing image enhancement methods, which are based on non-physical imaging models, and image generation methods, which are based on deep learning, are not ideal for low-light image processing. To solve the problem, this paper explores low -light image enhancement and target detection based on deep learning. Firstly, a simplified expression was constructed for the optical imaging model of low-light images, and a Haze -line was proposed for color correction of low-light images, which can effectively enhance low-light images based on the global background light and medium transmission rate of the optical imaging model of such images. Next, network framework adopted by the proposed low-light image enhancement model was introduced in detail: the framework includes two deep domain adaptation modules that realize domain transformation and image enhancement, respectively, and the loss functions of the model were presented. To detect targets based on the output enhanced image, a joint enhancement and target detection method was proposed for low-light images. The effectiveness of the constructed model was demonstrated through experiments.",computer vision; low-light images; color correction; image enhancement; object detection
"Vehicle model recognition is a typical fine-grained classification task that has a wide range of application prospects in safe cities and constitutes a research hotspot in the field of computer vision. Vehicles in images can appear at various angles, resulting in large differences in appearance. The existence of multiviews renders vehicle model recognition challenging. Recent research on vehicle model recognition has not fully explored the pose information of vehicles in different images, resulting in low model performance. In this study, we use vehicle pose information to solve the multiview vehicle model recognition (MV-VMR) problem and design a convolutional neural network (CNN) model with embedded vehicle pose information, known as the embedding pose CNN (EP-CNN). The proposed model includes two subnetworks: the pose estimation subnetwork (PE-SubNet) and vehicle model classification subnetwork (VMC-SubNet). PE-SubNet extracts the vehicle pose information, including the pose features and vehicle viewpoint. In VMC-SubNet, considering the scale variation of vehicles, an improved squeeze-and-excitation (SE) block, named the MultiSE block is implemented. We embed the vehicle viewpoint into the MultiSE block, which reweighs each channel such that the extracted features elicit different responses to different viewpoints. Subsequently, the pose features and classification features are integrated for classification. Experiments are conducted on the benchmark CompCars web-nature and Stanford Cars datasets. The results demonstrate that the proposed EP-CNN method can achieve higher recognition accuracy than most classic CNN models and several state-of-the-art fine-grained vehicle model classification algorithms. Code has been made available at: https://github.com/HFUT-CV/EP-CNN.",Feature extraction; Urban areas; Computational modeling; Task analysis; Measurement; Integrated circuit modeling; Data mining; Convolutional neural network; fine-grained classification; vehicle model recognition; pose estimation; scale-aware features
"One of the state-of-the-art computer vision applications is scene understanding and visual contextual awareness. Despite the numerous detection and classification-based studies, the literature lacks semantic segmentation methods for a more comprehensive and precise understanding of the soil included scene due to the scarcity of annotated datasets; the extracted information from an understood scene is worthwhile in project fleet management, claims management, equipment productivity analysis, safety, and soil classification. Hence, this study presents a vision-based approach for soil-included scene understanding and classifying them into different categories according to ASTM D2488, using semantic segmentation. An annotated dataset of various soil types containing 3043 images was developed to train four Deeplab v3+ variants with modified decoders. Five-fold cross-validation indicates the remarkable performance of the best variant with a mean Jaccard index of 0.9. The application and effects of subpixel upsampling and exit-flow CRF were also examined.",Scene understanding; Soil classification; Visual soil assessment; Context-aware system; Computer vision; Deep learning; Deeplab v3+; Semantic segmentation; Image segmentation; Project management
"Recently, text-to-image synthesis has become a hot issue in computer vision and has been widely concerned. Many methods have achieved encouraging results in this field at present, but it is still a great challenge to improve the quality of the synthesized image further. In this paper, we propose a multi-stage synthesis method, which starts composite from the foreground content. The whole synthesis process is divided into three stages. The first stage generates the foreground results, and the third stage synthesizes the final image results. The second stage results include two situations: one is to continue to synthesize the foreground results; the other is to synthesize the image results with background information. Experiments demonstrate that the method of continuing to generate the foreground results in the second stage can achieve better results on the Caltech-UCSD Birds (CUB) and Oxford-102 datasets, while the way of synthesizing foreground results only in the first stage can obtain better performance on the Microsoft Common Objects in Context (MS COCO) dataset. Besides, our synthesized results on the three datasets are subjectively more realistic with better detail processing. It also outperforms most existing methods in quantitative comparison results, which demonstrates the effectiveness and superiority of our method. (C) 2022 Elsevier Inc. All rights reserved.",Text-to-image synthesis; Generative adversarial networks; Computer vision; Deep learning
"Visual object tracking is an important area in computer vision, and many tracking algorithms have been proposed with promising results. Existing object tracking approaches can be categorized into generative trackers, discriminative trackers, and collaborative trackers. Recently, object tracking algorithms based on deep neural networks have emerged and obtained great attention from researchers due to their outstanding tracking performance. To summarize the development of object tracking, a few surveys give analyses on either deep or non-deep trackers. In this paper, we provide a comprehensive overview of state-of-the-art tracking frameworks including both deep and non-deep trackers. We present both quantitative and qualitative tracking results of various trackers on five benchmark datasets and conduct a comparative analysis of their results. We further discuss challenging circumstances such as occlusion, illumination, deformation, and motion blur. Finally, we list the challenges and the future work in this fast-growing field.",Object tracking; Computer vision; Discriminative trackers; Deep neural networks
"To generate a new ornamental image, add an image's oil painting style information to any image while preserving the image's semantic content. With the rapid advancement of deep learning (DL), image style transfer has become one of the most active areas of computer vision research (CV). This paper proposes an oil painting style transfer technique based on parallel convolutional neural networks to address the ineffective style transfer of locally similar regions in content images and the slow processing speed of existing methods. By incorporating Gaussian sampling and a parallelization algorithm, this method effectively transfers the style of an oil painting. The algorithm can combine the content of any image with a variety of well-known oil painting styles to create high-quality works of art. The experimental results indicate that, compared to existing methods, the proposed method can effectively reduce the style loss of the generated image, make the generated image's overall style more uniform, and produce a more pleasing visual effect.",
"Image segmentation is one of the most significant tasks in image analysis, and it plays an imperative job in image processing to analyse and attain meaningful information. Moreover, image segmentation is a major process of object recognition and categorization in computer vision domain. Image segmentation utilizes the image features for separating images into definite areas along with exclusive properties. Meanwhile, various colour image segmentation techniques are introduced in computer vision research area. However, these techniques are more time consuming and failed to afford anticipated segmentation outcome, because of poor segmentation results and high computational difficulty. To overcome these challenges, an effectual hybrid optimization-based Deep Learning (DL) technique is devised for colour image segmentation and classification in this research study. The median filter is applied for input image to eliminate the noises, which assists for better image segmentation and classification process. Moreover, Improved Invasive Weed Flower Pollination Optimization (IIWFPO) approach is introduced for image segmentation process in this work. In addition, Deep Residual Network (DRN) classifier is employed for image classification, and the classifier is trained by developed Fr-IIWFPO algorithm. The developed colour image segmentation and classification approach obtained better performance than traditional techniques with accuracy of 0.9187, sensitivity of 0.9334, and specificity of 0.8902.",Colour image segmentation; Median filtering; Improved invasive weed optimization; Flower pollination algorithm; Deep residual network
"Road cycling is a cycling discipline in which riders ride on public roads. Traffic calming measures are made to make public roads safer for everyday usage for all its users. However, these measures are not always yielding a safer cycling racecourse. In this paper we present a methodology that inspects the safety of roads tailored to road bicycle racing. The automated approach uses computer vision and geospatial analysis to give an indicative racecourse safety score based on collected, calculated and processed multimodal data. The current version of our workflow uses OpenStreetMap (OSM), turn detection and stage type / bunch sprint classification for the geospatial analysis and uses road segmentation and an extensible object detector that is currently trained to detect road cracks and imperfections for visual analysis. These features are used to create a mechanism that penalizes dangerous elements on the route based on the remaining distance and the generated penalties with its relative importance factors. This results in a comprehensive safety score along with a detailed breakdown of the most concerning passages on the course which can be used by race organizers and officials to help them in the iterative process to create an engaging, yet safe course for the riders.",Machine learning; Computer vision; Data analysis; Geospatial analysis; Sports data science
"Through the commencement of the COVID-19 pandemic, the whole globe is in disarray and debating on unique approaches to stop this viral transmission. Masks are being worn by people all around the world as one of the preventative measures to avoid contracting this sickness. Although some people are following and adopting this precaution, others are not, despite official recommendations from the administration and public health organisations has been announced. In this paper DTLMV2 (Deep Transfer Learning MobileNetV2 for the objective of classification) is proposed -A face mask identification model that can reliably determine whether an individual is wearing a mask or not is suggested and implemented in this work. The model architecture employs the peruse of MobileNetV2, a lightweight Convolutional Neural Network (CNN) that requires less computing power and can be readily integrated into computer vision and mobile systems. The computer vision with MobileNet is required to formulate a low-cost mask detection system for a group of people in open spaces that can assist in determining whether a person is wearing a mask or not, as well as function as a surveillance system since it is effective on both real-time pictures and videos. The face recognition model obtained 97.01% accuracy on validation data, 98% accuracy on training data and 97.45% accuracy on testing data. (C) 2022 Elsevier B.V. All rights reserved.",CNN; MobileNetV2; Object detection; Deep learning; Mask classifier; Computer vision; COVID19
"We present a novelty detection framework for Convolutional Neural Network (CNN) sensors that we call Sensor-Activated Feature Extraction One-Class Classification (SAFE-OCC). We show that this framework enables the safe use of computer vision sensors in process control architectures. Emergent control applications use CNN models to map visual data to a state signal that can be interpreted by the controller. Incorporating such sensors introduces a significant system operation vulnerability because CNN sensors can exhibit high prediction errors when exposed to novel (abnormal) visual data. Unfortunately, identifying such novelties in real-time is nontrivial. To address this issue, the SAFE-OCC framework leverages the convolutional blocks of the CNN to create an effective feature space to conduct novelty detection using a desired one-class classification technique. This approach engenders a feature space that directly corresponds to that used by the CNN sensor and avoids the need to derive an independent latent space. We demonstrate the effectiveness of SAFE-OCC via simulated control environments. (C) 2022 Elsevier Ltd. All rights reserved.",Computer vision; Novelty detection; Deep learning; Process control
"Inventory of stacked goods in the stereoscopic warehouse is important for modern logistics. Currently, this inventory task is completed by counting manually. With the advance of industry 4.0 and deep learning technology, automatic inventory based on machine vision comes true, greatly saving labor and material costs. In this work, we firstly collected WSGID, an image dataset about wine boxes stacked in a stereoscopic winey warehouse. Moreover, we presented an automatic inventory method based on machine vision, consisting of a stacked goods surface detecting model and a prior-based quantity calculating algorithm. To get a better detecting performance, we introduced STCNet, an improved detection network based on Swin Transformer. The final results of 86.7 mAP, 82.8 mAP, and 85.9 mAP on three sub-datasets are achieved and are higher than the baselines. To count the quantity of goods after detection, we proposed an adaptive and robust calculating algorithm. Our method got an accuracy of 85.71 on the largest sub-dataset. Extensive experiments on the WSGID and COCO benchmark demonstrate the effectiveness of our approach. Our work indicates that the machine vision method successfully facilitates inventory for stacked goods in the stereoscopic warehouse.",Computer vision; Object detection; Swin transformer; Stereoscopic inventory
"Bridge maintenance will become a widespread trend in the engineering industry as the number of bridges grows and time passes. Cracking is a common problem in bridges with concrete structures. Allowing it to expand will result in significant economic losses and accident risks This paper proposed an automatic detection and segmentation method of bridge surface cracks based on computer vision deep learning models. First, a bridge surface crack detection and segmentation dataset was established. Then, according to the characteristics of the bridge, we improved the You Only Look Once (YOLO) algorithm for bridge surface crack detection. The improved algorithm was defined as CR-YOLO, which can identify cracks and their approximate locations from multi-object images. Subsequently, the PSPNet algorithm was improved to segment the bridge cracks from the non-crack regions to avoid the visual interference of the detection algorithm. Finally, we deployed the proposed bridge crack detection and segmentation algorithm in an edge device. The experimental results show that our method outperforms other baseline methods in generic evaluation metrics and has advantages in Model Size(MS) and Frame Per Second (FPS).",Bridge crack; Crack detection; Crack segmentation; Deep learning; Computer vision
"Gesture recognition is the foremost need in building intelligent human-computer interaction systems to solve many day-to-day problems and simplify human life in this digital world. The traditional machine learning (ML) algorithm tried to capture specific handcrafted features, failed miserably in some real-world environments. Deep learning (DL) techniques have become a sensation among researchers in recent years, making the traditional ML approaches quite obsolete. However, existing reviews consider only a few datasets on which DL algorithm has been applied, and the categorization of the DL algorithms is vague in their review. This study provides the precise categorization of DL algorithms and considers around 15 gesture datasets on which these techniques have been applied. This study also provides a brief overview of the numerous challenging dataset available among the research community and insight into various challenges and limitations of a DL algorithm in vision-based dynamic gesture recognition.",datasets; deep learning; human-computer interaction; machine learning
"One of the most widespread illnesses of blindness is glaucoma. Optic nerve are essentialfor clear vision, but glaucoma effects the optic nerves and results blurred vision. This condition is often exacerbated by abnormally high intra-ocular pressure. Accurate early identification and continuous screening can help to minimize loss of vision. A non-invasive computer-aided diagnosis treatment uses optical fundus images to detect glaucoma in its early stages. This work includes image preprocessing, optic disk (OD) segmentation, feature extraction from the OD and recurrent neural network classification to identify glaucoma. The performance of the proposed system is tested using fundus image datasets such as DRISHTI-GS and Large-Scale Attention-Based Glaucoma (LAG). By this method, glaucoma detection accuracyof 96.1% is obtained for DRISHTI-GS and 92.73% for LAG dataset, which is higher thanthe existing state of arts. Proposed procedure can help ophthalmologists diagnose glaucomawith good performance.",glaucoma detection; computer-aided diagnosis; recurrent neural network
"Purpose Defects in concrete surfaces are inevitably recurring during construction, which needs to be checked and accepted during construction and completion. Traditional manual inspection of surface defects requires inspectors to judge, evaluate and make decisions, which requires sufficient experience and is time-consuming and labor-intensive, and the expertise cannot be effectively preserved and transferred. In addition, the evaluation standards of different inspectors are not identical, which may lead to cause discrepancies in inspection results. Although computer vision can achieve defect recognition, there is a gap between the low-level semantics acquired by computer vision and the high-level semantics that humans understand from images. Therefore, computer vision and ontology are combined to achieve intelligent evaluation and decision-making and to bridge the above gap. Design/methodology/approach Combining ontology and computer vision, this paper establishes an evaluation and decision-making framework for concrete surface quality. By establishing concrete surface quality ontology model and defect identification quantification model, ontology reasoning technology is used to realize concrete surface quality evaluation and decision-making. Findings Computer vision can identify and quantify defects, obtain low-level image semantics, and ontology can structurally express expert knowledge in the field of defects. This proposed framework can automatically identify and quantify defects, and infer the causes, responsibility, severity and repair methods of defects. Through case analysis of various scenarios, the proposed evaluation and decision-making framework is feasible. Originality/value This paper establishes an evaluation and decision-making framework for concrete surface quality, so as to improve the standardization and intelligence of surface defect inspection and potentially provide reusable knowledge for inspecting concrete surface quality. The research results in this paper can be used to detect the concrete surface quality, reduce the subjectivity of evaluation and improve the inspection efficiency. In addition, the proposed framework enriches the application scenarios of ontology and computer vision, and to a certain extent bridges the gap between the image features extracted by computer vision and the information that people obtain from images.",Concrete surface quality; Ontology; Computer vision; Evaluation and decision-making framework; Cracks and holes
"Health organizations advise social distancing, wearing face mask, and avoiding touching face to prevent the spread of coronavirus. Based on these protective measures, we developed a computer vision system to help prevent the transmission of COVID-19. Specifically, the developed system performs face mask detection, face-hand interaction detection, and measures social distance. To train and evaluate the developed system, we collected and annotated images that represent face mask usage and face-hand interaction in the real world. Besides assessing the performance of the developed system on our own datasets, we also tested it on existing datasets in the literature without performing any adaptation on them. In addition, we proposed a module to track social distance between people. Experimental results indicate that our datasets represent the real-world's diversity well. The proposed system achieved very high performance and generalization capacity for face mask usage detection, face-hand interaction detection, and measuring social distance in a real-world scenario on unseen data. The datasets are available at https://github.com/iremeyiokur/COVID-19-Preventions-Control-System.",COVID-19; Face mask detection; Face-hand interaction detection; Social distance measurement; CNN
"Various imaging techniques combined with machine learning (ML) models have been used to build computer-aided diagnosis (CAD) systems for breast cancer (BC) detection and classification. The rise of deep learning models in recent years, represented by convolutional neural network (CNN) models, has pushed the accuracy of ML-based CAD systems to a new level that is comparable to human experts. Existing studies have explored the usage of a wide spectrum of CNN models for BC detection, and supervised learning has been the mainstream. In this study, we propose a semi-supervised learning framework based on the Vision Transformer (ViT). The ViT is a model that has been validated to outperform CNN models on numerous classification benchmarks but its application in BC detection has been rare. The proposed method offers a custom semi-supervised learning procedure that unifies both supervised and consistency training to enhance the robustness of the model. In addition, the method uses an adaptive token sampling technique that can strategically sample the most significant tokens from the input image, leading to an effective performance gain. We validate our method on two datasets with ultrasound and histopathology images. Results demonstrate that our method can consistently outperform the CNN baselines for both learning tasks. The code repository of the project is available athttps://github.com/FeiYee/Breast-area-TWO.",semi-supervised learning; breast cancer detection; vision transformer; adaptive token sampling; data enhancement
"Adversarial examples can attack multiple unknown convolutional neural networks (CNNs) due to adversarial transferability, which reveals the vulnerability of CNNs and facilitates the development of adversarial attacks. However, most of the existing adversarial attack methods possess a limited transferability on vision transformers (ViTs). In this paper, we propose a partial blocks search attack (PBSA) method to generate adversarial examples on ViTs, which significantly enhance transferability. Instead of directly employing the same strategy for all encoder blocks on ViTs, we divide encoder blocks into two categories by introducing the block weight score and exploit distinct strategies to process them. In addition, we optimize the generation of perturbations by regularizing the self-attention feature maps and creating an ensemble of partial blocks. Finally, perturbations are adjusted by an adaptive weight to disturb the most effective pixels of original images. Extensive experiments on the ImageNet dataset are conducted to demonstrate the validity and effectiveness of the proposed PBSA. The experimental results reveal the superiority of the proposed PBSA to state-of-the-art attack methods on both ViTs and CNNs. Furthermore, PBSA can be flexibly combined with existing methods, which significantly enhances the transferability of adversarial examples.",Adversarial examples; Vision transformer; Computer vision; Deep neural network; Image classification
"Crops' production and quality of yields are heavily affected by crop diseases which cause adverse impacts on food security as well as economic losses. In India, agriculture is a prime source of income in most rural areas. Hence, there is an intense need to employ novel and accurate computer vision-based techniques for automatic crop disease detection and their classification so that prophylactic actions can be recommended in a timely manner. In literature, numerous computer vision-based techniques by utilizing divergent combinations of machine learning, deep learning, CNN, and various image-processing techniques along with their associated merits and demerits have already been discussed. In this study, we systematically reviewed recent research studies undertaken by a variety of scholars and researchers of fungal and bacterial plant disease detection and classification and summarized them based on vital parameters like type of crop utilized, deep learning/machine learning architecture used, dataset utilized for experiments, performance matrices, types of disease detected and classified, and highest accuracy achieved by the model. As per the analysis carried out, in the category of machine learning-based approaches, 70% of studies utilized real-field plant leaf images and 30% utilized laboratory condition plant leaf images for disease classification while in the case of deep learning-based approaches, 55% studied employed laboratory-conditioned images from the PlantVillage dataset, 25% utilized real-field images, and 20% utilized open image datasets. The average accuracy attained with deep learning-based approaches is quite higher at 98.8% as compared to machine learning-based approaches at 92.2%. In the case of deep learning-based methods, we also analyzed the performances of pretrained and training from scratch models that have been utilized in various studies for plant leaf disease classification. Pretrained models perform better with 99.64% classification accuracy compared to training from scratch models which achieved 98.64% average accuracy. We also highlighted some major issues encountered in the computer vision-based disease detection and classification approach used in literature and provided recommendations that will help and guide researchers to explore new dimensions in crop disease recognition.",
"Prostate cancer is one of the most common cancers in men worldwide, second only to lung cancer. The most common method used in diagnosing prostate cancer is the microscopic observation of stained biopsies by a pathologist and the Gleason score of the tissue microarray images. However, scoring prostate cancer tissue microarrays by pathologists using Gleason mode under many tissue microarray images is time-consuming, susceptible to subjective factors between different observers, and has low reproducibility. We have used the two most common technologies, deep learning, and computer vision, in this research, as the development of deep learning and computer vision has made pathology computer-aided diagnosis systems more objective and repeatable. Furthermore, the U-Net network, which is used in our study, is the most extensively used network in medical image segmentation. Unlike the classifiers used in previous studies, a region segmentation model based on an improved U-Net network is proposed in our research, which fuses deep and shallow layers through densely connected blocks. At the same time, the features of each scale are supervised. As an outcome of the research, the network parameters can be reduced, the computational efficiency can be improved, and the method's effectiveness is verified on a fully annotated dataset.",
"RGB-D data is essential for solving many problems in computer vision. Hundreds of public RGB-D datasets containing various scenes, such as indoor, outdoor, aerial, driving, and medical, have been proposed. These datasets are useful for different applications and are fundamental for addressing classic computer vision tasks, such as monocular depth estimation. This paper reviewed and categorized image datasets that include depth information. We gathered 231 datasets that contain accessible data and grouped them into three categories: scene/objects, body, and medical. We also provided an overview of the different types of sensors, depth applications, and we examined trends and future directions of the usage and creation of datasets containing depth data, and how they can be applied to investigate the development of generalizable machine learning models in the monocular depth estimation field.",RGB-D data; Monocular depth estimation; Computer vision; Depth datasets
"Computer vision systems in outdoor environments are strongly affected by different atmospheric/weather conditions. Therefore, understanding the actual behavior of outdoor scenes is necessary for effective removal and improvement of the overall performance of computer vision systems. Although the classification of atmospheric/weather conditions has been well explored, reporting on the same in multiclass problem using Convolutional Neural Networks (CNNs) has received very little attention. In response to address this disparity, we propose a new CNN architecture named the Adversarial Weather Degraded Multi-class scenes Classifi-cation Network (AWDMC-Net)'' for outdoor scene classification degraded by different atmospheric/weather conditions. The proposed network is based on adopting different combinations of skip connections in building blocks of CNN there after adaptively pruning the least important convolutional kernels from the network. For effective pruning, we proposed a new pruning criterion named Entropy Guided Mean-I1 Norm that can adaptively evaluate the importance of convolutional kernels by considering the filters and their corresponding output feature maps. The prediction performance of our proposed model was evaluated on our newly designed E-TUVD (Extended Tripura University Video Dataset) and on publicly available benchmark datasets. Our newly created video dataset, E-TUVD, consists of 147 video clips (approximately 793800 frames) that represent six atmospheric/weather conditions, namely, fog, dust, rain, haze, poor illumination, and clear day conditions. Our proposed model achieves an accuracy of 93.85%, a specificity of 93.79%, and a sensitivity of 94.18% on our dataset, which outperforms the prevailing standard CNN models and recent state-of-the-art methods for atmospheric/weather classification tasks. Furthermore, our network also reduces the time consumption for atmospheric/weather classification tasks, and therefore mostly meets the requirements of practical applications in real-world scenarios.",Atmospheric/weather conditions; E-TUVD dataset; Convolutional Neural Network; Skip connections; Pruning; Classification; Performance evaluation
"Aquaculture plays a critical role in food security and nutrition strategies. The application of intelligent aquaculture technology has shown promising performance in improving aquaculture productivity and increasing economic benefits with its rapid advancement and good prospects. However, degraded underwater images have hampered the existing computer vision applications in intelligent aquaculture. To this end, a novel Tied Bilateral learning network is proposed for Aquaculture Image Enhancement (TBAIE), which improves the degraded aquaculture images to meet the requirements of various computer vision applications in aquaculture. Concretely, a novel multiple tied guidance module is designed to generate a mull-channel feature map and capture long-range features based on input. Then, a feature fusion module is introduced with a novel tied attention block to blend the feature and suppress noise with a low computational resource. Experimental results demonstrate that the proposed TBAIE can improve the quality of aquaculture images and remove color distortion. Moreover, TBAIE can achieve state-of-the-art in quantitative and qualitative metrics and meet the practical requirements of different aquaculture vision tasks.",Tied Bilateral learning; Digital aquaculture; Aquaculture Image Enhancement
"Tomato is a widely consumed fruit across the world due to its high nutritional values. Leaf diseases in tomato are very common which incurs huge damages but early detection of leaf diseases can help in avoiding that. The existing practices for detecting different diseases by the human experts are costly, time consuming and subjective in nature. Computer vision plays important role toward early detection of tomato leaf detection. However, implementation of computationally less expensive model and improvement of detection performance is still open. This article reports a computer vision based system to classify seven different categories of diseases, namely, bacterial spot, early blight, late blight, leaf mold, septoria leaf spot, spider mites, and target spots using optimized MobileNetV2 architecture. A modified gray wolf optimization approach has been adopted for optimization of MobileNetV2 hyperparameters for improved performance. The model has been validated using standard internal and external validation methods and found to provide the classification accuracy in the tune of 98%. The results reflect the promising potential of the presented framework for early detection of tomato leaf diseases which can help to avoid substantial agricultural loss.",classification of plant diseases; computer vision; convolutional neural network; hyperparameter optimization; leaf disease identification; modified gray wolf optimization
"Automated real time quality monitoring is one of the key enablers for future high-speed production. In this research, an in-process monitoring procedure based on computer vision inspection and deep learning is proposed to indicate the tool and part quality during soft tooling injection moulding. Multiple types of injection moulding defects can be detected by the proposed method. Geometrical dimensions of the part can be measured simultaneously and the uncertainty can be quantified. Based on the obtained data, automated quality evaluation can be achieved in-process and a decision signal can be sent back to the injection moulding system for process adjustment. (C) 2022 The Author(s). Published by Elsevier Ltd on behalf of CIRP.",Digital manufacturing system; In-process measurement; Injection moulding
"Statement of Significance: The latest advances of computer vision approaches for dietary assessment are described in this review, and recent applications of image-based food recognition systems (IBFRS) in professional dietetic practice are presented. Open issues that should be tackled in the near future via interdisciplinary research to optimize the performance of IBFRS as well as to increase their adoption by the professionals of the field have been examined and discussed. Dietary assessment can be crucial for the overall well-being of humans and, at least in some instances, for the prevention and management of chronic, life-threatening diseases. Recall and manual record-keeping methods for food-intake monitoring are available, but often inaccurate when applied for a long period of time. On the other hand, automatic record-keeping approaches that adopt mobile cameras and computer vision methods seem to simplify the process and can improve current human-centric diet-monitoring methods. Here we present an extended critical literature overview of image-based food-recognition systems (IBFRS) combining a camera of the user's mobile device with computer vision methods and publicly available food datasets (PAFDs). In brief, such systems consist of several phases, such as the segmentation of the food items on the plate, the classification of the food items in a specific food category, and the estimation phase of volume, calories, or nutrients of each food item. A total of 159 studies were screened in this systematic review of IBFRS. A detailed overview of the methods adopted in each of the 78 included studies of this systematic review of IBFRS is provided along with their performance on PAFDs. Studies that included IBFRS without presenting their performance in at least 1 of the above-mentioned phases were excluded. Among the included studies, 45 (58%) studies adopted deep learning methods and especially convolutional neural networks (CNNs) in at least 1 phase of the IBFRS with input PAFDs. Among the implemented techniques, CNNs outperform all other approaches on the PAFDs with a large volume of data, since the richness of these datasets provides adequate training resources for such algorithms. We also present evidence for the benefits of application of IBFRS in professional dietetic practice. Furthermore, challenges related to the IBFRS presented here are also thoroughly discussed along with future directions.",nutrition monitoring; food image recognition; dietary assessment; machine learning; deep learning; artificial intelligence; computer vision; image-based food recognition
"Surface defect detection is a vital process in industrial production and a significant research direction in computer vision. Although today's deep learning defect detection methods based on computer vision can achieve high detection accuracy, they are mainly based on supervised learning. They require many defect samples to train the model, which is not compatible with the current situation that industrial defect sample is difficult to obtain and costly to label. So we propose a new unsupervised small sample defect detection model-ISU-GAN, which is based on the CycleGAN architecture. A skip connection, SE module, and Involution module are added to the Generator, enabling the feature extraction capability of the model to be significantly improved. Moreover, we propose an SSIM-based defect segmentation method that applies to GAN-based defect detection and can accurately extract defect contours without the need for redundant noise reduction post-processing. Experiments on the DAGM2007 dataset show that the unsupervised ISU-GAN can achieve higher detection accuracy and finer defect profiles with less than 1/3 of the unlabelled training data than the supervised model with the full training set. Relative to the supervised segmentation models UNet and ResUNet++ with more training samples, our model improves the detection accuracy by 2.84% and 0.41% respectively and the F1 score by 0.025 and 0.0012 respectively. In addition, the predicted profile obtained using our method is closer to the real profile than other models used for comparison.",
"The study of complex diseases relies on large amounts of data to build models toward precision medicine. Such data acquisition is feasible in the context of high-throughput screening, in which the quality of the results relies on the accuracy of the image analysis. Although state-of-the-art solutions for image segmentation employ deep learning approaches, the high cost of manually generating ground truth labels for model training hampers the day-to-day application in experimental laboratories. Alternatively, traditional computer vision-based solutions do not need expensive labels for their implementation. Our work combines both approaches by training a deep learning network using weak training labels automatically generated with conventional computer vision methods. Our network surpasses the conventional segmentation quality by generalising beyond noisy labels, providing a 25% increase of mean intersection over union, and simultaneously reducing the development and inference times. Our solution was embedded into an easy-to-use graphical user interface that allows researchers to assess the predictions and correct potential inaccuracies with minimal human input. To demonstrate the feasibility of training a deep learning solution on a large dataset of noisy labels automatically generated by a conventional pipeline, we compared our solution against the common approach of training a model from a small manually curated dataset by several experts. Our work suggests that humans perform better in context interpretation, such as error assessment, while computers outperform in pixel-by-pixel fine segmentation. Such pipelines are illustrated with a case study on image segmentation for autophagy events. This work aims for better translation of new technologies to real-world settings in microscopy-image analysis.",
"This paper presents an intelligent photo interpretation approach to automatically monitor and characterize dense interconnected microcracks in strain-hardening cementitious composite (SHCC) featuring unique crack patterns in terms of crack number and crack width. The presented approach employs a stereo vision system that integrates binocular and monocular cameras for automatic detection, ranging, and quantification of cracks as well as characterization of crack patterns. The presented approach was implemented into evaluation of SHCC in flexural tests and direct tension tests. Dense microcracks were detected and ranged by the stereo vision system, segmented by an encoder-decoder approach, and quantified by an efficient computer vision approach. Evolution of the cracks was traced throughout the loading process until failure, and a statistical analysis revealed that the crack width was retained while the crack number monotonically increased. The interpretation time was shorter than 0.4 s for each photo, making the approach promising for monitoring of SHCC. The proposed system can be deployed for automated assessment of cementitious composites with complex crack patterns in material research and engineering structures.",Binocular stereo vision; Computer vision; Crack detection; Crack quantification; Deep learning; Strain-hardening cementitious composites; (SHCC)
"Food allergies impose a significant health concern on the community. A small number of certain food items can cause an allergic reaction within the human body. The symptoms can range from mild hives or itchiness to life-threatening anaphylaxis. In most cases, such reactions can be prevented by simply being aware of the allergen-based food items and avoiding the consumption of the same. We are among the first research attempts to train a deep learning-based object detection model to detect the presence of such food items within an image. We introduce our Allergen30 dataset, which hosts more than 6,000 annotated images of 30 commonly used food items that can trigger an adverse reaction. We report the comparison of multiple variants of the current state-of-art object detection methods, YOLOv5 and YOLOR. Furthermore, we qualitatively analyzed the performance of these methods by surveying the predictions made on the test dataset images.",Food allergy; Food dataset; Deep learning; Computer vision; Object detection
"On June 24, 2021, a 12-story condominium building (Champlain Towers South) in Surfside, Florida partially collapsed, resulting in one of the deadliest building collapses in United States history with 98 people confirmed deceased. In this work, we analyze the collapse event using a video clip that is publicly available on social media. In our analysis, we apply computer vision algorithms to corroborate new information from the video clip that may not be readily interpreted by human eyes. By comparing the differential features against different video frames, our proposed method is used to quantify the falling structural components by mapping the directions and magnitudes of their movements. We demonstrate the potential of this video processing methodology in in-vestigations of catastrophic structural failures and hope our approach may serve as a basis for further in-vestigations into structure collapse events.",Building collapse; Hazard mitigation; Feature tracking; Social network; Video processing; Computer vision
"In order to meet the needs of accurately grasping the situation of people in the mall at all times, the author proposes an analysis method based on computer vision for people flow image detection system. This method combines the HOG feature with the SVM classifier, detects pedestrians through dual cameras, and builds an experimental research platform for dual-camera joint detection of pedestrians. The result shows that the error rate of human flow detected by the author's method is the lowest of 0% and the highest of 6.25%. Conclusion. This method has a good effect on the statistics of the number of people in the shopping mall and can reduce the workload of the monitoring personnel in the shopping mall.",
"Although deep learning-based computer-aided diagnosis systems have recently achieved expert-level performance, developing a robust model requires large, high-quality data with annotations that are expensive to obtain. This situation poses a conundrum that annually-collected chest x-rays cannot be utilized due to the absence of labels, especially in deprived areas. In this study, we present a framework named distillation for self-supervision and self-train learning (DISTL) inspired by the learning process of the radiologists, which can improve the performance of vision transformer simultaneously with self-supervision and self-training through knowledge distillation. In external validation from three hospitals for diagnosis of tuberculosis, pneumothorax, and COVID-19, DISTL offers gradually improved performance as the amount of unlabeled data increase, even better than the fully supervised model with the same amount of labeled data. We additionally show that the model obtained with DISTL is robust to various real-world nuisances, offering better applicability in clinical setting. Although deep learning-based computer-aided diagnosis systems have recently achieved expert level performance, developing a robust model requires large, high-quality data with annotations. Here, the authors present a framework which can improve the performance of vision transformer simultaneously with self-supervision and self-training.",
"With the advancement of automation, vision-based hand gesture recognition (HGR) is gaining popularity due to its numerous uses and ability to easily communicate with machines. However, identifying hand positions is the most difficult assignment due to the fact of crowded backgrounds, sensitivity to light, form, speed, size, and self-occlusion. This review summarizes the most recent studies on hand postures and motion tracking using a vision-based approach by applying Preferred Reporting Items for Systematic Reviews and Meta-Analysis (PRISMA). The parts and subsections of this review article are organized into numerous categories, the most essential of which are picture acquisition, preprocessing, tracking and segmentation, feature extraction, collation of key gesture identification phases, and classification. At each level, the various algorithms are evaluated based on critical key points such as localization, largest blob, per pixel binary segmentation, depth information, and so on. Furthermore, the datasets and future scopes of HGR approaches are discussed considering merits, limitations, and challenges.",Hand gesture recognition; spatio-temporal features; depth sequence; human-computer interaction (HCI)
"The natural phenomenon of harmful algae bloom (HAB) has a bad impact on the quality of pure and freshwater. It increases the risk to human health, water bodies and overall aquatic ecosystem. It is necessary to continuously monitor and perform proper action against HAB. The inspection of algae blooms by using conventional methods, like algae detection under microscopes, is a difficult, expensive, and time-consuming task, however, computer vision-based deep learning models play a vital role in identifying and detecting harmful algae growth in aquatic ecosystems and water reservoirs. Many studies have been conducted to address harmful algae growth by using a CNN based model, however, the YOLO model is considered more accurate in identifying the algae. This advanced deep learning method is extensively used to detect algae and classify them according to their corresponding category. In this study, we used various versions of the convolution neural network (CNN) based on the You Only Look Once (YOLO) model. Recently YOLOv5 has been getting more attention due to its performance in real-time object detection. We performed a series of experiments on our custom microscopic images dataset by using YOLOv3, YOLOv4, and YOLOv5 to detect and classify the harmful algae bloom (HAB) of four classes. We used pre-processing techniques to enhance the quantity of data. The mean average precision (mAP) of YOLOv3, YOLOv4, and YOLO v5 is 75.3%, 83.0%, and 91.0% respectively. For the monitoring of algae bloom in freshwater, computer-aided based systems are very helpful and effective. To the best of our knowledge, this work is pioneering in the AI community for applying the YOLO models to detect algae and classify from microscopic images.",HAB; algae detection; object detection; YOLO model; microscopic image
"Facial expressions are a prevalent way to recognize human emotions, and automatic facial expression recognition (FER) has been a significant task in cognitive science, artificial intelligence, and computer vision. The critical issue with the design of the FER model is the strong intra-class correlation of different emotions. The accuracy of the FER model is reduced due to other problems such as the variations in expressing the emotions, variations in lighting, and different ethnic biases. The latest convolutional neural network-based FER models have shown significant improvement in accuracy score but lack distinguishing the micro-expressions. This paper proposed a multi-input hybrid FER model that considers both hand-engineered and self-learnt features to classify facial expressions. The VGG-Face and the histogram of oriented gradients (HOG) features are derived from the faces to distinguish various facial expression patterns. The fusion of deep (VGG-Face) and hand-engineered (HOG) features has shown improved accuracy compared to the conventional CNN models. The results obtained showed that the proposed model's accuracy scores outperformed the accuracy scores of the other popular FER models on three facial expression datasets. Extended Cohn-Kanade (CK+), Yale-Face, and Karolinska directed emotional faces (KDEF) datasets are used to determine the model's classification efficiency. The proposed model scored 98.12%, 95.26%, and 96.36% accuracy using a fivefold cross-validation process on the CK+, Yale-Face and KDEF datasets.",Histogram of oriented gradients; Convolutional neural networks; VGG-Face; Computer-vision; Feature-fusion
"Object detection is a computer vision based technique which is used to detect instances of semantic objects of a particular class in digital images and videos. Crowd density analysis is one of the commonly utilized applications of object detection. Since crowd density classification techniques face challenges like non-uniform density, occlusion, inter-scene, and intra-scene deviations, convolutional neural network (CNN) models are useful. This paper presents a Metaheuristics with Deep Transfer Learning Enabled Intelligent Crowd Density Detection and Classification (MDTL-ICDDC) model for video surveillance systems. The proposed MDTL-ICDDC technique mostly concentrates on the effective identification and classification of crowd density on video surveillance systems. In order to achieve this, the MDTL-ICDDC model primarily leverages a Salp Swarm Algorithm (SSA) with NASNetLarge model as a feature extraction in which the hyperparameter tuning process is performed by the SSA. Furthermore, a weighted extreme learning machine (WELM) method was utilized for crowd density and classification process. Finally, the krill swarm algorithm (KSA) is applied for an effective parameter optimization process and thereby improves the classification results. The experimental validation of the MDTL-ICDDC approach was carried out with a benchmark dataset, and the outcomes are examined under several aspects. The experimental values indicated that the MDTL-ICDDC system has accomplished enhanced performance over other models such as Gabor, BoW-SRP, Bow-LBP, GLCM-SVM, GoogleNet, and VGGNet.",object detection; object tracking; video surveillance; computer vision; crowd density estimation; deep learning; parameter optimization
"Deep vision multimodal learning aims at combining deep visual representation learning with other modalities, such as text, sound, and data collected from other sensors. With the fast development of deep learning, vision multimodal learning has gained much interest from the community. This paper reviews the types of architectures used in multimodal learning, including feature extraction, modality aggregation, and multimodal loss functions. Then, we discuss several learning paradigms such as supervised, semi-supervised, self-supervised, and transfer learning. We also introduce several practical challenges such as missing modalities and noisy modalities. Several applications and benchmarks on vision tasks are listed to help researchers gain a deeper understanding of progress in the field. Finally, we indicate that pretraining paradigm, unified multitask framework, missing and noisy modality, and multimodal task diversity could be the future trends and challenges in the deep vision multimodal learning field. Compared with existing surveys, this paper focuses on the most recent works and provides a thorough discussion of methodology, benchmarks, and future trends.",multimodal learning; computer vision; deep learning; introductory; survey
"Face mask detection has become a great challenge in computer vision, demanding the coalition of technology with COVID-19 awareness. Researchers have proposed deep learning models to detect the use of face masks. However, the incorrect use of a face mask can be as harmful as not wearing any protection at all. In this paper, we propose a compound convolutional neural network (CNN) architecture based on two computer vision tasks: object localization to discover faces in images/videos, followed by an image classification CNN to categorize the faces and show if someone is using a face mask correctly, incorrectly, or not at all. The first CNN is built upon RetinaFace, a model to detect faces in images, whereas the second CNN uses a ResNet-18 architecture as a classification backbone. Our model enables an accurate identification of people who are not correctly following the COVID-19 healthcare recommendations on face mask use. To enable further global use of our technology, we have released both the dataset used to train the classification model and our proposed computer vision pipeline to the public, and optimized it for embedded systems deployment.",artificial intelligence; deep learning; computer vision; face mask recognition; object detection; image classification; COVID-19
"In this paper, we introduce a novel vision-based framework for tracking multiple active objects using guidance laws based on a rendezvous cone method. These guidance laws enable an unmanned aircraft system, equipped with a monocular camera, to continuously observe a set of moving objects within the field of view of its sensor. During the multi-object tracking process, we detect and categorize feature point estimators for controlling the occurrence of occlusions in a comprehensive fashion. Furthermore, we extend our open-source simulation environment and perform a series of simulations to show the efficacy of our proposed approach.",Autonomous systems; Unmanned aerial vehicles; Visual tracking
"Machine vision is being employed in defect detection, size measurement, pattern recognition, image fusion, target tracking and 3D reconstruction. Traditional cancer detection methods are dominated by manual detection, which wastes time and manpower, and heavily relies on the pathologists' skill and work experience. Therefore, these manual detection approaches are not convenient for the inheritance of domain knowledge, and are not suitable for the rapid development of medical care in the future. The emergence of machine vision can iteratively update and learn the domain knowledge of cancer cell pathology detection to achieve automated, high-precision, and consistent detection. Consequently, this paper reviews the use of machine vision to detect cancer cells in histopathology images, as well as the benefits and drawbacks of various detection approaches. First, we review the application of image preprocessing and image segmentation in histopathology for the detection of cancer cells, and compare the benefits and drawbacks of different algorithms. Secondly, for the characteristics of histopathological cancer cell images, the research progress of shape, color and texture features and other methods is mainly reviewed. Furthermore, for the classification methods of histopathological cancer cell images, the benefits and drawbacks of traditional machine vision approaches and deep learning methods are compared and analyzed. Finally, the above research is discussed and forecasted, with the expected future development tendency serving as a guide for future research.",Machine vision; Traditional machine learning; Deep learning; Histopathological images; Cancer cell detection; Image preprocessing and segmentation; Feature extraction; Classification
"In object detection, false negatives arise when a detector fails to detect a target object. To understand why object detectors produce false negatives, we identify five 'false negative mechanisms,' where each mechanism describes how a specific component inside the detector architecture failed. Focusing on two-stage and one-stage anchor-box object detector architectures, we introduce a framework for quantifying these false negative mechanisms. Using this framework, we investigate why Faster R-CNN and RetinaNet fail to detect objects in benchmark vision datasets and robotics datasets. We show that a detector's false negative mechanisms differ significantly between computer vision benchmark datasets and robotics deployment scenarios. This has implications for the translation of object detectors developed for benchmark datasets to robotics applications.",Deep learning for visual perception; object detection; segmentation and categorization
"Image segmentation is a key task in computer vision and image processing with important applications such as scene understanding, medical image analysis, robotic perception, video surveillance, augmented reality, and image compression, among others, and numerous segmentation algorithms are found in the literature. Against this backdrop, the broad success of deep learning (DL) has prompted the development of new image segmentation approaches leveraging DL models. We provide a comprehensive review of this recent literature, covering the spectrum of pioneering efforts in semantic and instance segmentation, including convolutional pixel-labeling networks, encoder-decoder architectures, multiscale and pyramid-based approaches, recurrent networks, visual attention models, and generative models in adversarial settings. We investigate the relationships, strengths, and challenges of these DL-based segmentation models, examine the widely used datasets, compare performances, and discuss promising research directions.",Image segmentation; Computer architecture; Semantics; Deep learning; Computational modeling; Generative adversarial networks; Logic gates; Image segmentation; deep learning; convolutional neural networks; encoder-decoder models; recurrent models; generative models; semantic segmentation; instance segmentation; panoptic segmentation; medical image segmentation
"Livestock farming is assisted more and more by technological solutions, such as robots. One of the main problems for shepherds is the control and care of livestock in areas difficult to access where grazing animals are attacked by predators such as the Iberian wolf in the northwest of the Iberian Peninsula. In this paper, we propose a system to automatically generate benchmarks of animal images of different species from iNaturalist API, which is coupled with a vision-based module that allows us to automatically detect predators and distinguish them from other animals. We tested multiple existing object detection models to determine the best one in terms of efficiency and speed, as it is conceived for real-time environments. YOLOv5m achieves the best performance as it can process 64 FPS, achieving an mAP (with IoU of 50%) of 99.49% for a dataset where wolves (predator) or dogs (prey) have to be detected and distinguished. This result meets the requirements of pasture-based livestock farms.",computer vision; threat identification; wolf recognition; herding; sheepdog robots; precision livestock farming
"Biometric technologies, such as handwritten signature verification, are extremely useful for identifying individuals inside an organization or finance department. The improvement of picture categorization using deep learning (DL) neural networks has offered an opportunity to exhibit computer vision in contemporary research applications by applying image processing approaches. Manual signature verification is inefficient, error-prone, time-consuming, and inconvenient; therefore, it is critical to create an automatic signature verification recognition system. This research offers an automatic recognition method based on DL that makes use of the Grupo de Procesado Digital de Seales. The biggest publicly accessible handwritten signature dataset, the synthetic signature dataset, was used to classify the signatures of 100 people, each of whom possessed 24 genuine signatures and 30 forged signatures. An inception V3 transfer learning (TL) model is proposed by hyper-tuning different layers from the middle of its architecture and this model is fine-tuned by adding layers, such as flatten, dense (1024), dropout (0.5), and dense (1). The suggested model was tested against six well-known pre-trained TL convolutional neural network models: VGG 16, VGG 19, ResNet 50, ResNet 101, MobileNet, and EfficientNet. The suggested model surpasses the pre-trained models. Precision, sensitivity, and F1-score are likewise outperformed by the model, with the values of 88%, 88%, and 87%, respectively. The accuracy of the pre-trained models was evaluated as 80%, 81%, 77%, 73%, 71%, and 74%, respectively. The suggested fine-tuned inception V3 gives the highest accurate classifications, distinguishing between genuine and forged signatures with an accuracy of 88%. This study will aid researchers in developing more effective CNN-based models for offline signature verification with application to computer vision.",signature verification; computer vision; convolutional neural network; Grupo de Procesado Digital de Seales synthetic database; transfer learning; deep neural network
"Object detection is a fundamental part of computer vision, with a wide range of real-world applications. It involves the detection of various objects in digital images or video. In this paper, we propose a proof of concept usage of computer vision algorithms to improve the maintenance of railway tracks operated by Via Rail Canada. Via Rail operates about 500 trains running on 12,500 km of tracks. These tracks pass through long stretches of sparsely populated lands. Maintaining these tracks is challenging due to the sheer amount of resources required to identify the points of interest (POI), such as growing vegetation, missing or broken ties, and water pooling around the tracks. We aim to use the YOLO algorithm to identify these points of interest with the help of aerial footage. The solution shows promising results in detecting the POI based on unmanned aerial vehicle (UAV) images. Overall, we achieved a precision of 74% across all POI and a mean average precision @ 0.5 (mAP @ 0.5) of 70.7%. The most successful detection was the one related to missing ties, vegetation, and water pooling, with an average accuracy of 85% across all three POI.",UAV; railway; computer vision
"Object detection from image is more challenging and integral part in the inter-discipline area of computer vision. The computer vision is highly attractive in many applications like human pose estimation, instance segmentation, recognizing action, disease predictions object prediction and many more applications. The traditional method of detecting objects from the images is done using bounding boxes with labels. It suffers from the overlapping of the boxes with various smaller objects, which leads to accuracy issues in detection problems. Hence, machine learning techniques are used to detect the relevant objects from the image using center point to avoid the nonmaximal suppression in bounding box. To accurately identify images, an U-Net architecture based object detection method is proposed. In this model, it effectively uses semantic level segmentation and instance segmentation. This system effectively identifies all the objects present in the given image using the efficient hybrid segmentation models and Gromov Hausdroff distance measure. For experimentation, two data sets are used for evaluation of the model to identify all categories of objects from the image. The proposed model achieves an accuracy of 91.8% and reliable when compared to existing effective object detection algorithms like fully convolution network (FCN), YOLO (you only look once) and mask region based-convolutional neural network (mask R-CNN) model.",CNN; image processing; image segmentation; machine learning; object detection; U-Net architecture
"Data augmentation is an established technique in computer vision to foster the generalization of training and to deal with low data volume. Most data augmentation and computer vision research are focused on everyday images such as traffic data. The application of computer vision techniques in domains like marine sciences has shown to be not that straightforward in the past due to special characteristics, such as very low data volume and class imbalance, because of costly manual annotation by human domain experts, and general low species abundances. However, the data volume acquired today with moving platforms to collect large image collections from remote marine habitats, like the deep benthos, for marine biodiversity assessment and monitoring makes the use of computer vision automatic detection and classification inevitable. In this work, we investigate the effect of data augmentation in the context of taxonomic classification in underwater, i.e., benthic images. First, we show that established data augmentation methods (i.e., geometric and photometric transformations) perform differently in marine image collections compared to established image collections like the Cityscapes dataset, showing everyday traffic images. Some of the methods even decrease the learning performance when applied to marine image collections. Second, we propose new data augmentation combination policies motivated by our observations and compare their effect to those proposed by the AutoAugment algorithm and can show that the proposed augmentation policy outperforms the AutoAugment results for marine image collections. We conclude that in the case of small marine image datasets, background knowledge, and heuristics should sometimes be applied to design an effective data augmentation method.",marine objects classification; underwater computer vision; deep learning; data augmentation
"As a challenging task in computer vision, instance segmentation has attracted extensive attention in recent years. Able to obtain very rich and refined object information, this technology shows important application value in many fields, such as intelligent driving, medical health, and remote sensing detection. Instance segmentation technology should not only identify the positions of objects but should also accurately mark the boundary of any single instance, which can be defined as solving object detection and semantic segmentation at the same time. Our study gives a detailed introduction to the background of instance segmentation technology, its development and the common datasets in this field, and further deeply discusses key issues appearing in the development of this field, with the future development direction of instance segmentation technology proposed. Our study provides an important reference for future research on this technology",instance segmentation; image segmentation; deep learning; computer vision
"The type and duration of construction workers' activities are useful information for project management purposes. Therefore, several studies have used surveillance cameras and computer vision to automate the time-consuming process of manually gathering this information. However, the three-stage method they have adopted consisting of separate detection, tracking, and activity classification modules is not fully optimized. Additionally, the activity classification module is trained per-clip/segment on trimmed video clips and fails when applied to long untrimmed construction videos. This paper aims to (1) investigate the benefits of a fully optimized method such as you only watch once (YOWO) and a per-frame and per-worker annotated untrimmed data set over the previous approach for activity recognition of construction workers; (2) propose an improved version of YOWO, called YOWO53, to improve detection performance; (3) propose a semiautomatic data set annotation; (4) conduct a sensitivity analysis to compare the performance of YOWO, YOWO53, and the three-stage method; and (5) conduct a case study to compute the percentage of different workers' activities. YOWO53 improves the detection recall of YOWO by up to 3%, and the classification accuracy of the three-stage method by 16.3%. Although YOWO53 has a lower inference speed, it is still sufficiently fast for productivity analysis.",Construction workers' activity recognition; Computer vision; Convolutional neural networks
"Object detectors are vital to many modern computer vision applications. However, even state-of-the-art object detectors are not perfect. On two images that look similar to human eyes, the same detector can make different predictions because of small image distortions like camera sensor noise and lighting changes. This problem is called inconsistency. Existing accuracy metrics do not properly account for inconsistency, and similar work in this area only targets improvements on artificial image distortions. Therefore, we propose a method to use nonartificial video frames to measure object detection consistency over time, across frames. Using this method, we show that the consistency of modern object detectors ranges from 83.2% to 97.1% on different video datasets from the multiple object tracking challenge. We conclude by showing that applying image distortion corrections such as WEBP Image Compression and Unsharp Masking can improve consistency by as much as 5.1%, with no loss in accuracy.",Detectors; Distortion; Neural networks; Measurement; Behavioral sciences; Computer vision; Cameras
"Recently, self-supervised learning methods have been shown to be very powerful and efficient for yielding robust representation learning by maximizing the similarity across different augmented views in embedding vector space. However, the main challenge is generating different views with random cropping; the semantic feature might exist differently across different views leading to inappropriately maximizing similarity objective. We tackle this problem by introducing Heuristic Attention Representation Learning (HARL). This self-supervised framework relies on the joint embedding architecture in which the two neural networks are trained to produce similar embedding for different augmented views of the same image. HARL framework adopts prior visual object-level attention by generating a heuristic mask proposal for each training image and maximizes the abstract object-level embedding on vector space instead of whole image representation from previous works. As a result, HARL extracts the quality semantic representation from each training sample and outperforms existing self-supervised baselines on several downstream tasks. In addition, we provide efficient techniques based on conventional computer vision and deep learning methods for generating heuristic mask proposals on natural image datasets. Our HARL achieves +1.3% advancement in the ImageNet semi-supervised learning benchmark and +0.9% improvement in AP(50) of the COCO object detection task over the previous state-of-the-art method BYOL. Our code implementation is available for both TensorFlow and PyTorch frameworks.",heuristic attention; perceptual grouping; self-supervised learning; visual representation learning; deep learning; computer vision
"With the advent of deep learning, many dense prediction tasks, i.e., tasks that produce pixel-level predictions, have seen significant performance improvements. The typical approach is to learn these tasks in isolation, that is, a separate neural network is trained for each individual task. Yet, recent multi-task learning (MTL) techniques have shown promising results w.r.t. performance, computations and/or memory footprint, by jointly tackling multiple tasks through a learned shared representation. In this survey, we provide a well-rounded view on state-of-the-art deep learning approaches for MTL in computer vision, explicitly emphasizing on dense prediction tasks. Our contributions concern the following. First, we consider MTL from a network architecture point-of-view. We include an extensive overview and discuss the advantages/disadvantages of recent popular MTL models. Second, we examine various optimization methods to tackle the joint learning of multiple tasks. We summarize the qualitative elements of these works and explore their commonalities and differences. Finally, we provide an extensive experimental evaluation across a variety of dense prediction benchmarks to examine the pros and cons of the different methods, including both architectural and optimization based strategies.",Task analysis; Deep learning; Optimization; Neural networks; Computer architecture; Taxonomy; Computer vision; Multi-task learning; dense prediction tasks; pixel-level tasks; optimization; convolutional neural networks
"The convolutional neural network (CNN) has become a basic model for solving many computer vision problems. In recent years, a new class of CNNs, recurrent convolution neural network (RCNN), inspired by abundant recurrent connections in the visual systems of animals, was proposed. The critical element of RCNN is the recurrent convolutional layer (RCL), which incorporates recurrent connections between neurons in the standard convolutional layer. With increasing number of recurrent computations, the receptive fields (RFs) of neurons in RCL expand unboundedly, which is inconsistent with biological facts. We propose to modulate the RFs of neurons by introducing gates to the recurrent connections. The gates control the amount of context information inputting to the neurons and the neurons' RFs therefore become adaptive. The resulting layer is called gated recurrent convolution layer (GRCL). Multiple GRCLs constitute a deep model called gated RCNN (GRCNN). The GRCNN was evaluated on several computer vision tasks including object recognition, scene text recognition and object detection, and obtained much better results than the RCNN. In addition, when combined with other adaptive RF techniques, the GRCNN demonstrated competitive performance to the state-of-the-art models on benchmark datasets for these tasks.",Radio frequency; Neurons; Logic gates; Convolution; Task analysis; Computational modeling; Optical character recognition software; Gated recurrent convolution neural network (GRCNN); gated recurrent convolution layer (GRCL); object recognition; object detection; scene text recognition
"Optical flow estimation is a fundamental task in computer vision and image processing. Due to the difficulty in obtaining the ground truth of flow field, unsupervised learning approaches attract more and more research interests in recent years. However, despite of their good generalization capability, unsupervised optical flow methods suffer in the scenarios with large displacement, small objects, and occlusions. In this work, we propose a novel optical flow network based on decoder with multi-scale kernels. Different from previous U-Net like or pyramidal methods, we design our network based on RAFT architecture that with a 4D correlation layer and recurrent decoder. More importantly, we incorporate three novel ideas with regard to the input, information processing and output of the update units improve the performance. Firstly, we utilize various motion-related information as input to the update units. Secondly, we propose a module of multi-scale update unit. Thirdly, for the final flow up-sampling procedure, we propose an image-guided up-sampling loss to guide the learning of up-sampling masks. Our model is trained by the occlusion-aware photometric loss, edge-aware smoothness loss, self-supervised loss, and image-guided up-sampling loss. Experimental results demonstrate that our model achieves the state-of-the-art performance on both Sintel and KITTI and outperforms other unsupervised optical flow methods remarkably.",Optical flow; Decoding; Estimation; Optical losses; Computer vision; Image motion analysis; Correlation; Optical flow; unsupervised learning; multi-scale decoder; recurrent decoder
"Environment perception and understanding represent critical aspects in most computer vision systems and/or applications. State-of-the-art techniques to solve this vision task (e.g., semantic instance segmentation) require either dedicated hardware resources to run or a longer execution time. Generally, the main efforts were to improve the accuracy of these methods rather than make them faster. This paper presents a novel solution to speed up the semantic instance segmentation task. The solution combines two state-of-the-art methods from semantic instance segmentation and optical flow fields. To reduce the inference time, the proposed framework (i) runs the inference on every 5th frame, and (ii) for the remaining four frames, it uses the motion map computed by optical flow to warp the instance segmentation output. Using this strategy, the execution time is strongly reduced while preserving the accuracy at state-of-the-art levels. We evaluate our solution on two datasets using available benchmarks. Then, we conclude on the results obtained, highlighting the accuracy of the solution and the real-time operation capability.",machine vision; scene understanding; semantic instance segmentation; dense optical flow; real-time
"Vision-language-navigation(VLN) is a challenging task that requires a robot to autonomously move to a destination based on visual observation following a human's natural language instructions. To improve the performance and generalization ability, the pre-training model based on the transformer is used instead of the traditional methods. However, the pre-training model is not suitable for sustainable computing and practical application because of its complex computations and large amount of hardware occupation. Therefore, we propose a slight pre-training model through knowledge distillation. Through knowledge distillation, the plenty of knowledge encoded in a large teacher model can be well transferred to a small student model, which greatly reduces the model parameters and inference time while maintaining the original performance. In the experiments, the model size is reduced by 87%, and the average inference time is reduced by approximately 86%. It can be trained and run much faster. At the same time, 95% performance of the original model was maintained, which is still better than the traditional VLN models.",Natural language processing; Computer vision; Cross-modality; Deep learning
"Object detection for industrial applications refers to analyzing the captured images and videos and finding the relationship between the detected objects for better optimization, data mining for decision making, and improved system performances. The dawn of the Internet of Things and the massive deployment of electronic sensors in the industrial floor lines, such as vision, opened new horizons for the analytics tools for processing. Fundamentals of Computer Vision are being used for analyzing big manufacturing data. Deep learning-based methods have recently overcome the problems existing in traditional methods by constructing deep Convolutional Neural Networks that extract multiple low-level and high-level features from the massive volume of labeled and unlabeled data. This paper presents a comprehensive survey of deep learning-based state-of-the-art object detection methods. It discusses their applications in an industrial setting where human workers perform specific tasks using different tools on assembly lines. Firstly, object detection methods using deep learning are discussed while their advantage over traditional methods is introduced. The current techniques for object detection algorithms and their deployment in industrial applications are also discussed. Lastly, challenges and future trends associated with object detection using deep learning are summarized with potential research on improving object detection for smart industrial manufacturing.",Computer vision; Deep learning; Inspection; Object detection; Surveillance
"With the rapid development of the Internet, various electronic products based on computer vision play an increasingly important role in people's daily lives. As one of the important topics of computer vision, human action recognition has become the main research hotspot in this field in recent years. The human motion recognition algorithm based on the convolutional neural network can realize the automatic extraction and learning of human motion features and achieve good classification performance. However, deep convolutional neural networks usually have a large number of layers, a large number of parameters, and a large memory footprint, while embedded wearable devices have limited memory space. Based on the traditional cross-entropy error-based training mode, the parameters of all hidden layers must be kept in memory and cannot be released until the end of forward and reverse error propagation. As a result, the memory used to store the parameters of the hidden layer cannot be released and reused, and the memory utilization efficiency is low, which leads to the backhaul locking problem, limiting the deployment and execution of deep convolutional neural networks on wearable sensor devices. Based on this, this topic designs a local error convolutional neural network model for human motion recognition tasks. Compared with the traditional global error, the local error constructed in this paper can train the convolutional neural network layer by layer, and the parameters of each layer can be trained independently according to the local error and does not depend on the gradient propagation of adjacent upper and lower layers. As a result, the memory used to store all hidden layer parameters can be released in advance without waiting for the end of forward and backward propagation, avoiding the problem of backhaul locking, and improving the memory utilization of convolutional neural networks deployed on embedded wearable devices.",
"Glaucoma is one of the most common chronic diseases that may lead to irreversible vision loss. The number of patients with permanent vision loss due to glaucoma is expected to increase at an alarming rate in the near future. A considerable amount of research is being conducted on computer-aided diagnosis for glaucoma. Segmentation of the optic cup (OC) and optic disc (OD) is usually performed to distinguish glaucomatous and nonglaucomatous cases in retinal fundus images. However, the OC boundaries are quite non-distinctive; consequently, the accurate segmentation of the OC is substantially challenging, and the OD segmentation performance also needs to be improved. To overcome this problem, we propose two networks, separable linked segmentation network (SLS-Net) and separable linked segmentation residual network (SLSR-Net), for accurate pixel-wise segmentation of the OC and OD. In SLS-Net and SLSR-Net, a large final feature map can be maintained in our networks, which enhances the OC and OD segmentation performance by minimizing the spatial information loss. SLSR-Net employs external residual connections for feature empowerment. Both proposed networks comprise a separable convolutional link to enhance computational efficiency and reduce the cost of network. Even with a few trainable parameters, the proposed architecture is capable of providing high segmentation accuracy. The segmentation performances of the proposed networks were evaluated on four publicly available retinal fundus image datasets: Drishti-GS, REFUGE, Rim-One-r3, and Drions-DB which confirmed that our networks outperformed the state-of-the-art segmentation architectures.",Artificial intelligence; Optic cup and optic disc segmentation; Glaucoma screening; Computer -aided diagnosis; SLS-Net and SLSR-Net
"A novel automated image processing-based methodology is proposed for quantification of stiffness degradation in non-ductile reinforced concrete moment frames after a seismic event. A database of 264 surface crack patterns from quasi-static experiments on 61 non-ductile beam-column subassemblies at various damage levels is used for development and verification of the methodology. The reference databank includes a wide range of structural and geometric parameters. Multifractal dimensions of the images of non-ductile beam-column joints are considered as the mathematical complexity indices of the surface crack patterns. Five predictive equations are developed for estimating the updated stiffness of damaged non-ductile reinforced concrete moment frames following an earthquake. The equations are obtained using symbolic regression method and their input parameters vary based on the accessibility of the characteristic parameters of the beam-column joint. The effectiveness of the proposed empirical equations is shown for a sample specimen at a variety of damage levels. Results reveal that the multifractal dimensions of the surface crack maps are highly correlated with the stiffness loss in the non-ductile reinforced concrete beam-column joints. The stiffness based damage index obtained by the proposed predictive equations can be used for post-earthquake system identification, stability assessment, or subsequent seismic analysis of the damaged structure.",Structural health monitoring; Secant stiffness based damage index; Computer vision; Non-ductile reinforced concrete moment frame; Surface crack patterns; Multifractal dimensions; Stiffness degradation
"Dashboards are increasingly being used by organizations to process and visualize complex information and to reduce data complexity for planning and reporting. Their main function is to synthesize information for rapid and smarter decision making. This paper details the development of an innovative interactive dashboard to process connected vehicle (CV) data securely, report CV application evaluation analytics, and monitor CV infrastructure system performance. We detail the methods used to process complex, high-frequency (up to 10 Hz) information generated by more than 1,000 participants' vehicles in the Tampa Hillsborough Expressway Authority (THEA) CV Pilot deployment over the course of two and half years of operation. The dashboard provides advanced query capabilities and custom visualization via interactive maps, graphics, and dynamic reporting to help inform decision making and securely share information. Further, individual CV application warnings can be analyzed using the warning profile feature that is equipped with visual animation replay. This novel approach not only compensated for the lack of in-vehicle dashboard cameras integrated into the CV Pilot infrastructure to assess behavioral responses to the human-machine interface, but also sped up manual validation time. The dashboard allows different levels of user access with customized views to meet a variety of stakeholder types and needs. This tool has been successfully used by the THEA CV Pilot evaluation team and U.S. Department of Transportation independent evaluators to perform precursory evaluations and to automate and perform false positive assessments of vehicle-to-vehicle and vehicle-to-infrastructure safety and mobility applications.",analytic data visualization; data and data science; data for decision making; data visualization; executive management issues; interactive visualization; policy and organization; reporting; visualization; and dashboards; transportation asset management; visualization in transportation
"Transformer has shown its effectiveness and advantage in many computer vision tasks, for example, image classification and object re-identification (ReID). However, existing vision transformers are stacked layer by layer, lacking direct information exchange among every layer. Inspired by DenseNet, we propose a dense transformer framework (termed Denseformer) that connects each layer to every other layer through class tokens. We demonstrate that Denseformer can consistently achieve better performance on person ReID tasks across datasets (Market-1501, DukeMTMC, MSMT17, and Occluded-Duke), only at a negligible increase of computation. We show that Denseformer has several compelling advantages: it pays more attention to the main parts of human bodies and obtains discriminative global features.",
"Currently, computer vision technology has been applied to detect and recognize pests for integrated pest management (IPM). Recent studies have shown that the accuracy of pest detection and recognition has been rapidly improved with the development of deep learning. However, complex backgrounds, various poses, and different scales among insect species in the field will aggravate the difficulty of pest detection. To address the pest detection and recognition problem in wild field, in this paper, we firstly devise a novel automatic data augmentation method to search for the appropriate augmentation strategy adaptively and model data more effectively. Secondly, Res2Net is used as backbone for obtaining richer detailed information of small pest, and a reverse feature fusion layer is introduced into feature pyramid networks (FPN) to learn more details. During network training, the CIoU bounding box regression loss function and cross entropy loss after label smoothing are introduced for accurate localization and recognition of small pests. When testing, the test time augmentation (TTA) strategy is used to further improve pest detection performance and reduce the probability of missing detection by inferring pest images at different scales. We evaluate the performance of our method on the pest dataset including 4 k images and 4 classes (wheat sawfly, wheat aphid, wheat mite and rice planthopper). Our method achieves the pest detection performance of 81.0% mean Average Precision (mAP), which improves 5.7%, 4.0% and 3.1% compared to three state-of-the-art approaches YOLOv4, Faster R-CNN, and Cascade R-CNN detectors, respectively.",Deep learning; Pest detection and recognition; Data augmentation; Computer vision
"With numerous countermeasures, the number of deaths in the construction industry is still higher compared to other industries. Personal Protective Equipment (PPE) is constantly being improved to avoid these accidents, although workers intentionally or unintentionally forget to use such safety measures. It is challenging to manually run a safety check as the number of co-workers on a site can be large; however, it is a prime duty of the authority to provide maximum protection to the workers on the working site. From these motivations, we have created a computer vision (CV) based automatic PPE detection system that detects various types of PPE. This study also created a novel dataset named CHVG (four colored hardhats, vest, safety glass) containing eight different classes, including four colored hardhats, vest, safety glass, person body, and person head. The dataset contains 1,699 images and corresponding annotations of these eight classes. For the detection algorithm, this study has used the You Only Look Once (YOLO) family???s anchor-free architecture, YOLOX, which yields better performance than the other object detection models within a satisfactory time interval. Moreover, this study found that the YOLOX-m model yields the highest mean average precision (mAP) than the other three versions of the YOLOX.",PPE detection; YOLOX; Image dataset; Construction safety; Deep learning; Anchor-free
"Medical image automatic segmentation plays an important role in Computer-Aided Diagnosis system. Although convolution-based network has achieved great performance in medical image segmentation, it has limitations in modeling long-range contextual interactions and spatial dependencies. Due to the powerful ability of long-range information interaction of Vision Transformer, Vision Transformer have achieved advanced performance in several downstream tasks via self-supervised learning. In this paper, motivative by Swin Transformer, we proposed BTSwin-Unet, which is a 3D U-shaped symmetrical Swin Transformer-based network for brain tumor segmentation. Moreover, we construct a self-supervised learning framework to pre-train the model encoder through the reconstruction task. Extensive experiments on tumor segmentation tasks validated the performance of our proposed model, and our results consistently demonstrate favorable benchmarks.",3D Swin Transformer; Unet; Tumor segmentation; Self-supervised learning
"Combining computer vision technology with process design, a new design and production method is obtained, which breaks through the limitations of traditional jewelry creation and provides new possibilities for the realization of complex jewelry structures. When technology no longer becomes the bottleneck of artistic expression, the space of art will be greatly expanded. Science and technology leading design method has become a new way to assist jewelry artists in subjective creation. According to various thoughts and ideas in design, establishing the corresponding algorithm rules and parameters can generate the scheme through calculation. The design result obtained in this way not only has a scientifically logical basis but also obtains the result beyond the normal imagination space due to the intelligent design process. This paper tries to apply computer vision technology to modern jewelry design, analyzes several aspects of computer vision application in process design, and combines the latest technical means to put forward algorithms for verification. The results prove that computer vision can improve the efficiency of crafts design significantly.",
"With the wide application of deep learning in the field of computer vision, the technology of object detection continues to make breakthroughs, and the bounding box regression technology is closely related to the accuracy of object detection results. This study proposes an Absolute size IoU (AIoU) loss function for bounding box regression, which further improves the object detection accuracy. Firstly, this study intro-duces the common location loss functions in bounding box regression, and then describes the limitations of common loss functions based on Intersection over Union (IoU). To overcome these limitations, this study puts forward an AIoU loss function, which can facilitate bounding box regression. Specifically, when the loss penalty term becomes invalid, it can replace the existing penalty term for model optimization. In addition, it can focus the models further on the difficult objects during training. Moreover, as a comprehensive regression factor, this penalty term contains various optimization features. The effective-ness and wide range of application of the AIoU proposed are demonstrated in experiments with three different detectors. It improves the performance of YOLOv4 by 0.61% mAP on the VOC dataset and by 1.98% mAP on the COCO dataset. Finally, we have obtained a-AIoU which uses a power function for AIoU improvement, and it achieves the best performance in the experiments. The evaluation results on several different detectors show that the method proposed in this study has important application significance for object detection technology. (C) 2022 Elsevier B.V. All rights reserved.",Object detection; Loss function; Deep learning; Computer vision
"Image colorization, as an essential problem in computer vision (CV), has attracted an increasing amount of researchers attention in recent years, especially deep learning-based image colorization techniques(DLIC). Generally, most recent image colorization methods can be regarded as knowledge-based systems because they are usually trained by big datasets. Unlike the existing reviews, this paper adopts a unique deep learning-based perspective to review the latest progress in image colorization techniques systematically and comprehensively. In this paper, a comprehensive review of recent DLIC approaches from algorithm classification to existing challenges is provided to facilitate researchers' in-depth understanding of DLIC. In particular, we review DLIC algorithms from various perspectives, including color space, network structure, loss function, level of automation, and application fields. Furthermore, other important issues are discussed, such as publicly available benchmark datasets and performance evaluation metrics. Finally, we discuss several open issues of image colorization and outline future research directions. This survey can serve as a reference for researchers in image colorization and related fields.",Image colorization; Deep learning; Convolutional neural network; Generative adversarial network; Transformer
"Human Action Recognition (HAR) is a challenging task used in sports such as volleyball, basketball, soccer, and tennis to detect players and recognize their actions and teams' activities during training, matches, warm-ups, or competitions. HAR aims to detect the person performing the action on an unknown video sequence, determine the action's duration, and identify the action type. The main idea of HAR in sports is to monitor a player's performance, that is, to detect the player, track their movements, recognize the performed action, compare various actions, compare different kinds and skills of acting performances, or make automatic statistical analysis.As an action that can occur in the sports field refers to a set of physical movements performed by a player in order to complete a task using their body or interacting with objects or other persons, actions can be of different complexity. Because of that, a novel systematization of actions based on complexity and level of performance and interactions is proposed.The overview of HAR research focuses on various methods performed on publicly available datasets, including actions of everyday activities. That is just a good starting point; however, HAR is increasingly represented in sports and is becoming more directed towards recognizing similar actions of a particular sports domain. Therefore, this paper presents an overview of HAR applications in sports primarily based on Computer Vision as the main contribution, along with popular publicly available datasets for this purpose.",Machine learning; Human Action Recognition; Action systematization; Sports dataset; Human action recognition in sports; Sport
"Traffic sign recognition is one of the most important tasks in autonomous driving. Camera-based computer vision techniques have been proposed for this task, and various convolutional neural network structures are used and validated with multiple open datasets. Recently, novel Transformer-based models have been proposed for various computer vision tasks and have achieved state-of-the-art performance, outperforming convolutional neural networks in several tasks. In this study, our goal is to investigate whether the success of Vision Transformers can be replicated within the traffic sign recognition area. Based on existing resources, we first extract and contribute three open traffic sign classification datasets. Based on these datasets, we experiment with seven convolutional neural networks and five Vision Transformers. We find that Transformers are not as competitive as convolutional neural networks for the traffic sign classification task. Specifically, there are performance gaps of up to 12.81%, 2.01%, and 4.37% existing for the German, Indian, and Chinese traffic sign datasets, respectively. Furthermore, we propose some suggestions to improve the performance of Transformers.",
"Micro-expression recognition (MER) is an interdisciplinary research task that has attracted attention. This is because MER can be relevant to multiple fields, such as computer vision, psychology, human-computer interaction, and social security. Because the scarcity of databases and difficulty in video semantics understanding, end-to-end MER still faces many challenges. In this study, we propose an MER framework with attention mechanism and region enhancement (MER-AMRE). Attention mechanisms are introduced to enhance the representation performance of the model, which can improve the recognition accuracy. Additionally, we use Euler video magnification in data preprocessing to enhance facial variation areas. AffectNet is leveraged to pretrain a facial region of interest (RoI) feature extractor with attention regions. Finally, we combine the facial RoI features with global facial features to recognize micro-expressions. Extensive experiments on two well-known micro-expression datasets, CASME II and SAMM, verified the robustness and generalization of the proposed MER-AMRE framework.",Micro-expression recognition; Attention mechanism; Euler video magnification; Joint loss optimization
"In the last several years, computer vision tasks involving visual identification and tracking have seen a rise in the usage of deep learning technologies in recent years. An extremely difficult but rewarding endeavor is identifying and following football players' targets. This may be used to study football tactical visualization. Due to the similar appearance and frequent occlusion of targets in football video, traditional methods often can only segment targets such as players and balls in the image but cannot track them or can only track them for a short time. Based on the related research of computer vision and deep learning, using several cameras, this study develops a system that can properly monitor many targets in a football stadium for a lengthy period of time. The main research contents of this paper are as follows: (1) a CNN for target displacement prediction is proposed, which no longer relies on the previous linear motion model or quadratic motion model, so that the multitarget tracking algorithm can be applied to more scenes. (2) For the first time in a multitarget tracking algorithm, a continuous conditional random field is used to model the asymmetric nature of the target relationship. At the same time, the CNN for target displacement prediction can be cascaded with the continuous conditional random field for end-to-end training, which greatly reduces the training difficulty. The parameters of the experiment in this paper are simple, and comprehensive and systematic experiments verify the validity and correctness of this work from different aspects.",
"Featured Application This work has applied computer vision and deep learning technology to develop a real-time weapon detector system and tested it on different computing devices for large-scale deployment. Weapon detection in CCTV camera surveillance videos is a challenging task and its importance is increasing because of the availability and easy access of weapons in the market. This becomes a big problem when weapons go into the wrong hands and are often misused. Advances in computer vision and object detection are enabling us to detect weapons in live videos without human intervention and, in turn, intelligent decisions can be made to protect people from dangerous situations. In this article, we have developed and presented an improved real-time weapon detection system that shows a higher mean average precision (mAP) score and better inference time performance compared to the previously proposed approaches in the literature. Using a custom weapons dataset, we implemented a state-of-the-art Scaled-YOLOv4 model that resulted in a 92.1 mAP score and frames per second (FPS) of 85.7 on a high-performance GPU (RTX 2080TI). Furthermore, to achieve the benefits of lower latency, higher throughput, and improved privacy, we optimized our model for implementation on a popular edge-computing device (Jetson Nano GPU) with the TensorRT network optimizer. We have also performed a comparative analysis of the previous weapon detector with our presented model using different CPU and GPU machines that fulfill the purpose of this work, making the selection of model and computing device easier for the users for deployment in a real-time scenario. The analysis shows that our presented models result in improved mAP scores on high-performance GPUs (such as RTX 2080TI), as well as on low-cost edge computing GPUs (such as Jetson Nano) for weapon detection in live CCTV camera surveillance videos.",weapon detection; object detection; deep learning; optimization; computer vision
"Semantic segmentation using machine learning and computer vision techniques is one of the most popular topics in autonomous driving-related research. With the revolution of deep learning, the need for more efficient and accurate segmentation systems has increased. This paper presents a detailed review of deep learning-based frameworks used for semantic segmentation of road scenes, highlighting their architectures and tasks. It also discusses well-known standard datasets that evaluate semantic segmentation systems in addition to new datasets in the field. To overcome a lack of enough data required for the training process, data augmentation techniques and their experimental results are reviewed. Moreover, domain adaptation methods that have been deployed to transfer knowledge between different domains in order to reduce the domain gap are presented. Finally, this paper provides quantitative analysis and performance evaluation and discusses the results of different frameworks on the reviewed datasets and highlights future research directions in the field of semantic segmentation using deep learning.",deep learning; semantic segmentation; road scenes
"Low-light images are obtained in dark environments or in environments where there is insufficient light. Because of this, low-light images have low intensity values and dimmed features, making it difficult to directly apply computer vision or image recognition software to them. Therefore, to use computer vision processing on low-light images, an image improvement procedure is needed. There have been many studies on how to enhance low-light images. However, some of the existing methods create artifact and distortion effects in the resulting images. To improve low-light images, their contrast should be stretched naturally according to their features. This paper proposes the use of a low-light image enhancement method utilizing an image-adaptive mask that is composed of an image-adaptive ellipse. As a result, the low-light regions of the image are stretched and the bright regions are enhanced in a way that appears natural by an image-adaptive mask. Moreover, images that have been enhanced using the proposed method are color balanced, as this method has a color compensation effect due to the use of an image-adaptive mask. As a result, the improved image can better reflect the image's subject, such as a sunset, and appears natural. However, when low-light images are stretched, the noise elements are also enhanced, causing part of the enhanced image to look dim and hazy. To tackle this issue, this paper proposes the use of guided image filtering based on using triple terms for the image-adaptive value. Images enhanced by the proposed method look natural and are objectively superior to those enhanced via other state-of-the-art methods.",low-light image enhancement; adaptive ellipse; image-adaptive mask
"The balance between production volume and flexibility is an essential element in modern industry. CNC machining centers present high flexibility. However, machine idle time must be reduced due to the high acquisition costs of the machine tools. This work details the development of a computer vision-based system to automate the part referencing operation in CNC machining centers using low-cost optical equipment. The developed system was composed of an off-the-shelf hobby camera with a wide-angle lens, whose distortion was corrected through calibration, and computational routines that identify the part position and orientation and apply the results to the NC code, thus correcting the tool path before machining. The feasibility of the proposed system was investigated by experimental validation, where the linear and angular deviations were evaluated. A significant influence of the part orientation on the system accuracy was observed: accuracy improved as the angular position of the part departs from the machine tool coordinate system, with a minimum error for parts oriented at - 45 degrees and a maximum error for parts aligned with the worktable. The results indicate that the developed system presents accuracy comparable with other studies, which, however, is not sufficient for finishing operations in high production systems. However, since the developed system corrects both position and orientation of the produced parts, it may be suited for the production of engineered parts and prototypes in small and medium industries.",Computer vision; Feature extraction; CNC machining; Machine setup; Image processing
"Over the last decade, the Advanced Driver Assistance System (ADAS) concept has evolved significantly. ADAS involves several technologies such as automotive electronics, vehicle-to-vehicle (V2V) vehicle-to-infrastructure (V2I) communication, RADAR, LIDAR, computer vision, and machine learning. Of these, computer vision and machine learning based solutions have mainly been effective that have allowed real-time vehicle control, driver aided systems, etc. However, most of the existing works deal with the deployment of ADAS and autonomous driving functionality in countries with well-disciplined lane traffic. Nevertheless, these solutions and frameworks do not work in countries and cities with less-disciplined/chaotic traffic. This paper identifies the research gaps, reviews the state-of-the-art looking at the different functionalities of ADAS and its levels of autonomy. Importantly, it provides a detailed description of vision intelligence and computational intelligence for ADAS. The eye-gaze and head pose estimation in vision intelligence is detailed. Notably, the learning algorithms such as supervised, unsupervised, reinforcement learning and deep learning solutions for ADAS are considered and discussed. Significantly, this would enable developing a real-time recommendation system for system-assisted/autonomous vehicular environments with less-disciplined road traffic.",Sensors; Roads; Vehicles; Cameras; Laser radar; Safety; Radar; Autonomous driving; computer vision; intelligent transportation; machine learning; multi-sensor
"One of the most challenging technical implementations of today is self-driving vehicles. An important segment of self-driving is the ability of the computer to see/detect objects of interest at a distance which enables safe vehicle operation. An algorithm for the detection of railway infrastructure objects, namely, track and signals, is proposed in this paper to enable detection of signals which are relevant for the track the train is moving along. The algorithm integrates traditional computer vision (CV) algorithms, including Canny edge detection, Hough transform, and You Only Look Once (YOLO) algorithm, based on convolutional neural networks (CNNs). Each of the concepts (CV and CNNs) deals with a different object of detection which together form a unique system that aims to detect both the rails and the relevant signals. This approach ensures that the artificial intelligence (AI) system is aware of which route the signal belongs to. The reliability of the proposed algorithm in detection of a relevant signal, verified by the performed tests, is up to 99.7%. The metric method used for validation was intersection over union (IoU). The obtained value of IoU applied on the entire validation dataset exceeds 0.7. Calculated values of average precision and recall were 0.89 and 0.76, respectively. The algorithm created in this way solves the problem of detection of relevant signals along the train route, especially in multitrack scenarios such as stations and yards.",railway track; railway signals; You Only Look Once (YOLO); convolutional neural networks (CNNs); object detection; computer vision; Canny edge detection; Hough transform
"We present SfSNet, an end-to-end learning framework for producing an accurate decomposition of an unconstrained human face image into shape, reflectance and illuminance. SfSNet is designed to reflect a physical lambertian rendering model. SfSNet learns from a mixture of labeled synthetic and unlabeled real-world images. This allows the network to capture low-frequency variations from synthetic and high-frequency details from real images through the photometric reconstruction loss. SfSNet consists of a new decomposition architecture with residual blocks that learns a complete separation of albedo and normal. This is used along with the original image to predict lighting. SfSNet produces significantly better quantitative and qualitative results than state-of-the-art methods for inverse rendering and independent normal and illumination estimation. We also introduce a companion network, SfSMesh, that utilizes normals estimated by SfSNet to reconstruct a 3D face mesh. We demonstrate that SfSMesh produces face meshes with greater accuracy than state-of-the-art methods on real-world images.",Faces; Lighting; Image reconstruction; Shape; Rendering (computer graphics); Three-dimensional displays; Training; Vision and scene understanding; physically based modeling; shading; 3D; stereo scene analysis; computer vision; reflectance; shading; shape
"Lung cancer is the most significant cancer that heavily contributes to cancer-related mortality rate, due to its violent nature and late diagnosis at advanced stages. Early identification of lung cancer is essential for improving the survival rate. Various imaging modalities, including X-rays and computed tomography (CT) scans, are employed to diagnose lung cancer. Computer-aided diagnosis (CAD) models are necessary for minimizing the burden upon radiologists and enhancing detection efficiency. Currently, computer vision (CV) and deep learning (DL) models are employed to detect and classify the lung cancer in a precise manner. In this background, the current study presents a cat swarm optimization-based computer-aided diagnosis model for lung cancer classification (CSO-CADLCC) model. The proposed CHO-CADLCC technique initially pre-process the data using the Gabor filtering-based noise removal technique. Furthermore, feature extraction of the pre-processed images is performed with the help of NASNetLarge model. This model is followed by the CSO algorithm with weighted extreme learning machine (WELM) model, which is exploited for lung nodule classification. Finally, the CSO algorithm is utilized for optimal parameter tuning of the WELM model, resulting in an improved classification performance. The experimental validation of the proposed CSO-CADLCC technique was conducted against a benchmark dataset, and the results were assessed under several aspects. The experimental outcomes established the promising performance of the CSO-CADLCC approach over recent approaches under different measures.",computer-aided diagnosis; deep learning; intelligent models; healthcare; cat swarm optimization; computer vision
"Eye gaze is an important natural behavior in social interaction as it delivers complex exchanges between observer and observed, by building up the geometric constraints and relation of the exchanges. These interperson exchanges can be modeled based on gaze direction estimated using computer vision. Despite significant progresses in vision-based gaze estimation in last 10 years, it is still nontrivial since the accuracy of gaze estimation is significantly affected by such intrinsic factors as head pose variance, individual bias between optical axis and visual axis, eye blink, occlusion and image blur, degrade gaze features, lead to inaccurate gaze-involved human social interaction analysis. This article aims to review and discuss existing methods addressing above-mentioned problems, gaze involved applications and data sets against the state of the arts in vision-based gaze estimation. It also points out future research directions and challenges of gaze estimation in terms of metalearning, causal inference, disentangled representation, and social gaze behavior for unconstrained gaze estimation.",Estimation; Feature extraction; Faces; Three-dimensional displays; Solid modeling; Iris; Visualization; 3-D gaze estimation; computer vision; head pose; optical axis; visual axis
"Efficient learning of 3D shape representation from point cloud is one of the biggest requirements in 3D computer vision. In recent years, convolutional neural networks have achieved great success in 2D image representation learning. However, unlike images that have a Euclidean structure, 3D point clouds are irregular since the neighbors of each node are inconsistent. Many studies have tried to develop various convolutional graph neural networks to overcome this problem and to achieve great results. Nevertheless, these studies simply took the centroid point and its corresponding neighbors as the graph structure, thus ignoring the structural information. In this paper, an Affinity-Point Graph Convolutional Network (AP-GCN) is proposed to learn the graph structure for each reference point. In this method, the affinity between points is first defined using the feature of each point feature. Then, a graph with affinity information is built. After that, the edge-conditioned convolution is performed between the graph vertices and edges to obtain stronger neighborhood information. Finally, the learned information is used for recognition and segmentation tasks. Comprehensive experiments demonstrate that AP-GCN learned much more reasonable features and achieved significant improvements in 3D computer vision tasks such as object classification and segmentation.",3D point cloud analysis; deep learning; graph convolution network; 3D classification; semantic segmentation
"Railway networks systems are by design open and accessible to people, but this presents challenges in the prevention of events such as terrorism, trespass, and suicide fatalities. With the rapid advancement of machine learning, numerous computer vision methods have been developed in closed-circuit television (CCTV) surveillance systems for the purposes of managing public spaces. These methods are built based on multiple types of sensors and are designed to automatically detect static objects and unexpected events, monitor people, and prevent potential dangers. This survey focuses on recently developed CCTV surveillance methods for rail networks, discusses the challenges they face, their advantages and disadvantages and a vision for future railway surveillance systems. State-of-the-art methods for object detection and behaviour recognition applied to rail network surveillance systems are introduced, and the ethics of handling personal data and the use of automated systems are also considered.",surveillance; rail network systems; image and video analytics; computer vision; machine learning; sensors; video anomaly detection
"Single image haze removal, which is to recover the clear version of a hazy image, is a challenging task in computer vision. In this paper, an additive haze model is proposed to approximate the hazy image formation process. In contrast with the traditional optical model, it regards the haze as an additive layer to a clean image. The model thus avoids estimating the medium transmission rate and the global atmospherical light. In addition, based on a critical observation that haze changes gradually and smoothly across the image, a haze smoothness prior is proposed to constrain this model. This prior assumes that the haze layer is much smoother than the clear image. Benefiting from this prior, we can directly separate the clean image from a single hazy image. Experimental results and comparisons with synthetic images and real-world images demonstrate that the proposed method outperforms state-of-the-art single image haze removal algorithms.",Atmospheric modeling; Optical imaging; Optical scattering; Computational modeling; Image color analysis; Additives; Optical computing; Single image haze removal; addition model; haze smoothness prior
"Occluded person re-identification is one of the challenging areas of computer vision, which faces problems such as inefficient feature representation and low recognition accuracy. Recently, vision transformer is introduced into the field of re-identification and achieved state-of-the-art results by constructing global feature relationships between patch sequences. However, vision transformer is not good at capturing short-range correlations of patch sequence and exploiting spatial correlation in patch sequence, which leads to a decrease in the accuracy and robustness of the network in the face of occluded person re-identification. Therefore, to address the above problems, we design a partial feature transformer-based occluded person re-identification framework named PFT. The proposed PFT utilizes three modules to enhance the efficiency of vision transformer. (1) Patch full dimension enhancement module. We design a learnable tensor with the same size as patch sequences, which is full-dimensional and deeply embedded in patch sequences to enrich the diversity of training samples. (2) Fusion and reconstruction module. We extract the less important part of obtained patch sequences, and fuse them with original patch sequence to reconstruct the original patch sequences. (3) Spatial Slicing Module. We slice and group patch sequences from spatial direction, which can effectively improve the short-range correlation of patch sequences. Experimental results over occluded and holistic re-identification datasets demonstrate that the proposed PFT network achieves superior performance consistently and outperforms the state-of-the-art methods.",
"Existing deep-learning tools for road network generation have limited applications in flat urban areas due to their overreliance on the geometric and spatial configurations of street networks and inadequate considerations of topographic information. This paper proposes a new method of street network generation based on a generative adversarial network by designing a pre-positioned geo-extractor module and a geo-merging bypath. The two improvements employ the complementary use of geometric configurations and topographic features to automate street network generation in both flat and hilly urban areas. Our experiments demonstrate that the improved model yields a more realistic prediction of street configurations than conventional image inpainting techniques. The model's effectiveness is further enhanced when generating streets in hilly areas. Furthermore, the geo-extractor module provides insights from the computer vision perspective in recognizing when topographic information should be considered and which topographic information should receive more attention.",Street network generation; topographic information; computer vision; generative adversarial network; planning support systems
"Computer Vision-based smart surveillance systems are needed in the present era that can analyse crowd events for behaviour assessment, activity and event recognition, anomaly detection and recognition, crowd density estimation, and counting etc. Even with the human resource available for surveillance of an event, any turn of events can convert a peaceful crowd to a violent one which can cause causalities in no time. Therefore, smart systems need to be introduced which recognizes the behaviour of the crowd and inform the officials beforehand. However, datasets related to the above-mentioned problems are diversely classified. Thus, a need was felt to organize crowd datasets available on the web on their crowd definition, applications, methodologies, and metadata. This paper attempts to do this and gives a comprehensive survey of online publicly available datasets for studying crowd dynamics. It was also observed that available datasets do not cover several important natural events like gate entry and exit surveillance, exit events after religious rituals, and violent activities etc. Some of such events play a crucial role in defining abnormal behaviour. Furthermore, the number of crowd events in some of the available datasets are quite a few and are simulated. To overcome the limitations of the existing datasets, a crowd dataset, named CRUETPAK (CRowd UET PAKistan) is presented. The dataset includes video clips of group and crowd activities related to surveillance, sports, dining, education, and various human interactions (surpassing counts and realism of existing datasets).",Crowd datasets; Crowd analysis; Computer vision; Machine learning
"Machine vision is an important branch of the rapid development of modern artificial intelligence, and it is a key technology to convert the image information of monitoring targets into digital signals. However, due to the wide range of machine vision applications, this research focuses on its application in video surveillance. In the era of artificial intelligence, the detection and tracking of moving objects have always been a key issue in video surveillance. The simulation of human vision is realized by combining the relevant functions of the computer and the image acquisition device, which enables the computer to have the ability to recognize the surrounding environment through images. The intelligent video analysis technology can automatically analyze and extract the key useful information from the video source with the powerful data processing ability of the computer, so as to realize the computer's understanding of the video. It allows the computer to understand what is shown in the video or what kind of event happened and provides a new method and reliable basis for accident detection and accident analysis. Therefore, after a brief introduction to machine vision, moving target monitoring methods, and intelligent tracking algorithms, this paper will focus on moving target monitoring and intelligent tracking strategies for video surveillance. In addition, this paper will focus on introducing the principle of intelligent tracking algorithm through formulas and compare the accuracy and success rate of target monitoring and intelligent tracking between the machine vision-based algorithm and other algorithms during the experiment. Finally, experiments show that the monitoring and tracking effect of machine vision combined with cloud is the best, and the overall average can reach 85.7%. Based on this, this paper fully confirms the feasibility of the moving target monitoring and intelligent tracking algorithm based on machine vision.",
"Graph Convolutional Network (GCN) which models the potential relationship between non-Euclidean spatial data has attracted researchers' attention in deep learning in recent years. It has been widely used in different computer vision tasks by modeling the latent space, topology, semantics, and other information in Euclidean spatial data and has achieved significant success. To better understand the work principles and future GCN applications in the computer vision field, this study reviewed the basic principles of GCN, summarized the difficulties and solutions using GCN in different visual tasks, and introduced in detail the methods for constructing graphs from the Euclidean spatial data in different visual tasks. At the same time, the review divided the application of GCN in basic visual tasks into image recognition, object detection, semantic segmentation, instance segmentation and object tracking. The role and performance of GCN in basic visual tasks were summarized and compared in detail for different tasks. This review emphasizes that the application of GCN in computer vision faces three challenges: computational complexity, the paradigm of constructing graphs from the Euclidean spatial data, and the interpretability of the model. Finally, this review proposes two future trends of GCN in the vision field, namely model lightweight and fusing GCN with other models to improve the performance of the visual model and meet the higher requirements of vision tasks.",Graph convolution network; Non-Euclidean space; Relational modeling; Computer vision
"This paper presents alternative solutions for classifying concrete spall severity based on computer vision approaches. Extreme Gradient Boosting Machine (XGBoost) and Deep Convolutional Neural Network (DCNN) are employed for categorizing image samples into two classes: shallow spall and deep spall. To delineate the properties of a concrete surface subject to spall, texture descriptors including local binary pattern, center symmetric local binary pattern, local ternary pattern, and attractive repulsive center symmetric local binary pattern (ARCS-LBP) are employed as feature extraction methods. In addition, the prediction performance of XGBoost is enhanced by Aquila optimizer metaheuristic. Meanwhile, DCNN is capable of performing image classification directly without the need for texture descriptors. Experimental results with a dataset containing real-world concrete surface images and 20 independent model evaluations point out that the XGBoost optimized by the Aquila metaheuristic and used with ARCS-LBP has achieved an outstanding classification performance with a classification accuracy rate of roughly 99%.",Concrete spall severity; Gradient boosting machine; Local binary pattern; Metaheuristic; Deep learning
"The performance of image classification technology based on deep network has been greatly improved, making computer vision enter the stage of industrialization and be gradually applied to many aspects of human work and life. As a typical classification task in computer vision, human behavior recognition has immeasurable potential value in medical, family, transportation, and other scenarios. At the same time, in the field of competitive sports, the integration of artificial intelligence technology and sports technical and tactical analysis is undoubtedly an important way to innovate and improve the technical and tactical level. Taking karate as an example, the study of athletes' training and competition videos is an important means and method for technical and tactical analysis in competitive sports. Traditional tactical intelligence analysis methods have many shortcomings, such as high labor cost, serious data loss, long delay, and low accuracy. Therefore, based on the convolutional neural network, this paper establishes a new graph convolution model for automatic intelligent analysis of karate athletes' technical action recognition, action frequency statistics, and trajectory tracking. The technology effectively makes up for the disadvantages of traditional tactical intelligence analysis methods. The research results show that the new topology map construction method has a significant effect on improving the accuracy of behavior recognition and also lays a foundation for technical and tactical analysis.",
"The functional performance of concrete structures degrades over time as a result of continuous loads, stress fatigue, and external environmental changes. Thus, periodic diagnoses and inspections are essential because such conditions can eventually lead to disaster. Hence, the detection of cracks in concrete is a key component of structural management. In recent years, deep-learning-based computer vision technologies have emerged as a promising trend and have been actively used for crack detection. Unfortunately, the performance of existing crack detection technologies decreases under environmental conditions that vary widely. To resolve this issue, we propose a new deep neural network that applies an optimal mixing ratio of training data to improve recognition performance alongside an adversarial learning-based balanced ensemble discriminator network. Furthermore, a method to reconstruct the 3-dimensional shape of cracks is proposed using a stereo-vision-based triangulation measurement technique that determines the size of detected cracks. Experimental results show that the proposed algorithm achieved a crack detection performance with a mean intersection-over-union of 84.53% and an F1 score of 82.91%. The proposed inspection technology for concrete structures is expected to be implemented in the future in connection with various automation techniques.",Adversarial learning; crack detection; ensemble discriminator; semantic segmentation; stereo vision
"Machine learning has become the state-of-the-art technique for many tasks including computer vision, natural language processing, speech processing tasks, etc. However, the unique challenges posed by machine learning suggest that incorporating user knowledge into the system can be beneficial. The purpose of integrating human domain knowledge is also to promote the automation of machine learning. Human-in-the-loop is an area that we see as increasingly important in future research due to the knowledge learned by machine learning cannot win human domain knowledge. Human-in-the-loop aims to train an accurate prediction model with minimum cost by integrating human knowledge and experience. Humans can provide training data for machine learning applications and directly accomplish tasks that are hard for computers in the pipeline with the help of machine-based approaches. In this paper, we survey existing works on human-in-the-loop from a data perspective and classify them into three categories with a progressive relationship: (1) the work of improving model performance from data processing, (2) the work of improving model performance through interventional model training, and (3) the design of the system independent human-in-the-loop. Using the above categorization, we summarize the major approaches in the field; along with their technical strengths/weaknesses, we have a simple classification and discussion in natural language processing, computer vision, and others. Besides, we provide some open challenges and opportunities. This survey intends to provide a high-level summarization for human-in-the-loop and to motivate interested readers to consider approaches for designing effective human-in-the-loop solutions. Keywords: Human-in-the-loop Machine learning Deep learning Data processing Computer vision Natural language processing (C) 2022 Elsevier B.V. All rights reserved.",Human-in-the-loop; Machine learning; Deep learning; Data processing; Computer vision; Natural language processing
"Fatigue cracks caused by repetitive loads are one of the major threats to the structural integrity of civil infrastructure. Human inspection is the most common method for detecting fatigue cracks, but it is time-consuming, labor-intensive, and unreliable. In this paper, we propose a new vision-based fatigue crack detection and localization method that can detect the fatigue crack with marker-free and high precision using a consumer-grade digital camera. A motion tracking technology called optical flow algorithm is applied to the video for tracking the surface motion of the monitored structure under repetitive load. Then, a crack detection and localization algorithm based on optical flow information entropy are developed to search differential features at different video frames caused by the crack opening and closing. The proposed method's precision is first validated by doing two experiments and then comparing its precision and efficiency to the existing crack detection methods, including image processing and digital image correlation. The results show that, when compared to the existing vision-based methods, the proposed method can accurately and efficiently identify the fatigue crack even when the crack is surrounded by other crack-like edges, covered by complex surface textures, or invisible to human eyes. In addition, based on the proposed methods, a practical application for calculating the stress intensity factor is given to track crack development.",Fatigue cracks; vision-based method; crack detection; optical flow; information entropy
"Convolutional neural networks (CNNs) have been widely deployed in artificial intelligence, including computer vision and pattern recognition. In these applications, CNN is the most computationally intensive part. Recently, many researchers have used depthwise convolution to decrease the computational load in the execution of CNNs; on the other hand, today, CNNs have become larger and larger. Consequently, they need more computational budget for their executions. The problem is more serious when this application is run in an embedded system, especially in the edge devices, as the embedded processor can hardly handle these heavy computational loads. This paper proposes a lightweight, low-power, and efficient CNN hardware accelerator for edge computing devices. This accelerator is explicitly designed for depthwise CNN. The proposed accelerator can be configured and programmed to run any lightweight CNN of a wide range of AI networks such as MobileNet, Xception, and shuffleNet. Our experimental results show that our accelerator can run MobileNet 70 times per second in a remote sensing AI application with a 224 x 224 pixel image from the ImageNet dataset.",Convolutional neural networks (CNNs); Hardware accelerator; Low power; AI at edge; Vision at edge; Edge processing
"Object recognition is among the fundamental tasks in the computer vision applications, paving the path for all other image understanding operations. In every stage of progress in object recognition research, efforts have been made to collect and annotate new datasets to match the capacity of the state-of-theart algorithms. In recent years, the importance of the size and quality of datasets has been intensified as the utility of the emerging deep network techniques heavily relies on training data. Furthermore, data sets lay a fair benchmarking means for competitions and have proved instrumental to the advancements of object recognition research by providing quantifiable benchmarks for the developed models. Taking a closer look at the characteristics of commonly-used public datasets seems to be an important first step for data-driven and machine learning researchers. In this survey, we provide a detailed analysis of datasets in the highly investigated object recognition areas. More than 160 datasets have been scrutinized through statistics and descriptions. Additionally, we present an overview of the prominent object recognition benchmarks and competitions, along with a description of the metrics widely adopted for evaluation purposes in the computer vision community. All introduced datasets and challenges can be found online at github.com/AbtinDjavadifar/ORDC.(c) 2022 Elsevier B.V. All rights reserved.",Computer vision; Object recognition; Deep learning
"For the last two decades, image processing techniques have been used frequently in computer vision applications. The most challenging task in image processing is restoring images that are degraded due to various weather conditions. Mainly, the visibility of outdoor images is corrupted due to adverse atmospheric effects. The visibility of acquired images is reduced in these circumstances. Haze is an atmospheric phenomenon that reduces the clarity of an image. Due to the presence of particles such as dust, dirt, soot, or smoke, there is significant decay in the color and contrast of captured images. Haze present in acquired images causes issues in a variety of computer vision applications. Therefore, enhancing the contrast of a hazy image and restoring the visibility of the scene is essential. Since clear images are required in every application, image dehazing is an important step. Hence, many researchers are working on it. Different methods have been presented in the literature for image dehazing. This study describes various traditional and deep learning techniques of image dehazing from an analytical perspective. The main intention behind this work is to provide an intuitive understanding of the major techniques that have made a relevant contribution to haze removal. In this paper, we have covered different types of contributions toward dehazing based on the traditional method as well as deep learning approaches. With a considerable amount of instinctive simplifications, the reader is expected to have an improved ability to visualize the internal dynamics of these processes.",Atmospheric modeling; Image restoration; Scattering; Image color analysis; Meteorology; Degradation; Computer vision; Single image dehazing; contrast enhancement; image restoration; traditional methods; deep learning-based approaches; survey
"With the rapid advancement of artificial intelligence technology and the widespread use of sensing technology in education, human-computer interaction teaching has gradually developed in sports and education. Traditionally, teachers explain and demonstrate the fundamentals of movements first, then organize exercises, and students gradually consolidate technical movements through repetition. This process requires teachers to repeatedly explain, such that students can develop movement concepts, and to assist students in correcting their movements through practice. Eventually, students can master the dragon boat's paddling movements. Teachers frequently struggle to observe all of their students' movements and are therefore unable to correct them in a timely and effective manner. To address the aforementioned issues, this paper proposes a big data system for multidimensional stereo vision training in dragon boat paddling action. The stock price action recognition of dragon boat paddles and the movement of students' dragon boat paddles are realized through the multidimensional fusion of attention mechanism and spatiotemporal graph convolution. Make judgments to more effectively guide and train students' paddling movements.",
"Sign language video understanding requires capturing both spatial and temporal information in sign language video clips. We propose Lightweight Sign Transformer Framework, which is a two-stream lightweight network incorporating transformer architecture that consists of RGB flow and RGB difference. It leverages the latest advances in computer vision and natural language processing and applies them to video understanding. Then we implement video transformer network on sign language datasets and got excellent performance. Furthermore, we compare the performance of our network with I3D network (Carreira and Ziswrman in Quo Vadis, Action recognition? A new model and the kinetics dataset. IEEE) and show better performance.",
"Entire world has been affected by Covid-19 pandemic. In fighting against the Covid-19, social distancing and face mask have a paramount role in freezing the spread of the disease. People are asked to limit their interactions with each other, to reduce the spread of the disease. Here an alert system has to be maintained to caution people traveling in vehicles. Our proposed solution will work primarily on computer vision. The video stream is captured using a camera. Footage is processed using single shot detector algorithm for face mask detection. Second, YOLOv3 object detection algorithm is used to detect if social distancing is maintained or not inside the vehicle. If passengers do not follow the safety rules such as wearing a mask at any point of the time in the whole journey, alarm/alert is given via buzzer/speaker. This ensures that people abide by the safety rules without affecting their daily norms of transportation. It also helps the government to keep the situation under control.",computer vision; COVID-19; face mask; single shot detector; social distancing; YOLOv3
"Recently, pushed by the COVID-19 pandemic, the need of respecting social distancing has motivated several researchers to define novel technological solutions to monitor and track user movements. Information and Communications Technologies (ICT) world has addressed this challenge by means of the use of different technologies, such as Bluetooth, in order to track user interdistance and encounter time. Technology solutions should be able to not only track contacts, but also alert users to restore social distancing. In this article, we present IMPERSONAL framework, with the twofold aim of both: 1) tracking and monitoring social distancing and 2) alerting users in case of gatherings. The framework is based on a subnetwork of computer vision-based devices that are adopted to monitor and track users' movements to estimate their interdistance and compute the encounter time. Such information is then the input to an Internet of Things subnetwork, aiming to retrieve the anonymous IDs of people belonging to a gathering, as well as to send alert messages to them. We assess IMPERSONAL framework by means of extensive Monte Carlo simulations and experimental results, showing its effectiveness in terms of accuracy in correctly identifying users and gatherings in videos taken from live cameras, both in case of indoor and outdoor real scenarios. The benefits of the IMPERSONAL framework are expressed in terms of the ability to track people, solve gatherings, and send warning messages.",Monitoring; Social factors; Human factors; Cameras; Internet of Things; COVID-19; Pandemics; Computer vision (CV); e-health; Internet of Things (IoT) networks; social distancing
"As a basic task of computer vision task, object localization plays an important role in many computer vision based applications. Supervised methods employ manual location labels to learn to localize the objects directly, but incomplete or incorrectly assigned location labels affect localization accuracy, and the cost of manual labelling should also be extremely large. This paper proposes a weakly-supervised localization method based on a multi-scale gradient-pyramid feature, which employs the weighted gradient features on the multiple convolutional layers in order to generate a gradient-pyramid feature for object localization. Pairs of gradients and features from different layers are first extracted to compute the gradient features. Then, during the fusion of the gradient features through a pyramid model, the larger value is selected as the result of the fusion task without using the concatenated method. Finally, the multi-scale gradient-pyramid feature is obtained and used to have a more accurate object localization by using the region scaling operation. Our proposed method can be directly integrated into the pre-trained classification model to perform object localization without additional training. Experimental results on the ILSVRC 2016 dataset and CUB-200-2011 dataset show that the proposed method can achieve better object localization performance.",Weakly-supervised object localization; Multi-scale gradients and features; Pyramid model
"Computer vision-based motion target detection and tracking, which is widely used in video surveillance, human-computer interaction, range interpretation, and other fields, is one of the current research hotspots in the field of computer vision. In engineering scenarios, the two are inseparable and need to work together to accomplish specific tasks. The related research is progressing rapidly, but there is still room for improving its timeliness, accuracy, and automation. In this paper, we summarize and classify some classical target detection methods, analyze the basic principles of convolutional neural networks, and analyze the classical detection algorithms based on region suggestion and deep regression networks. After that, we improve the SSD algorithm for the shortage of low-level feature convolution layers, which has insufficient feature extraction and leads to poor detection of small targets. For the motion target tracking problem, this paper studies the motion target tracking method based on support vector machine and proposes the tracking method of support vector regression and the corresponding online support vector regression solution method based on the analysis of support vector tracking method and structural support vector tracking method. In this paper, we propose a tracking method that fuses structural support vector machines and correlation filtering. The structure is based on the idea of Inception, which adds and replaces some feature convolution layers of the original network while maintaining the original lightweight backbone. The final experiments on the VOC data set demonstrate that the improved algorithm improves the average detection accuracy by 2.6% compared to the original algorithm and basically maintains the real-time speed as well. Experimental simulations on a subset of VOC data (human set) show a significant improvement in AP values and more effective and reliable detection tracking of moving targets. The stability and accuracy of motion target detection and tracking are improved by setting parameters, such as confidence level; the effectiveness and continuity of detection and tracking are judged by setting the interframe centroid distance.",
"Image matting plays a vital role in a variety of computer vision tasks including video editing and image fusion. Previously presented image matting algorithms might fail in producing favorable results since most of them concentrate on the similarity between the neighboring pixels while neglecting the corresponding spatial relationship. To address this issue, an end-to-end image matting framework through leveraging deep learning mechanism and graph theory is proposed. The proposed pipeline is a concatenation of one deep feature extraction component and a Graph Convolutional Network (GCN). The former part takes an image and its corresponding trimap as inputs and can generate the pixel-wise features, which are then exploited as the input of the GCN locating at the latter part of the proposed framework. The GCN would refine the features for every pixel and predict the alpha matte outcome of the image. The approach outperforms a group of state-of-the-art matting techniques as shown by the theoretical analysis and experimental results in terms of both accuracy and visual effects.",
"Recently, stransformer has achieve remarkable performance on various computer vision tasks. Unfortunately, vision transformer suffers from high computational cost to calculate the pair-wise relations of patches for images, and the computations grows quadratically with the number of patches in the images, which blocks their deployment on resource-limited devices such as mobile phones and various IoT devices. In this paper, we find that there are large amount of redundant computations and communications in the self-attention operation of vision transformers because of the high degree patch locality, and present a hardware-software co-designed solution for vision transformer exploiting the patch locality, termed as DiVIT. DiVIT substantially reduces redundant computations and communications in vision transformers and improves the performance and energy efficiency. The experiments demonstrates that DiVIT achieves an average of 8.2x and 41.3x speedup and over three orders of magnitude improvements energy-efficiency over CPUs and GPUs on real world datasets.",Vision transformer; Accelerators; Patch locality; Neural networks; ASIC
"The loss function, also known as cost function, is used for training a neural network or other machine learning models. Over the past decade, researchers have designed many loss functions for machine learning, such as mean squared error and mean absolute error. However, in deep learning, neurons of the last layer are usually activated by a sigmoid or softmax function. Thus, training with traditional losses would cause lower efficiency and accuracy. Recently, designing loss functions for deep learning methods has become one of the most challenging problems. This paper provides a comprehensive review of the recent progress and frontiers about loss functions in deep learning, especially for computer vision tasks. Specifically, we discuss the loss functions in three main computer vision tasks, i.e., object detection, face recognition, and image segmentation. Scholars have proposed several novel loss functions to cope with the specific problems such as imbalanced data, uncertain distribution of the predicted bounding boxes, varied overlapped mode between two bounding boxes and over-fitting. The survey details the source, derivation, and properties of each loss function. Furthermore, we also provide some advanced challenges about robust losses, generative adversarial networks, noise-tolerant losses, and semantic data augmentation. Finally, we deliver a summary and some promising future research directions. (c) 2022 Elsevier B.V. All rights reserved.",Loss function; Deep learning; Object detection; Face recognition; Image segmentation
"Acknowledgment This research work was supported by joint collaboration of Computer Vision and Pattern Vision (CVPR) Lab, Shri Guru Gobind Singhji Institute of Engineering and Technology, Nanded, India and Center for Intelligent Signal and Imaging Research (CISIR), Universiti Teknologi PETRONAS (UTP), Seri Iskandar, Malaysia under International Grant 015ME0-018.",Radiology report generation; X-ray; Encoder-Decoder; Residual attention module; Multilevel multi-attention mechanism; Radiology-trained word embedding
"Farey sequences have captured the attention of several researchers because of their wide applications in polygonal approximation, generation of Ford circles, and shape analysis. In this work, we extend the applications of these sequences to optimize chamfer masks for computation of distance maps in images. Compared with previous methods, the proposed method can more effectively generate optimal weights from larger chamfer masks without considering multiple and rather complex defining variables of the masks. Furthermore, our work demonstrates the relationship between size of the chamfer kernel, Farey sequence, and optimal weights of the chamfer mask. This interesting relationship, which may be useful in various image processing and computer vision tasks, has never been revealed by any other previous study. Results from the current research may advance our understanding on the applications of Farey sequences in computational geometry and vision-related tasks. To allow reproducibility of the results, implementation codes and datasets can be accessed in the public repository at https://www.mathworks.com/matlabcentral/fileexchange/71652-optimization-of-chamfer-masks.",
"Human pose estimation is a core component in applications for which some level of human-computer interaction is required, such as assistive robotics, ambient assisted living or the motion capture systems used in biomechanics or video games production. In this paper, we propose an end-to-end pipeline for estimating 3D human poses that works in real-time in an off-the-shelf computer, using as input video sequences captured with a commercial RGBD sensor. Our hybrid approach is composed of two stages: 2D pose estimation using deep neural networks and 3D registration, for which a lightweight algorithm based on classic computer vision techniques has been developed. We compare several 2D pose estimators and validate the performance of our proposed method against the state-of-the-art, using as benchmark an international and publicly available dataset. Our 2D to 3D registration module alone can reach frame rates of up to 99 fps, while achieving an average error per joint of 132 mm. Furthermore, the proposed solution is agnostic to the model used for 2D pose estimation and can be upgraded with new upcoming solutions or adapted for different articulated objects.",Computer vision; Human pose estimation; RGBD sensors; Human-computer interaction
"Deep feature fusion plays a significant role in the strong learning ability of convolutional neural networks (CNNs) for computer vision tasks. Recently, works continually demonstrate the advantages of efficient aggregation strategy and some of them refer to multiscale representations. In this article, we describe a novel network architecture for high-level computer vision tasks where densely connected feature fusion provides multiscale representations for the residual network. We term our method the ResDNet which is a simple and efficient backbone made up of sequential ResDNet modules containing the variants of dense blocks named sliding dense blocks (SDBs). Compared with DenseNet, ResDNet enhances the feature fusion and reduces the redundancy by shallower densely connected architectures. Experimental results on three classification benchmarks including CIFAR-10, CIFAR-100, and ImageNet demonstrate the effectiveness of ResDNet. ResDNet always outperforms DenseNet using much less computation on CIFAR-100. On ImageNet, ResDNet-B-129 achieves 1.94% and 0.89% top-1 accuracy improvement over ResNet-50 and DenseNet-201 with similar complexity. Besides, ResDNet with more than 1000 layers achieves remarkable accuracy on CIFAR compared with other state-of-the-art results. Based on MMdetection implementation of RetinaNet, ResDNet-B-129 improves mAP from 36.3 to 39.5 compared with ResNet-50 on COCO dataset.",Task analysis; Object detection; Network architecture; Convolution; Image segmentation; Feature extraction; Computer vision; Convolutional neural networks (CNNs); feature fusion; image classification; multiscale; object detection
"As a crucial task in Computer Vision, object detection has substantially improved in recent years, with the aid of deep learning and increasingly abundant datasets. However, compared with natural image detection, medical CT images require more precision due to the obvious clinical implications. Detecting multiple lesions or clusters with relatively few training samples and indistinctive feature representation is extremely problematic. In this paper, we propose comprehensive improvements to the original YOLOv3, such as data augmentation, feature attention enhancement and feature complementarity enhancement to increase general lesion area detection performance. Ablation studies use the open DeepLesion dataset to validate these improvements and confirm the effectiveness of each amendment. Comparisons between state-of-the-art counterparts demonstrated that the proposed lesion object detector has enhanced salient accuracy (under two commonly used metrics) and an exceptional speed-accuracy trade-off. The proposed model achieved 57.5% mAP and 85.07% sensitivity at 4 false positives (FPs) per image, while running at reliable 35.6 frames per second (FPS). These findings indicate that the proposed detector is more practicable than other currently available computer aided diagnostics (CAD).",Computer aided diagnostics; Computer vision; Deep learning; Object detection
"Due to the posteriori knowledge provided by the Human-Computer Interaction algorithms, interactive segmentation based on Graph Cuts can successfully extract the foreground in an image, which will, however, limit the scope of their application. Inspired by recent years of research on computer vision with fully convolutional neural networks, a fully automated image segmentation method based on FCN and Graph Cuts has been proposed. First, FCN is employed to obtain an original mask for the input image, but its boundary is poor. Second, we generate seed regions heuristically using color histograms and mathematical morphological operations on the original mask. Finally, iterative segmentation is performed using generative seeds and superpixel-level Graph Cuts. Experimental results show that our method has higher segmentation accuracy compared to other representative methods including interactive and automatic segmentation.",Automatic image segmentation; Graph cuts; FCN; Color histogram; Superpixel
"Construction sites are highly hazardous due to the dynamic interaction between workers and moving equipment, with high fatality rates caused by collision and falling from height, etc. Hence, identifying unsafe behaviors among workers is crucial for enhancing site safety, such as tracking their on-site movement and personal protective equipment (PPE). Vision-based video processing has been actively used to automatically recognize workers and their behaviors on construction sites. However, existing studies mainly monitor workers within a single camera capturing only a small sub-region. As workers typically move around fairly large sites, continuously tracking their movement across multiple cameras would enable more comprehensive behavioral analyses. Hence, this paper proposes a framework for monitoring safety compliance among workers, by combining worker re-identification (ReID) and PPE classification. Deep learning-based approaches are developed to address the challenges for these two tasks respectively. For ReID, a new loss function named similarity loss is designed to encourage deep learning models to learn more discriminative human features, realizing a more robust tracking of individual workers. For classifying PPE statuses, a weighted-class strategy is proposed to mitigate model bias when given imbalanced samples among classes, for improved performance despite limited training samples. By combining the ReID and PPE classification results, a workflow is developed to log any incident of workers not wearing the necessary PPEs. With an actual construction site dataset, the proposed methods improve worker ReID and PPE classification by 4% and 13% accuracies respectively, which will facilitate site video analytics and inspection of site safety compliance among workers.",Computer vision; Deep learning; Personal protective equipment classification; Safety compliance monitoring; Worker re-identification
"Although floods cause millions of dollars in economic and social losses each year, many people living in developing countries, such as Brazil, do not have access to a flooding alert system because of its cost. To address this issue, we propose a cheap and robust River Flooding Detection System, which can be easily deployed in any river with a flat surface at its bedside. The novelty of our system is the use of raw images from off-the-shelf cameras with no preprocessing required. Hence, our methodology can be deployed using existing surveillance cameras in urban environments. The proposed system measures the river level by first performing a semantic segmentation of the river water blade using Deep Neural Networks (DNNs). Then, it uses Computer Vision (CV) to estimate the water level. If the water level is near or above a dangerous threshold, it sends alerts automatically without human intervention. Moreover, our system can successfully measure a river's water level with a Mean Absolute Error (MAE) of 3.32 cm, which is enough to detect when a river is about to overflow. The system is also reliable in measuring a river's water level from different camera viewpoints and lighting conditions. We show our approach's viability and evaluate our prototype's performance and overhead by deploying it to monitor two urban rivers in the city of Sao Carlos, SP, Brazil.",River flooding detection; Semantic segmentation; Deep learning; Computer vision
"Digitization of cultural assets has become an important sub-area of computer vision (CV). Thus far, the value of digitization has been emphasized in terms of asset preservation and exhibition. The third aspect of digitization value is that the obtained digital data can be used to perform archaeological analysis based on physics and optics theories and simulations. This position paper emphasizes the importance of this third aspect, using our Kyushu decorative tumuli project as an illustrative example. In particular, we focus on the photometric approaches in the third aspect and explain the equipment and methods developed there as well as archaeological findings. This paper, then, proposes to establish this area as cyber-archaeology through categorizing and organizing those methodologies.",e-Heritage; Cultural assets; Cyber-archaeology; Decorative tumulus; Museum; Photometric data; 3D data; Reflectance; Color analysis; Simulation; Principle component analysis; Normalized cut; Restoration; Segmentation
"Unmanned aerial systems (UASs) are increasingly applied for bridge inspection. A vision-guided UAS with a lightweight convolutional neural network is developed to detect and locate bridge cracks, spalling, and corrosion. The contributions are as follows: (1) To address the problem that traditional UASs are global positioning system (GPS) required while GPS signals under bridge bottom generally are weak. A vision-guided UAS is designed and applied, in which a stereo vision-inertial fusion method is used to provide position data instead of GPS and an ultrasonic ranger is applied to avoid obstacles. (2) Most of the deep learning-based damage detection methods are offline detection, which is unsuitable for UAS-based inspection because the endurance time is limited. To solve this problem, a lightweight end-to-end object detection network is proposed, by replacing the backbone of the original You Only Look Once v3 network with MobileNetv2, and the proposed network of much faster inference speed can be transplanted to the onboard computer of the designed UAS so that real-time edge computing can be performed during inspection. (3) A damage location method based on vision positioning data and simultaneous localization and mapping is also proposed to meet the urgent needs of locating damage in the whole structure. Finally, the proposed system is applied to inspect a long-span bridge to detect and locate the most common damages: crack, spalling, and corrosion with high accuracy and efficiency, which verified the practicability of the system.",Unmanned aerial system; convolutional neural network; bridge inspection; visual-inertial odometry; simultaneous localization and mapping
"Arbuscular mycorrhizal fungi (AMF) infect plant roots and are hypothesized to improve plant growth. Recently, AMF is now available for axenic culture. Therefore, AMF is expected to be used as a microbial fertilizer. To evaluate the usefulness of AMF as a microbial fertilizer, we need to investigate the relationship between the degree of root colonization of AMF and plant growth. The method popularly used for calculation of the degree of root colonization, termed the magnified intersections method, is performed manually and is too labor-intensive to enable an extensive survey to be undertaken. Therefore, we automated the magnified intersections method by developing an application named Tool for Analyzing root images to calculate the Infection rate of arbuscular Mycorrhizal fungi: TAIM.  TAIM is a web-based application that calculates the degree of AMF colonization from images using automated computer vision and pattern recognition techniques. Experimental results showed that TAIM correctly detected sampling areas for calculation of the degree of infection and classified the sampling areas with 87.4% accuracy. TAIM is publicly accessible at .",arbuscular mycorrhizal fungi; magnified intersections method; computer vision; pattern recognition; deep convolutional neural networks; system development
The mathematical statement of the problem of recognizing rivet joint defects in aircraft products is given. A computational method for the recognition of rivet joint defects in aircraft equipment based on video images of aircraft joints has been proposed with the use of neural networks YOLO-V5 for detecting and MobileNet V3 Large for classifying rivet joint states. A novel dataset based on a real physical model of rivet joints has been created for machine learning. The accuracy of the result obtained during modeling was 100% in both binary and multiclass classification.,defect; rivet joint; aircraft equipment; detection; classification; pattern recognition; deep neural network; computer vision
"Selective weed treatment is a cost-effective method that reduces manpower and usage of the agrochemical, at the same time it requires an effective computer vision system to identify weeds and should be smaller in size to run in resourceconstrained devices. To accomplish this, a convolution neural network named Reduced Residual U-Net using Depth-wise separable Convolution (RRUDC) network is proposed in this paper. Residual Depth-wise separable Convolution Block (RDCB) is introduced as a functional unit in both contractive and expanding paths. Residual connection is incorporated inside every RDCB unit. This network employs semantic segmentation to analyze the crop field images pixel-wise. To reduce the parameter size, a depth-wise separable convolution technique is used which curtail the number of parameters generated by the model at a similar to 1/9 ratio with a very negligible drop in accuracy. The model is trained using Crop Weed Field Image Dataset (CWFID) and then the trained model is pruned to reduce the model size further. It compresses the fmal model size by around similar to 70% without affecting the performance. It has achieved segmentation accuracy of similar to 96%, a lesser error rate with a model size less than 3 MB. It can be compatible with converting the proposed deep learning model into a real-time computer vision application that seems more convenient for farmers in their resource-constrained devices on their agricultural land.",Computer vision; Convolution neural network; Deep learning; Pruning; Semantic segmentation; Weed detection
"Developing deep neural network (DNN) models for computer vision applications for construction is challenging due to the shortage of training data. To address this issue, we proposed a novel data augmentation method that integrates a conditional generative adversarial networks (GANs) framework with a target classifier. The integrated architecture enables adversarial attack and defense during end-to-end training, thereby making it possible to generate effective images for the target classifier's training. We trained and tested two image classification DNNs with and without data augmentation, where we confirmed the effectiveness of the proposed method: with the data augmentation, the classification accuracy improved by 4.2 percentage points, from 71.24% to 75.46%, with qualitatively improved feature extraction more focused on the target object. Given that the application areas of our method are open-ended, the result is noteworthy. The proposed method can help construction researchers offset the data insufficiency, which will contribute to having more accurate and scalable DNN-powered vision models in construction applications.",Data augmentation; Conditional generative adversarial networks (GANs); Adversarial attack; Adversarial defense
"Moving Object Segmentation (MOS) is a fundamental task in computer vision. Due to undesirable variations in the background scene, MOS becomes very challenging for static and moving camera sequences. Several deep learning methods have been proposed for MOS with impressive performance. However, these methods show performance degradation in the presence of unseen videos; and usually, deep learning models require large amounts of data to avoid overfitting. Recently, graph learning has attracted significant attention in many computer vision applications since they provide tools to exploit the geometrical structure of data. In this work, concepts of graph signal processing are introduced for MOS. First, we propose a new algorithm that is composed of segmentation, background initialization, graph construction, unseen sampling, and a semi-supervised learning method inspired by the theory of recovery of graph signals. Second, theoretical developments are introduced, showing one bound for the sample complexity in semi-supervised learning, and two bounds for the condition number of the Sobolev norm. Our algorithm has the advantage of requiring less labeled data than deep learning methods while having competitive results on both static and moving camera videos. Our algorithm is also adapted for Video Object Segmentation (VOS) tasks and is evaluated on six publicly available datasets outperforming several state-of-the-art methods in challenging conditions.",Videos; Task analysis; Signal processing algorithms; Object segmentation; Semisupervised learning; Deep learning; Complexity theory; Moving object segmentation; graph signal processing; semi-supervised learning; unseen videos; video object segmentation
"Semiconductors are essential components in many electronic devices. Because wafers are produced quickly and in large quantities, defects occur that adversely affect semiconductor properties. This makes it necessary to install powerful and robust inspection systems which use artificial intelligence techniques in the early stages of the manufacturing chain in order to detect and classify those defects. This paper proposes a method for defect detection and classification on images of semiconductor wafer materials obtained by means of a scanning electron microscope based in the following stages: (i) use of computer vision techniques to isolate the defect from the background; (ii) use of several descriptors based on shape, size, texture, histogram, and key-points to create a feature vector for the characterization of the defect; (iii) application of an exhaustive search as a feature selection method to determine the optimal subset of feature descriptors; and (iv) evaluation of the feature descriptors by using a support vector machine classifier providing the optimal set with highest F1-score metrics. Finally, the effectiveness of the proposed approach is compared with five popular feature selection methods, reporting better classification results than the latter.",Feature extraction; Support vector machines; Analysis of variance; Computer vision; Scanning electron microscopy; Manufacturing; Inspection; Semiconductor manufacturing; defect classification; feature selection; exhaustive search; machine learning; support vector machine
"In recent years, due to the advancements in machine learning, object detection has become a mainstream task in the computer vision domain. The first phase of object detection is to find the regions where objects can exist. With the improvements in deep learning, traditional approaches, such as sliding windows and manual feature selection techniques, have been replaced with deep learning techniques. However, object detection algorithms face a problem when performed in low light, challenging weather, and crowded scenes, similar to any other task. Such an environment is termed a challenging environment. This paper exploits pixel-level information to improve detection under challenging situations. To this end, we exploit the recently proposed hybrid task cascade network. This network works collaboratively with detection and segmentation heads at different cascade levels. We evaluate the proposed methods on three complex datasets of ExDark, CURE-TSD, and RESIDE, and achieve a mAP of 0.71, 0.52, and 0.43, respectively. Our experimental results assert the efficacy of the proposed approach.",object detection; challenging environments; low-light; complex environments; deep neural networks; computer vision
"The latest research in computer vision highlighted the effectiveness of the vision transformers (ViT) in performing several computer vision tasks; they can efficiently understand and process the image globally unlike the convolution which processes the image locally. ViTs outperform the convolutional neural networks in terms of accuracy in many computer vision tasks but the speed of ViTs is still an issue, due to the excessive use of the transformer layers that include many fully connected layers. Therefore, we propose a real-time ViT-based monocular depth estimation (depth estimation from single RGB image) method with encoder-decoder architectures for indoor and outdoor scenes. This main architecture of the proposed method consists of a vision transformer encoder and a convolutional neural network decoder. We started by training the base vision transformer (ViT-b16) with 12 transformer layers then we reduced the transformer layers to six layers, namely ViT-s16 (the Small ViT) and four layers, namely ViT-t16 (the Tiny ViT) to obtain real-time processing. We also try four different configurations of the CNN decoder network. The proposed architectures can learn the task of depth estimation efficiently and can produce more accurate depth predictions than the fully convolutional-based methods taking advantage of the multi-head self-attention module. We train the proposed encoder-decoder architecture end-to-end on the challenging NYU-depthV2 and CITYSCAPES benchmarks then we evaluate the trained models on the validation and test sets of the same benchmarks showing that it outperforms many state-of-the-art methods on depth estimation while performing the task in real-time (similar to 20 fps). We also present a fast 3D reconstruction (similar to 17 fps) experiment based on the depth estimated from our method which is considered a real-world application of our method.",monocular depth estimation; convolutional neural networks; vision transformers; real-time processing
"Modern roundabouts are popular intersection control designs in many countries and are increasingly popular in the United States. Roundabouts facilitate reduced vehicle delays with naturally optimized conflict resolution for turning traffic, which also reduces the risks of severe crashes. However, evaluating the roundabout capacity for multilane configurations can be challenging due to the randomized decision making to accept or reject a headway to enter the roundabout. In addition, considering the follow-up headway between two vehicles entering the roundabout from the same lane is critical to evaluate accurate roundabout capacity. Several manual techniques are popularly used to evaluate roundabout capacity using computer vision powered by multiple video cameras and observers. However, manual processing of videos with a narrow field of view (FoV) requires significant computational effort. Traditional techniques used in manual processing involve a complex two-step time stamp recording and interpreting the parameters required for capacity evaluation. In this case study, a one-step gap-based methodology is proposed to accurately measure the roundabout capacity parameters. In addition, a computer vision algorithm is developed to integrate with deep learning to detect and track vehicular traffic in a multilane roundabout. A software-defined technique is developed to process videos with wider FoV powered by unmanned aerial vehicles (UAVs) and evaluate roundabout capacity parameters, such as accept, reject, and follow-up headways. Furthermore, the mean critical headway is calculated using a maximum likelihood estimation method. The evaluated roundabout capacity parameters are compared with manual technique results, and the corresponding values are published in the current standards. (C) 2022 American Society of Civil Engineers.",
"Automatic garment size measurement approaches using computer vision algorithms have been attempted in various ways, but there are still many limitations to overcome. One limitation is that the process involves 2D images, which results in constraints in the process of determining the actual distance between the estimated points. To solve this problem, in this paper, we propose an automated method for measuring garment sizes using computer vision deep learning models and point cloud data. In the proposed method, a deep learning-based keypoint estimation model is first used to capture the clothing size measurement points from 2D images. Then, point cloud data from a LiDAR sensor are used to provide real-world distance information to calculate the actual clothing sizes. As the proposed method uses a mobile device equipped with a LiDAR sensor and camera, it is also more easily configurable than extant methods, which have varied constraints. Experimental results show that our method is not only precise but also robust in measuring the size regardless of the shape, direction, or design of the clothes in two different environments, with 1.59% and 2.08% of the average relative error, respectively.",garment measurement; LiDAR; point cloud data; deep leaning; convolutional neural networks; keypoint estimation
"Featured Application Computer vision, visual scenes understanding, and image retrieval. The abundant visual information contained in multi-view images is widely used in computer vision tasks. Existing visual relationship detection frameworks have extended the feature vector to improve model performance. However, single view information can not fully reveal the visual relationships in complex visual scenes. To solve this problem and explore the multi-view information in a visual relationship detection (VRD) model, a novel multi-view VRD framework based on a monocular RGB image and an estimated depth map is proposed. The contributions of this paper are threefold. First, we construct a novel multi-view framework which fuses information of different views extracted from estimated RGB-D images. Second, a multi-view image generation method is proposed to transfer flat visual space to 3D multi-view space. Third, we redesign the visual relationship balanced classifier which can process multi-view feature vectors simultaneously. Detailed experiments were conducted on two datasets to demonstrate the effectiveness of the multi-view VRD framework. The experimental results showed that the multi-view VRD framework resulted in state-of-the-art zero-shot learning performance in specific depth conditions.",visual relationship detection; RGB-D image; depth map; multi view; computer vision
"Weather prediction from real-world images can be termed a complex task when targeting classification using neural networks. Moreover, the number of images throughout the available datasets can contain a huge amount of variance when comparing locations with the weather those images are representing. In this article, the capabilities of a custom built driver simulator are explored specifically to simulate a wide range of weather conditions. Moreover, the performance of a new synthetic dataset generated by the above simulator is also assessed. The results indicate that the use of synthetic datasets in conjunction with real-world datasets can increase the training efficiency of the CNNs by as much as 74%. The article paves a way forward to tackle the persistent problem of bias in vision-based datasets.",weather classification; synthetic data; dataset; autonomous car; computer vision; advanced driver assistance systems; deep learning; intelligent transportation systems
"Vision-based detection of fingertips is useful for freehand Human-Computer Interaction (HCI)-especially in virtual, augmented, and mixed reality-to have a seamless experience. The estimation of fingertips position in an RGB image involves overcoming various challenges like occlusion, appearance ambiguities, etc. The general approach relies on a two-stage pipeline involving hand location and detection of fingertips for a single hand. This paper presents an effective single-stage Convolutional Neural Network (CNN) for the detection of fingertips of both hands. We use a set of reference points, referred to as pose particles, and train a CNN model end-to-end to find the N-nearest particles in the proximity of each fingertip. Moreover, the same CNN model is used to compute the position vector's components with reference to these N-nearest neighbors. Finally, a fingertip position is estimated by computing the centroid of all the points given by these position vectors. With the proposed approach, it is possible to estimate the fingertips position for single or double hands. Moreover, there is no requirement for prior hand localization. We demonstrated the feasibility and effectiveness of the proposed methodology by performing experiments on three different datasets.",Location awareness; Indexes; Image segmentation; Pose estimation; Human computer interaction; Heating systems; Skin; Computer vision; deep neural network; monocular; RGB; fingertips detection; human-computer interaction; color image
"Since safety plays a crucial role and the top priority, in both unmanned and driver-assistance driving systems, there is a need of efficient and accurate detection of captured objects by object detection algorithms in real-time. Directly applying existing models to tackle real-time pedestrian and vehicle detection tasks captured by high speed moving vehicle scenarios has two problems. First, the target scale varies drastically because the vehicle speed changes greatly. Second, captured images contain both tiny targets and high density targets, which brings in occlusion between targets. To solve the two issues, an efficient light weight real-time detection algorithm is proposed, which is referred to as EfficientLiteDet. Based on Tiny-YOLOv4, one more prediction head is introduced in the proposed model to detect multi-scale targets effectively. In order to detect tiny and occluded denser targets, we used Transformer Prediction Heads (TPH) instead of original anchor detection heads in our model. To explore the potential of self-attention mechanism in TPH, the proposed model integrates convolutional block attention model to locate crucial attention region on scenarios with denser targets. Further to improve the detection performance of our model, we applied various data augmentation strategies such as mosaic, mix-up, multi-scale, and random-horizontal-flip during the model training. Extensive experiments are conducted on five challenging pedestrian and vehicle datasets shows that the EfficientLiteDet model has better performance in real-time scenarios. On Pascal Voc-2007, Highway and Udacity datasets, the proposed model achieves mean average precision (mAP) 87.3%, 80.1% and 77.8%, respectively, which is quite better than Tiny-YOLOv4 state-of-the-art algorithm by + 2.4%, 1.8% and + 2.4%, respectively.",Computer vision (CV); EfficientLiteDet; Light-weight; Pedestrian and vehicle detection; Tiny-YOLOv4
"Recent developments in video analysis of sports and computer vision techniques have achieved significant improvements to enable a variety of critical operations. To provide enhanced information, such as detailed complex analysis in sports such as soccer, basketball, cricket, and badminton, studies have focused mainly on computer vision techniques employed to carry out different tasks. This paper presents a comprehensive review of sports video analysis for various applications: high-level analysis such as detection and classification of players, tracking players or balls in sports and predicting the trajectories of players or balls, recognizing the team's strategies, and classifying various events in sports. The paper further discusses published works in a variety of application-specific tasks related to sports and the present researcher's views regarding them. Since there is a wide research scope in sports for deploying computer vision techniques in various sports, some of the publicly available datasets related to a particular sport have been discussed. This paper reviews detailed discussion on some of the artificial intelligence (AI) applications, GPU-based work-stations and embedded platforms in sports vision. Finally, this review identifies the research directions, probable challenges, and future trends in the area of visual recognition in sports.",sports; ball detection; player tracking; artificial intelligence; computer vision; embedded platforms
"Cartoons and animation domain videos have very different characteristics compared to real-life images and videos. In addition, this domain carries a large variability in styles. Current computer vision and deep-learning solutions often fail on animated content because they were trained on natural images. In this paper we present a method to refine a semantic representation suitable for specific animated content. We first train a neural network on a large-scale set of animation videos and use the mapping to deep features as an embedding space. Next, we use self-supervision to refine the representation for any specific animation style by gathering many examples of animated characters in this style, using a multi-object tracking. These examples are used to define triplets for contrastive loss training. The refined semantic space allows better clustering of animated characters even when they have diverse manifestations. Using this space we can build dictionaries of characters in an animation videos, and define specialized classifiers for specific stylistic content (e.g., characters in a specific animation series) with very little user effort. These classifiers are the basis for automatically labeling characters in animation videos. We present results on a collection of characters in a variety of animation styles.Code and resources are available at: .",CCS Concepts; center dot Imaging and Video -> Video Summarization; center dot Methods and Applications -> Artificial Intelligence; Computer Vision; Neural Nets
"With the advent of the 4th Industrial Revolution, research on anomaly detection in the manufacturing process using deep learning and machine vision is being actively conducted. There have been various attempts to innovate the manufacturing site by adopting advance information technologies such as machine vision, machine learning, and deep learning in many manufacturing processes. However, there have been no cases of designing and implementing these technologies at the mask manufacturing site, which is essential to tackle COVID-19 pandemic. The originality of this paper is to implement sustainability in the mask manufacturing environment and industrial eco-system by introducing the latest computer technology into the manufacturing process essential for pandemic-related disasters. In this study, the intention is to establish a machine vision-based quality inspection system in actual manufacturing process to improve sustainable productivity in the mask manufacturing process and try a new technical application that can contribute to the overall manufacturing process industry in Korea in the future. Therefore, the purpose of this paper is to specifically present hardware and software system construction and implementation procedures for inspection process automation, control automation, POP (Point Of Production) manufacturing monitoring system construction, smart factory implementation, and solutions. This paper is an application study applied to an actual mask manufacturing plant, and is a qualitative analysis study focused on improving mask productivity. Company A is a mask manufacturing company that produces tons of masks everyday located in Korea. This company planned to automate the identification of good and defective products in the mask manufacturing process by utilizing machine vision technology. To this end, a deep learning and machine vision-based anomaly detection manufacturing environment is implemented using the LAON PEOPLE NAVI AI Toolkit. As a result, the productivity of Company A's mask defect detection process can be dramatically improved, and this technology is expected to be applied to similar mask manufacturing processes in the future to make similar manufacturing sites more sustainable.",machine vision; quality inspection; quality assurance; anomaly detection; machine learning; deep learning; smart factory
"Single-view computer vision models for vehicle damage inspection often suffer from strong light reflections. To resolve this, multiple images under various viewpoints can be used. However, multiple views increase the complexity as multi-view training data, specialized models, and damage re-identification over different views are required. In addition, traditional point cloud applications require large computational power, being impractical for edge computing. Therefore, multi-view damage inspection has not yet found its way into practical applications. We present a novel approach that projects the results from widely available single-view computer vision models onto 3D representations, to combine the detections from various viewpoints. With this, we leverage all advantages of multi-view damage inspection, without the need for multi-view training data and specialized models or hardware. We conduct a practical evaluation using a drive-through camera setup, to show the applicability of the methods in practice. We show that our proposed method successfully combines similar damages across viewpoints, reducing the number of duplicate damages by almost 99%. In addition, we show that our approach reduces the number of false positives by 96%. The proposed method leverages the existing single-view training data and single-view deep learning models to make multi-view inspection more accessible for practical implementations.",Damage detection; Inspection; Multi-view; 3D models; Ray tracing; Vehicles
"The ChaLearn large-scale gesture recognition challenge has run twice in two workshops in conjunction with the International Conference on Pattern Recognition (ICPR) 2016 and International Conference on Computer Vision (ICCV) 2017, attracting more than 200 teams around the world. This challenge has two tracks, focusing on isolated and continuous gesture recognition, respectively. It describes the creation of both benchmark datasets and analyzes the advances in large-scale gesture recognition based on these two datasets. In this article, we discuss the challenges of collecting large-scale ground-truth annotations of gesture recognition and provide a detailed analysis of the current methods for large-scale isolated and continuous gesture recognition. In addition to the recognition rate and mean Jaccard index (MJI) as evaluation metrics used in previous challenges, we introduce the corrected segmentation rate (CSR) metric to evaluate the performance of temporal segmentation for continuous gesture recognition. Furthermore, we propose a bidirectional long short-term memory (Bi-LSTM) method, determining video division points based on skeleton points. Experiments show that the proposed Bi-LSTM outperforms state-of-the-art methods with an absolute improvement of 8.1% (from 0.8917 to 0.9639) of CSR.",Gesture recognition; Measurement; Task analysis; Training; Conferences; Computer vision; Bidirectional long short-term memory (Bi-LSTM); gesture recognition; RGB-D
"Machine vision is the key to realizing computer-vision tasks such as human-computer interaction and autonomous driving. However, human perception of an image's beauty is innate. If a machine can increase aesthetic awareness, it will greatly improve the comfort of human perception in human-computer interaction. The bokeh effect is one of the most important ways to improve the artistic beauty of photographic images and the image aesthetic quality. Bokeh rendering of an image can highlight the main object of the image and blur unnecessary or unattractive background details. The existing methods usually have unrealistic rendering effects with obvious artifacts around the foreground boundary. Therefore, we propose a natural bokeh-rendering method based on depth estimation that satisfies the following characteristics: objects in the focal plane are clear and out-of-focus objects are blurred; and the further away from the focal plane, the more blurred the objects are. Our method consists of three modules: depth estimation, background subdivision, and bokeh rendering. The background-subdivision module can select different focal planes to obtain different blur radii, making the bokeh-rendering effect more diverse, so that it does not oversegment objects. The bokeh-rendering module adjusts the degree of bokeh by adjusting the blur-radius factor. In the experimental section, we analyze the model results and present the visualization results.",depth estimation; bokeh effect; Transformer; refocused image; machine vision
"Specular highlight in images is detrimental to accuracy in object recognition tasks. The prior model-based methods for single image highlight removal (SHIR) are limited in images with large highlight regions or achromatic regions, and recent learning-based methods do not perform well due to lack of proper datasets for training either. A network for SHIR is proposed, which is trained with losses that utilize image intrinsic features and can reconstruct a smooth and natural specular-free image from a single input highlight image. Dichromatic reflection model is used to compute the pseudo specular-free image for providing complementary information for the network. A real-world dataset with highlight images and the corresponding ground-truth specular-free images is collected for network training and quantitative evaluation. The proposed network is validated by comprehensive quantitative experiments and outperforms state-of-the-art highlight removal approaches in structural similarity and peak signal-to-noise ratio. Experimental results also show that the network could improve the recognition performance in applications of computer vision. Our source code is available at https://github.com/coach-wang/SIHRNet. (C) 2022 SPIE and IS&T",image processing; highlight removal; image enhancement; computer vision; neural network
"Bridge inspection plays a critical role in mitigating the safety risks associated with bridge deterioration and decay. CV (computer vision) technology can facilitate bridge inspection by accurately automating the structural recognition tasks, especially useful in UAV (unmanned aerial vehicles)-assisted bridge inspections. This study proposed a framework for the multilevel inspection of bridges based on CV technology, and provided verification using CNN (convolution neural network) models. Using a long-distance dataset, recognition of the bridge type was performed using the Resnet50 network. The dataset was built using internet image captures of 1200 images of arched bridges, cable-stayed bridges and suspension bridges, and the network was trained and evaluated. A classification accuracy of 96.29% was obtained. The YOLOv3 model was used to recognize bridge components in medium-distance bridge images. A dataset was created from 300 images of girders and piers collected from the internet, and image argumentation techniques and the tuning of model hyperparameters were investigated. A detection accuracy of 93.55% for the girders and 82.64% for the piers was obtained. For close-distance bridge images, segmentation and recognition of bridge components were investigated using the instance segmentation algorithm of the Mask-RCNN model. A dataset containing 800 images of girders and bearings was created, and annotated based on Yokohama City bridge inspection image records data. The trained model showed an accuracy of 90.8% for the bounding box and 87.17% for the segmentation. This study also contributed to research on bridge image acquisition, computer vision model comparison, hyperparameter tuning, and optimization techniques.",bridge inspection; computer vision; components segmentation; multilevel detection
"Document classification is an important area in Natural Language Processing (NLP). Because a huge amount of scientific papers have been published at an accelerating rate, it is beneficial to carry out intelligent paper classifications, especially fine-grained classification for researchers. However, a public scientific paper dataset for fine-grained classification is still lacking, so the existing document classification methods have not been put to the test. To fill this vacancy, we designed and collected the PaperNet-Dataset that consists of multi-modal data (texts and figures). PaperNet 1.0 version contains hierarchical categories of papers in the fields of computer vision (CV) and NLP, 2 coarse-grained and 20 fine-grained (7 in CV and 13 in NLP). We ran current mainstream models on the PaperNet-Dataset, along with a multi-modal method that we propose. Interestingly, none of these methods reaches an accuracy of 80% in fine-grained classification, showing plenty of room for improvement. We hope that PaperNet-Dataset will inspire more work in this challenging area.",artificial intelligence application; dataset; multi-modal information processing; machine learning; paper classification
"In recent years, teleoperation has experienced rapid development. Numerous teleoperation applications in diverse areas have been reported. Among all teleoperation-related components, computer vision (CV) is treated as one of the must-have technologies, because it allows users to observe remote scenarios. In addition, CV can further help the user to identify and track the desired targets from complex scenes. It has been proven that efficient CV methods can significantly improve the operation accuracy and relieve user's physical and mental fatigue. Therefore, furthering understanding about CV techniques and reviewing the latest research outcomes is necessary for teleoperation designers. In this context, this review article was composed.",teleoperation; computer vision; object detection; distance measurement
"Deep neural networks are efficient methods of recognizing image patterns and have been largely implemented in computer vision applications. Object detection has many applications in computer vision, including face and vehicle detection, video surveillance, and plant leaf detection. An automatic flower identification system over categories is still challenging due to similarities among classes and intraclass variation, so the deep learning model requires more precisely labeled and high-quality data. In this proposed work, an optimized and generalized deep convolutional neural network using Faster-Recurrent Convolutional Neural Network (Faster-RCNN) and Single Short Detector (SSD) is used for detecting, localizing, and classifying flower objects. We prepared 2000 images for various pretrained models, including ResNet 50, ResNet 101, and Inception V2, as well as Mobile Net V2. In this study, 70% of the images were used for training, 25% for validation, and 5% for testing. The experiment demonstrates that the proposed Faster-RCNN model using the transfer learning approach gives an optimum mAP score of 83.3% with 300 and 91.3% with 100 proposals on ten flower classes. In addition, the proposed model could identify, locate, and classify flowers and provide essential details that include flower name, class classification, and multilabeling techniques.",
"Background Plant diseases significantly affect the crop, so their identification is very important. Correct identification of these diseases is crucial for establishing a good disease control strategy to avoid time and financial losses. In general, machines can greatly reduce the possibility of human error. In particular, computer vision techniques developed through deep learning have paved a way to detect and diagnose these plant diseases on the leaf. Methods In this work, the model AFD-Net was developed to detect and identify various leaf diseases in apple trees. The dataset is from Kaggle 2020 and 2021 and was financially supported by the Cornell Initiative for Digital Agriculture. An AFD-Net was proposed for leaf disease classification in apple trees and the results of the efficiency of the model are compared with other state-of-the-art deep learning approaches. Results The results of the experiments in the validation dataset show that the proposed AFD-Net model achieves the highest values of 98.7% accuracy for Plant Pathology 2020 and 92.6% for Plant Pathology 2021 compared to other deep learning models in the original and extended datasets. Discussion The results also indicate the efficiency of the proposed model in identifying leaf diseases on apple trees for major and minor classes, i.e., for multiple classification.",Computer Vision; Deep Learning; Kaggle; Foliar Disease; Efficient Net
"The ability for researchers to re-identify animal individuals upon re-encounter is fundamental for the study of population dynamics, community, and behavioural ecology. Animal re-identification is traditionally performed using tagging or DNA sampling, which is laborious, invasive to an animal, and expensive. An alternative approach to re-identify is the use of computer vision in combination with pattern recognition algorithms. Deep learning has accelerated the success when solving pattern recognition in the field of computer vision when high data volume is available; however, conventional deep learning approaches require ample training data for a fixed number of classes. An alternative deep learning paradigm is similarity comparison networks which are trained to identify if two inputs are the same or different. This principle can be applied to images of animal individuals and allows for the re-identification of individuals beyond the original training data. Here, we test the potential and generality of similarity comparison networks for animal re-identification considering five datasets of different species: humans, chimpanzees, humpback whales, fruit flies, and Siberian tigers, each with their own unique set of challenges. We compare 10 similarity comparison networks by testing five well-established network architectures (AlexNet, VGG19, DenseNet201, ResNet152, and InceptionV3) and two different methods to train each of them: contrastive and triplet loss. Models were trained to re-identify individuals and those trained using the triplet loss outperformed contrastive loss for all species. Our work shows that without any species-specific modifications, similarity comparison networks can act as a general purpose animal re-identification system considering individuals from images. Our expectation is that similarity comparison networks are the beginning of a major trend that has the potential to revolutionize the study of population dynamics, community, and behavioural ecology. This work is an extension of a technical report presented at WACV 2020 (IEEE/CVF winter conference on applications of computer vision workshops) catered for an ecological audience.",Camera traps; Computer vision; Contrastive learning; Deep learning; Photographic identification (photo-ID); Population ecology; Re-identification
"Searching for the nearest neighbor is a fundamental problem in the computer vision field, and deep hashing has become one of the most representative and widely used methods, which learns to generate compact binary codes for visual data. In this paper, we first delve into the representation learning of deep hashing and surprisingly find that deep hashing could be a double-edged sword, i.e., deep hashing can accelerate the query speed and decrease the storage cost in the nearest neighbor search progress, but it greatly sacrifices the discriminability of deep representations especially with extremely short target code lengths. To solve this problem, we propose a two-step deep hashing learning framework. The first step focuses on learning deep discriminative representations with metric learning. Subsequently, the learning framework concentrates on simultaneously learning compact binary codes and preserving representations learned in the former step from being sacrificed. Extensive experiments on two general image datasets and four challenging image datasets validate the effectiveness of our proposed learning framework. Moreover, the side effect of deep hashing is successfully mitigated with our learning framework. (C) 2022 The Author(s). Published by Elsevier B.V.",Computer vision; Deep hashing; Representation learning; Metric learning; Transfer learning
"With rapid development of deep learning, a lot of computer vision tasks, such as object detection and semantic segmentation, have been applied to various Advanced Driver Assistance Systems. However, few computer vision solutions to estimate rainfall amount have been developed so far. So, we propose a rainfall amount estimation method based on deep learning and computer vision. The proposed method mainly consists of two steps. The first step is raindrop segmentation, and the second step is rainfall amount estimation. The raindrop segmentation is specifically based on three techniques: A relational ASPP to explore the correlation between raindrop features, a height attention module to consider the features that vary depending on raindrop locations, and a masking loss to further improve the performance of raindrop segmentation. Second, using the segmented raindrops, we present a rainfall amount estimation algorithm for auto-wiping. We experimentally achieved acceptable raindrop segmentation performance, i.e., mean IoU (mIoU) score of 70.6% that is much higher than other algorithms. This verifies that the proposed network is good at segmenting raindrops on a windshield. And, we demonstrate that the proposed rainfall amount estimation scheme provides sufficiently high accuracy of about 93%. In addition, we have built a rainy driving dataset for computer vision-based auto-wiping purpose and published it publicly on https://github.com/jjh930910/raindrop-segmentation.",Rainfall amount estimation; Raindrop segmentation; ADAS; Auto-wiping
"The use of computer vision techniques to detect objects in images has grown in recent years. These techniques are especially useful to automatically extract and analyze information from an image or a sequence of them. One of the problems addressed by computer vision is multi-object tracking over frames sequences. To know the path and direction of objects can be crucial for some areas like traffic control and supervision; by doing that the system can be able to reduce traffic jams or redirect vehicles over less condensed areas. These algorithms include several aspects to have in mind in order to start a new development or research in this area, for instance, is important to review the current state-of-the art techniques, the hardware requirements, the main evaluation metrics, the commonly used datasets, among others. Therefore, the objective of this research is to present a systematic literature review which analyzes the recent works developed in the area of multi-object tracking in traffic environments. This paper reviews the techniques, hardware, datasets, metrics, and open lines of research in this area. (C) 2022 The Author(s). Published by Elsevier B.V.",CNNs; Datasets; Evaluation metrics; MOT; Multi-object tracking; SLR; Systematic literature review; Traffic environments
"Object detection is one of the most fundamental and challenging tasks to locate objects in images and videos. Over the past, it has gained much attention to do more research on computer vision tasks such as object classification, counting of objects, and object monitoring. This study provides a detailed literature review focusing on object detection and discusses the object detection techniques. A systematic review has been followed to summarize the current research work's findings and discuss seven research questions related to object detection. Our contribution to the current research work is (i) analysis of traditional, two-stage, one-stage object detection techniques, (ii) Dataset preparation and available standard dataset, (iii) Annotation tools, and (iv) performance evaluation metrics. In addition, a comparative analysis has been performed and analyzed that the proposed techniques are different in their architecture, optimization function, and training strategies. With the remarkable success of deep neural networks in object detection, the performance of the detectors has improved. Various research challenges and future directions for object detection also has been discussed in this research paper.",Computer vision; Object detection; Dataset; Deep learning
"Many road accidents are happening due to the negligent behaviour of the drivers, which increases the death rate day by day. The tiredness and drowsiness of the drivers are the primary cause of road accidents. Due to technological advancement, various techniques evolved to identify the drowsy state and alert the driver. As per the literature, the drowsiness detection techniques are categorized into three classes based on driving pattern, physiological characteristics and Computer vision. Among these techniques, we have focussed mainly on the Computer Vision technique in our survey due to its low cost and non-intrusive nature. This technique analyses the various images of driver's posture, such as facial expression, yawning duration, head movement and eye closure to identify drowsy state. A detailed comparative study is presented in this paper and observed that spatial feature based techniques have given highest result with precision 97.12%. Also, state-of-the-art drowsiness detection techniques are exposed, analyzed and reviewed rigorously.",Drowsiness detection techniques; Driver fatigue; Image processing; Face detection; Eye detection; Accidents
"Aggregate segregation is a major form of defect that accelerates the pavement deterioration rate. Therefore, asphalt pavement segregation needs to be detected accurately and early during the quality survey process. This study proposes and verifies a computer vision based method for automatic identification of aggregate segregation. The new method includes Extreme Gradient Boosting Machine integrated with Attractive Repulsive Center Symmetric Local Binary Pattern (ARCSLBP-XGBoost) and Deep Convolutional Neural Network (DCNN). Experimental results obtained from a repetitive random data sampling process with 20 runs show that the ARCSLBPXGBoost is a capable approach for detecting asphalt pavement segregation with outstanding performance measurement metrics (classification accuracy rate = 0.95, precision = 0.93, recall = 0.98, and F1 score = 0.95).",Computer vision; Asphalt pavement segregation; Machine learning; Deep learning; XGBoost; Texture analysis
"Computer vision (CV) techniques have played an important role in promoting the informatization, digitization, and intelligence of industrial manufacturing systems. Considering the rapid development of CV techniques, we present a comprehensive review of the state of the art of these techniques and their applications in manufacturing industries. We survey the most common methods, including feature detection, recognition, segmentation, and three-dimensional modeling. A system framework of CV in the manufacturing environment is proposed, consisting of a lighting module, a manufacturing system, a sensing module, CV algorithms, a decision-making module, and an actuator. Applications of CV to different stages of the entire product life cycle are then explored, including product design, modeling and simulation, planning and scheduling, the production process, inspection and quality control, assembly, transportation, and disassembly. Challenges include algorithm implementation, data preprocessing, data labeling, and benchmarks. Future directions include building benchmarks, developing methods for nonannotated data processing, developing effective data preprocessing mechanisms, customizing CV models, and opportunities aroused by 5G.",Image edge detection; Image segmentation; Task analysis; Robot sensing systems; Sensors; Feature detection; Three-dimensional displays; Assembly; computer vision (CV); deep learning; inspection; machine intelligence; machine learning; manufacturing; production; robotics; survey
"Shadow detection and removal play an important role in the field of computer vision and pattern recognition. Shadow will cause some loss and interference to the information of moving objects, resulting in the performance degradation of subsequent computer vision tasks such as moving object detection or image segmentation. In this paper, each image is regarded as a small sample, and then a method based on material matching of intelligent computing between image regions is proposed to detect and remove image shadows. In shadow detection, the proposed method can be directly used for detection without training and ensures the consistency of similar regions to a certain extent. In shadow removal, the proposed method can minimize the influence of shadow removal operation on other features in the shadow region. The experiments on the benchmark dataset demonstrate that the proposed approach achieves a promising performance, and its improvement is more than 6% in comparison with several advanced shadow detection methods.",
"Human action recognition is an important field in computer vision that has attracted remarkable attention from researchers. This survey aims to provide a comprehensive overview of recent human action recognition approaches based on deep learning using RGB video data. Our work divides recent deep learning-based methods into five different categories to provide a comprehensive overview for researchers who are interested in this field of computer vision. Moreover, a pure-transformer architecture (convolution-free) has outperformed its convolutional counterparts in many fields of computer vision recently. Our work also provides recent convolution-free-based methods which replaced convolution networks with the transformer networks that achieved state-of-the-art results on many human action recognition datasets. Firstly, we discuss proposed methods based on a 2D convolutional neural network. Then, methods based on a recurrent neural network which is used to capture motion information are discussed. 3D convolutional neural network-based methods are used in many recent approaches to capture both spatial and temporal information in videos. However, with long action videos, multistream approaches with different streams to encode different features are reviewed. We also compare the performance of recently proposed methods on four popular benchmark datasets. We review 26 benchmark datasets for human action recognition. Some potential research directions are discussed to conclude this survey.",
"Object detection is one significant field of computer vision. The imbalance problem exerts negative effects on achieving satisfactory performance. We reveal two sources of imbalance in existing object detection methods. Correspondingly, we propose our methods in terms of the model architecture and optimization target. Different from general object detection benchmarks, the location distribution of objects with different sizes is unbalanced in many practical applications. In addition, the representation information of different categories of objects is unbalanced. In this paper, we propose a location scale equilibrium module to utilize the prior location scale information and generate more balanced feature maps. More appropriate feature maps are selected and merged for different locations. After merging, feature maps become more consistent in terms of representation content, exerting positive effects on the following classification and regression tasks. For the imbalance caused by similar objects, we propose the repulsive loss to strengthen the punishment. Our method will not treat all categories of objects equally since we take the imbalance between them into consideration. With the enhanced supervision, the training will pay more attention to similar objects. Our proposed model is evaluated on the VisDrone benchmark and UAVDT benchmark. Sufficient experiments are conducted. Our model achieves the highest precision on most evaluation metrics, outperforming the other strong models.@ 2021 Elsevier B.V. All rights reserved.",Deep learning; Computer vision; Convolutional neural network; Object detection
"The emergence of machine vision has promoted the automation of defect detection (DD) in the industrial field. Therefore, scholars at home and abroad have carried out a lot of research and exploration on the traditional visual DD method of mechanical design products. At the same time, this method has been widely used in the field of modern manufacturing due to its noncontact and fast detection speed. The traditional visual detection method is to use cameras, computers, and other equipment instead of people to detect the detected objects, although this method improves the production efficiency to a certain extent. However, this detection method is greatly affected by light, has a certain false detection rate, and has poor adaptability. The intelligent detection method based on deep learning developed on the basis of traditional vision is a further optimization of traditional visual detection methods. The rapid development of deep learning makes the advantages of visual DD more obvious.",
"Premise Angiosperm leaves present a classic identification problem due to their morphological complexity. Computer-vision algorithms can identify diagnostic regions in images, and heat map outputs illustrate those regions for identification, providing novel insights through visual feedback. We investigate the potential of analyzing leaf heat maps to reveal novel, human-friendly botanical information with applications for extant- and fossil-leaf identification. Methods We developed a manual scoring system for hotspot locations on published computer-vision heat maps of cleared leaves that showed diagnostic regions for family identification. Heat maps of 3114 cleared leaves of 930 genera in 14 angiosperm families were analyzed. The top-5 and top-1 hotspot regions of highest diagnostic value were scored for 21 leaf locations. The resulting data were viewed using box plots and analyzed using cluster and principal component analyses. We manually identified similar features in fossil leaves to informally demonstrate potential fossil applications. Results The method successfully mapped machine strategy using standard botanical language, and distinctive patterns emerged for each family. Hotspots were concentrated on secondary veins (Salicaceae, Myrtaceae, Anacardiaceae), tooth apices (Betulaceae, Rosaceae), and on the little-studied margins of untoothed leaves (Rubiaceae, Annonaceae, Ericaceae). Similar features drove the results from multivariate analyses. The results echo many traditional observations, while also showing that most diagnostic leaf features remain undescribed. Conclusions Machine-derived heat maps that initially appear to be dominated by noise can be translated into human-interpretable knowledge, highlighting paths forward for botanists and paleobotanists to discover new diagnostic botanical characters.",cleared leaves; computer vision; fossil identification; fossil leaves; heat maps; leaf architecture; leaf identification; leaf margin; leaf teeth; leaf venation
"Generally, images acquired under low-light conditions suffer from contrast loss, blurred scene details and colour distortion, resulting in poor visibility. It conceals certain features helpful to computer vision tasks. Hence, a low-light image enhancement is vital in improving the quality of such images. Inspired by Retinex theory, different low-light image enhancement methods implement light correction and deterioration minimization independently in the illumination and reflectance components produced by decomposing the given low-light image. Besides, ZeroDCE is a recent low-light image enhancement method that utilizes deep light curve estimation with the DCE-Net model. In this paper, we propose a hybrid low-light image enhancement method that combines the Zero-DCE method and Retinex decomposition for better performance. It employs deep light curve estimation of the Zero-DCE method on the illuminance component obtained by the Joint intrinsic-extrinsic prior model for Retinex decomposition. To validate, we conducted experiments on the SICE dataset and compared the results obtained with different methods qualitatively and quantitatively. It is evident from the results that our proposed method performed superior when compared to the state-of-the-art methods.",Computer vision; Decomposition; Image enhancement; Illuminance; Low-light image; Light curve estimation; Reflectance; Retinex theory
"Automated image de-fencing is an important area of computer vision that deals with the problem of virtually removing fence structures, if any, from images and produce aesthetically pleasing images without the fence structures. Unlike most of the previous de-fencing approaches that employ a two-stage process of fence mask detection followed by image inpainting, here we present a single-stage end-to-end conditional generative adversarial network-based de-fencing model that takes as input a fenced image and produces the corresponding de-fenced image in only 16 ms. The proposed network has been trained using an extensive dataset of fenced and ground-truth de-fenced image pairs by employing a combination of adversarial loss, L1 loss, perceptual loss, and estimated fence mask loss till convergence. The experimental results shows that our approach is capable of successfully handling images with even broken, irregular, and occluded fence structures. Qualitative and quantitative comparative study with previous de-fencing methods also show that our approach outperforms these existing techniques in terms of both response time and quality of de-fencing.",Image de-fencing; Single-stage cGAN; Mask loss; Computer vision
"Drowsiness is the principal cause of road crashes nowadays, as per the existing data. Drowsiness may put many precious lives in jeopardy. Drowsiness may be detected early and accurately, which can save lives. Using computer vision and deep learning techniques, this research proposes a new approach to detect driver drowsiness at an early stage with improved accuracy. In our developed model, we have considered the most significant temporal features such as head pose angles (Yaw, Pitch, and Roll), centers of pupil movement, and distance for the emotional feature that help in the detection of drowsiness state more accurately. Our method solves the possibility of occluded frames at initial stage via imposing the occlusion criteria depending on the relationship of distance between pupil centers and the horizontal length of the eye. As a result, it outperformed existing approaches in terms of overall system accuracy and consistency. Furthermore, retrieved features from correct frames are used as training and test data by the long short-term memory network to classify the driver's state. Here, results are elaborated in terms of area under the curve-receiver operating characteristic curve scores.",Computer vision; Classification; Drowsiness; Deep learning
"Marine scientists use remote underwater image and video recording to survey fish species in their natural habitats. This helps them get a step closer towards understanding and predicting how fish respond to climate change, habitat degradation and fishing pressure. This information is essential for developing sustainable fisheries for human consumption, and for preserving the environment. However, the enormous volume of collected videos makes extracting useful information a daunting and time-consuming task for a human being. A promising method to address this problem is the cutting-edge deep learning (DL) technology. DL can help marine scientists parse large volumes of video promptly and efficiently, unlocking niche information that cannot be obtained using conventional manual monitoring methods. In this paper, we first provide a survey of computer visions (CVs) and DL studies conducted between 2003 and 2021 on fish classification in underwater habitats. We then give an overview of the key concepts of DL, while analysing and synthesizing DL studies. We also discuss the main challenges faced when developing DL for underwater image processing and propose approaches to address them. Finally, we provide insights into the marine habitat monitoring research domain and shed light on what the future of DL for underwater image processing may hold. This paper aims to inform marine scientists who would like to gain a high-level understanding of essential DL concepts and survey state-of-the-art DL-based fish classification in their underwater habitat.",Computer Vision; Deep Learning; Fish Habitat; Monitoring
"Computer vision in sport is a very interesting application. People spend a lot of time watching sports videos because this is one of the best field of entertainment. Sports video broadcasts generally take a lot of time, ranging from two to four hours. However, the interesting part happens for just a few minutes. Detecting the highlighted event in a sport will be useful for people who like to watch only the prominent events section instead of watching the whole video broadcast. Event detection will give precise details about the action that occurred for a particular time, but the detection of highlighted events is more complex. This is due to the fact that a sports video contains collections of events. Among them, segregation of the required event is a time-consuming process but it requires more knowledge about the sport as well as processing time. Hence, a novel work is proposed focused on identifying the location of the functional object using agglomerative clustering and annotating the event highlights automatically by means of the rule inference mechanism. The SHRED (Sports Highlight Recognition and Event Detection) system achieves an overall accuracy of about 97.38% relative to other state-of-art methods in event class annotation.",Computer vision; event detection; event highlights; object localization; rule inference system
"Foggy weather can cause such problems as blurred image information and the loss of image details, which may pose great challenges to road traffic target detection based on images and videos. In this study, we propose a domain-adaptive road vehicle target detection method to implement domain adaptation for the real foggy scene. We firstly constructed a highway vehicle detection dataset with foggy images (HVFD), which contains normal weather images and foggy images and provides a complete data support for vehicle detection based on computer vision. Secondly, by improving CycleGAN we designed an improved generative confrontation network (CPGAN), which realised the style transfer between foggy images and normal weather images. Finally, we formulated a YOLOv4 target detection framework according to the domain adaptation based on the pre-trained YOLOv4 fog vehicle detection model. The experimental results show that the method we put forward can effectively improve vehicle detection performance and reduce the work of manually labelling a large number of foggy image tags, which has a strong generalisation ability for computer vision-based applications in low-visibility weather.",
"In recent years, the security concerns about the vulnerability of deep convolutional neural networks to adversarial attacks in slight modifications to the input image almost invisible to human vision make their predictions untrustworthy. Therefore, it is necessary to provide robustness to adversarial examples with an accurate score when developing a new classifier. In this work, we perform a comparative study of the effects of these attacks on the complex problem of art media categorization, which involves a sophisticated analysis of features to classify a fine collection of artworks. We tested a prevailing bag of visual words approach from computer vision, four deep convolutional neural networks (AlexNet, VGG, ResNet, ResNet101), and brain programming. The results showed that brain programming predictions' change in accuracy was below 2% using adversarial examples from the fast gradient sign method. With a multiple-pixel attack, brain programming obtained four out of seven classes without changes and the rest with a maximum error of 4%. Finally, brain programming got four categories without changes using adversarial patches and for the remaining three classes with an accuracy variation of 1%. The statistical analysis confirmed that brain programming predictions' confidence was not significantly different for each pair of clean and adversarial examples in every experiment. These results prove brain programming's robustness against adversarial examples compared to deep convolutional neural networks and the computer vision method for the art media categorization problem.",Brain programming; Adversarial attacks; Image classification; Art media categorization; Genetic programming
"As an important research issue in computer vision, human action recognition has been regarded as a crucial mean of communication and interaction between humans and computers. To help computers automatically recognize human behaviors and accurately understand human intentions, this paper proposes a separable three-dimensional residual attention network (defined as Sep-3D RAN), which is a lightweight network and can extract the informative spatial-temporal representations for the applications of video-based human computer interaction. Specifically, Sep-3D RAN is constructed via stacking multiple separable three-dimensional residual attention blocks, in which each standard three-dimensional convolution is approximated as a cascaded two-dimensional spatial convolution and a one-dimensional temporal convolution, and then a dual attention mechanism is built by embedding a channel attention sub-module and a spatial attention sub-module sequentially in each residual block, thereby acquiring more discriminative features to improve the model guidance capability. Furthermore, a multi-stage training strategy is used for Sep-3D RAN training, which can relieve the over-fitting effectively. Finally, experimental results demonstrate that the performance of Sep-3D RAN can surpass the existing state-of-the-art methods.",Human computer interaction; Human action recognition; Residual network; Attention mechanism; Multi-stage training strategy
"The liver tumor is one of the most foremost critical causes of death in the world. Nowadays, Medical Imaging (MI) is one of the prominent Computer Vision fields (CV), which helps physicians and radiologists to detect and diagnose liver tumors at an early stage. Radiologists and physicians use manual or semi-automated systems to read hundreds of images, such as Computed Tomography (CT) for the diagnosis. Therefore, there is a need for a fully-automated method to diagnose and detect the tumor early using the most popular and widely used imaging modality, CT images. The proposed work focuses on the Machine Learning (ML) methods: Random Forest (RF), J48, Logistic Model Tree (LMT), and Random Tree (RT) with multiple automated Region of Interest (ROI) for multiclass liver tumor classification. The dataset comprises four tumor classes: hemangioma, cyst, hepatocellular carcinoma, and metastasis. Converted the images into gray-scale, and the contrast of images was improved by applying histogram equalization. The noise was reduced using the Gabor filter, and image quality was improved by applying an image sharpening algorithm. Furthermore, 55 features were acquired for each ROI of different pixel dimensions using texture, binary, histogram and rotational, scalability, and translational (RST) techniques. The correlation-based feature selection (CFS) technique was deployed to obtain 20 optimized features from these 55 features for classification. The results showed that RF and RT performed better than J48 and LMT, with an accuracy of 97.48% and 97.08%, respectively. The proposed novel framework will help radiologists and physicians better diagnose liver tumors.",
"In this paper we propose a deep learning based approach for image retrieval using EEG. Our approach makes use of a multi-modal deep neural network based on metric learning, where the EEG signal from a user observing an image is mapped together with visual information extracted from the image. The inspiration behind this work is the vision of a system which allows the user to navigate their image catalogue just by thinking about the image they want to see. Thanks to our metric learning approach, the system is scalable in that it can operate with new images that have never been used in training, resulting in a zero-shot image retrieval system. This framework is tested in two different standard EEG image-viewing datasets, where we demonstrate state-of-the-art results in this complex scenario.Crown Copyright (c) 2022 Published by Elsevier B.V. All rights reserved.",Brain-Computer Interfaces; Metric learning; Computer vision; EEG
"Information is obtained from human eyes for thinking divergence, and further associated with computer equipment, so human beings endow computers with the ability of vision to convey and feel information. This field has developed for many years, and many aspects can be in line with other research directions, such as artificial intelligence, which has become popular in recent years, and pattern recognition, which has been applied a lot. In order to sort out the structure and content of multitarget recognition smoothly, this paper starts from the perspective of shallow vision, uses theory and practical experiments, and chooses the core technology with the largest weight from massive computer technologies, so that the recognition algorithm can compare with the recognition algorithm. The research shows that (1) CNN shows its unique feature ability and incomparable detection accuracy from many models, and the error rate can be reduced from 28.07% to 18.40%. (2) The method of candidate region is complex, and the larger the region, the more difficult it is to calculate. The method based on regression is far beyond it in both precision and speed and is more suitable for the research of this subject. (3) When the mAP increases, the speed is forced to slow down. If the image resolution is high with the same model, the mAP will be high (SSD and YOLO models are often used). Experiments show that the recognition effect is obvious. At the end of the article, the advantages and disadvantages of this study are summarized. In the field of computer vision, people need to do more in-depth research. Follow-up can optimize multitarget recognition and detection and strive to improve the accuracy.",
"We investigate and analyze methods to violence detection in this study to completely disassemble the present condition and anticipate the emerging trends of violence discovery research. In this systematic review, we provide a comprehensive assessment of the video violence detection problems that have been described in state-of-the-art researches. This work aims to address the problems as state-of-the-art methods in video violence detection, datasets to develop and train real-time video violence detection frameworks, discuss and identify open issues in the given problem. In this study, we analyzed 80 research papers that have been selected from 154 research papers after identification, screening, and eligibility phases. As the research sources, we used five digital libraries and three high ranked computer vision conferences that were published between 2015 and 2021. We begin by briefly introducing core idea and problems of video-based violence detection; after that, we divided current techniques into three categories based on their methodologies: conventional methods, end-to-end deep learning-based methods, and machine learning-based methods. Finally, we present public datasets for testing video based violence detectionmethods' performance and compare their results. In addition, we summarize the open issues in violence detection in videoand evaluate its future tendencies.",Violence detection; Machine learning; Deep learning; Computer vision; Artificial intelligence; Video features; Datasets
"Fractional calculus is an abstract idea exploring interpretations of differentiation having non-integer order. For a very long time, it was considered as a topic of mere theoretical interest. However, the introduction of several useful definitions of fractional derivatives has extended its domain to applications. Supported by computational power and algorithmic representations, fractional calculus has emerged as a multifarious domain. It has been found that the fractional derivatives are capable of incorporating memory into the system and thus suitable to improve the performance of locality-aware tasks such as image processing and computer vision in general. This article presents an extensive survey of fractional-order derivative-based techniques that are used in computer vision. It briefly introduces the basics and presents applications of the fractional calculus in six different domains viz. edge detection, optical flow, image segmentation, image de-noising, image recognition, and object detection. The fractional derivatives ensure noise resilience and can preserve both high and low-frequency components of an image. The relative similarity of neighboring pixels can get affected by an error, noise, or non-homogeneous illumination in an image. In that case, the fractional differentiation can model special similarities and help compensate for the issue suitably. The fractional derivatives can be evaluated for discontinuous functions, which help estimate discontinuous optical flow. The order of the differentiation also provides an additional degree of freedom in the optimization process. This study shows the successful implementations of fractional calculus in computer vision and contributes to bringing out challenges and future scopes.(c) 2022 Elsevier B.V. All rights reserved.",Fractional-order derivative; Computer vision; Image processing
"Road pavements are subject to various forms of degradation compromising their functionality with negative effects on safety. For assuring the highest quality, all the distresses have to be properly identified and quantified by road administrators. For increasing efficiency and reducing costs and times of surveys, several innovative methods to detect, classify and measure surface distresses were proposed, with variable results. In this context, the authors propose an algorithm for automated pothole detection through the processing of 3D data of pavement surfaces, acquired using an innovative high-performance equipment. The algorithm, derived from computer vision, is able of identifying potholes in road sections, assuring a reliable estimation of shape and severity, in terms not only of area, perimeter, but also depth, with practical benefits. The numerical results show the remarkable performance of the proposed algorithm, even compared to alternative traditional methodologies. In terms of Precision, Recall and F-Score, it assures mean values equal respectively to 89.75%, 92.95% 91.28%. Validation was also performed in terms of area error rate, with an average value of 5.15%, significantly lower than other approaches. Then, the algorithm represents a reliable alternative to traditional approaches and allows road administrators to derive data to optimize maintenance and road functionality.",Pavement potholes; automatic distress detection; 3D data; pavement quality; computer vision
"The self-attention mechanism has been widely used to capture long-range relationships in various computer-vision tasks and is designed to update the representation of each pixel using a weighted sum of the features of all pixels in an image. However, it is computationally expensive, due to its potentially large matrix multiplication. It also does not make full use of position information, although this is crucial for modeling position-dependent interactions. To deal with these problems, the commonly used dot-product similarity is replaced by a position-aware similarity in this paper, which is introduced as a metric to evaluate the correlation between any two spatial positions. Then, on the basis of an axial decomposition operation, two concise and lightweight variants of self-attention are carefully constructed in sequence, namely the axial attention module and the complete decomposition module. The former decomposes only the first matrix multiplication of the self-attention mechanism, but the latter decomposes the entire process. Detailed experiments conducted on a real-world dataset of printed circuit board surface defects demonstrate the effectiveness and efficiency of the two variants. Their performance is comparable to that of state-of-the-art methods, and their computational costs is lower, which suggests that they could be widely utilized in various industrial inspection tasks based on computer vision.",Defect detection; Visual inspection; Deep learning; Segmentation network; Computer vision
"Manual parsing of invoices is a tedious, arduous and error-prone task. Due to the academic and business importance of this problem, it has attracted the attention of machine learning enthusiasts. There are several complexities and challenges in the automated parsing of invoices. Some of them include a paucity of useful datasets, eclectic template formats, and poor performance of algorithms in real life scenarios. This problem can be solved by the automatic traversal of the invoices by object detection algorithms such as YOLO, SSD and R-CNN. These state-of-the-art algorithms will be trained to detect various fields or entities present in an invoice. In this paper, a dataset of 315 invoices has been generated using web testing tools. The dataset has been annotated for eight entities: billing address, shipping address, invoice date, invoice number, product name, price, quantity, and total amount. The text boxes detected by the models is converted to machine encoded text, using text extraction methods such as Optical Character Recognition (OCR). Hyperparameter tuning has been performed to improve model accuracy. The models have been evaluated on myriad metrics such as mean Average Precision (mAP), common objects in context (COCO) evaluation metrics and total loss during training and validation. The loss vs iteration graph has been visualized using Tensorboard. A front-end application encapsulates all the functions of the research paper and allows testing of various models.",Document parsing; Computer vision; Object detection; Transfer learning; Optical character recognition; Region-based convolutional neural network
"Recently, many semantic segmentation methods based on fully supervised learning are leading the way in the computer vision field. In particular, deep neural networks headed by convolutional neural networks can effectively solve many challenging semantic segmentation tasks. To realize more refined semantic image segmentation, this paper studies the semantic segmentation task with a novel perspective, in which three key issues affecting the segmentation effect are considered. Firstly, it is hard to predict the classification results accurately in the high-resolution map from the reduced feature map since the scales are different between them. Secondly, the multi-scale characteristics of the target and the complexity of the background make it difficult to extract semantic features. Thirdly, the problem of intra-class differences and inter-class similarities can lead to incorrect classification of the boundary. To find the solutions to the above issues based on existing methods, the inner connection between past research and ongoing research is explored in this paper. In addition, qualitative and quantitative analyses are made, which can help the researchers to establish an intuitive understanding of various methods. At last, some conclusions about the existing methods are drawn to enhance segmentation performance. Moreover, the deficiencies of existing methods are researched and criticized, and a guide for future directions is provided.",Semantic segmentation; Supervised learning; Computer vision; Convolutional neural networks
"Using techniques derived from the syntactic methods for visual pattern recognition is not new and was much explored in the area called syntactical or structural pattern recognition. Syntactic methods have been useful because they are intuitively simple to understand and have transparent, interpretable, and elegant representations. Their capacity to represent patterns in a semantic, hierarchical, compositional, spatial, and temporal way have made them very popular in the research community. In this article, we try to give an overview of how syntactic methods have been employed for computer vision tasks. We conduct a systematic literature review to survey the most relevant studies that use syntactic methods for pattern recognition tasks in images and videos. Our search returned 597 papers, of which 71 papers were selected for analysis. The results indicated that in most of the studies surveyed, the syntactic methods were used as a high-level structure that makes the hierarchical or semantic relationship among objects or actions to perform the most diverse tasks.",Computer vision; image representation; formal languages; syntacticmethods; pattern recognition
"Multi-task learning in Convolutional Neural Networks (CNNs) has led to remarkable success in a variety of applications of computer vision. Towards effective multi-task CNN architectures, recent studies automatically learn the optimal combinations of task-specific features at single network layers. However, they generally learn an unchanged operation of feature combination after training, regardless of the characteristic changes of task-specific features across different inputs. In this paper, we propose a novel Adaptive Feature Aggregation (AFA) layer for multi-task CNNs, in which a dynamic aggregation mechanism is designed to allow each task adaptively determines the degree to which the knowledge sharing or preserving between tasks is needed based on the characteristics of inputs. We introduce two types of aggregation modules to the AFA layer, which realize the adaptive feature aggregation by capturing the feature dependencies of different tasks along the channel and spatial axes, respectively. The AFA layer is a plug-and-play component with low parameter and computation overheads, and can be trained end-to-end along with backbone networks. For both pixel-level and image-level tasks, we empirically show that our approach substantially outperforms the previous state-of-the-art methods of multi-task CNNs. The code and models are available at https://github.com/zhenshen-mla/AFANet.",Task analysis; Training; Visualization; Convolution; Residual neural networks; Feature extraction; Computer vision; Multi-task learning; convolutional neural networks; adaptive feature aggregation; attention mechanism
"Street-level imagery holds a significant potential to scale-up in-situ data collection. This is enabled by combining the use of cheap high-quality cameras with recent advances in deep learning compute solutions to derive relevant thematic information. We present a framework to collect and extract crop type and phenological information from street level imagery using computer vision. Monitoring crop phenology is critical to assess gross primary productivity and crop yield. During the 2018 growing season, high-definition pictures were captured with side looking action cameras in the Flevoland province of the Netherlands. Each month from March to October, a fixed 200-km route was surveyed collecting one picture per second resulting in a total of 400,000 geo-tagged pictures. At 220 specific parcel locations, detailed on the spot crop phenology observations were recorded for 17 crop types (including bare soil, green manure, and tulips): bare soil, carrots, green manure, grassland, grass seeds, maize, onion, potato, summer barley, sugar beet, spring cereals, spring wheat, tulips, vegetables, winter barley, winter cereals and winter wheat. Furthermore, the time span included specific pre-emergence parcel stages, such as differently cultivated bare soil for spring and summer crops as well as post-harvest cultivation practices, e.g. green manuring and catch crops. Classification was done using TensorFlow with a well-known image recognition model, based on transfer learning with convolutional neural network (MobileNet). A hypertuning methodology was developed to obtain the best performing model among 160 models. This best model was applied on an independent inference set discriminating crop type with a Macro F1 score of 88.1% and main phenological stage at 86.9% at the parcel level. Potential and caveats of the approach along with practical considerations for implementation and improvement are discussed. The proposed framework speeds up high quality in-situ data collection and suggests avenues for massive data collection via automated classification using computer vision.",Phenology; Plant recognition; Agriculture; Computer vision; Deep learning; Remote sensing; CNN; BBCH; Crop type; Street view imagery; Survey; In-situ; Earth observation; Parcel; In situ
"Early detection and diagnosis of COVID-19, as well as the exact separation of non-COVID-19 cases in a non-invasive manner in the earliest stages of the disease, are critical concerns in the current COVID-19 pandemic. Convolutional Neural Network (CNN) based models offer a remarkable capacity for providing an accurate and efficient system for the detection and diagnosis of COVID-19. Due to the limited availability of RT-PCR (Reverse transcription-polymerase Chain Reaction) tests in developing countries, imaging-based techniques could offer an alternative and affordable solution to detect COVID-19 symptoms. This paper reviewed the current CNN-based approaches and investigated a custom-designed CNN method to detect COVID-19 symptoms from CT (Computed Tomography) chest scan images. This study demonstrated an integrated method to accelerate the process of classifying CT scan images. In order to improve the computational time, a hardware-based acceleration method was investigated and implemented on a reconfigurable platform (FPGA). Experimental results highlight the difference between various approximations of the design, providing a range of design options corresponding to both software and hardware. The FPGA-based implementation involved a reduced pre-processed feature vector for the classification task, which is a unique advantage of this particular application. To demonstrate the applicability of the proposed method, results from the CPU-based classification and the FPGA were measured separately and compared retrospectively.",Convolutional Neural Networks (CNN); computer vision; reconfigurable architectures; intelligent system design; COVID-19; embedded devices
"Vehicle re-identification (re-id) is an essential task in the field of intelligent transportation systems (ITS). The main goal of re-id is to find the same vehicle in different scenarios, which can is still a challenging task in both ITS and computer vision (CV). The existing vehicle re-identification methods simply combine the coarse-grained and the fine-grained attributes together with multi-task training. However, such combination may still have limited performance in vehicles with trivial appearance differences, or with rare models and colors. To solve this problem, we propose a simple yet effective framework, called dual domain multi-task model (DDM), that divides the vehicle images into two domains based on the frequency. And then two parallel branches are proposed to recover the two domains. Furthermore, a multitask method is proposed, which combines the classification loss in color and model together with triplet loss for fine-grained distance measurement. Besides, a progressive strategy is used in the training process. Two public datasets, PKU VehiclelD and VeRi are used to validate the proposed DDM. The experimental results demonstrate that the proposed approach outperforms the existing methods on both datasets.",Computer vision; information security; metric learning; vehicle re-identification
"Image dehazing, as a common solution to weather-related degradation, holds great promise for photography, computer vision, and remote sensing applications. Diverse approaches have been proposed throughout decades of development, and deep-learning-based methods are currently predominant. Despite excellent performance, such computationally intensive methods as these recent advances amount to overkill, because image dehazing is solely a preprocessing step. In this paper, we utilize an autonomous image dehazing algorithm to analyze a non-deep dehazing approach. After that, we present a corresponding FPGA design for high-quality real-time vision systems. We also conduct extensive experiments to verify the efficacy of the proposed design across different facets. Finally, we introduce a method for synthesizing cloudy images (loosely referred to as hazy images) to facilitate future aerial surveillance research.",image dehazing; automation; FPGA; synthetic cloud; haze generation; aerial surveillance
"Due to the advantages of economics, safety, and efficiency, vision-based analysis techniques have recently gained conspicuous advancements, enabling them to be extensively applied for autonomous constructions. Although numerous studies regarding the defect inspection and condition assessment in underground sewer pipelines have presently emerged, we still lack a thorough and comprehensive survey of the latest developments. This survey presents a systematical taxonomy of diverse sewer inspection algorithms, which are sorted into three categories that include defect classification, defect detection, and defect segmentation. After reviewing the related sewer defect inspection studies for the past 22 years, the main research trends are organized and discussed in detail according to the proposed technical taxonomy. In addition, different datasets and the evaluation metrics used in the cited literature are described and explained. Furthermore, the performances of the state-of-the-art methods are reported from the aspects of processing accuracy and speed.",survey; computer vision; defect inspection; condition assessment; sewer pipes
"Hypothesis pruning is an important prerequisite while working with outlier-contaminated data in many computer vision problems. However, the underlying random data structures are barely explored in the literature, limiting designing efficient algorithms. To this end, we provide a novel graph-theoretic perspective on hypothesis pruning exploiting invariant structures of data. We introduce the planted clique model, a central object in computational statistics, to investigate the information-theoretical and computational limits of the hypothesis pruning problem. In addition, we propose an inductive learning framework for finding hidden cliques that learns heuristics on synthetic graphs with planted cliques and generalizes to real vision problems. We present competitive experimental results with large runtime improvement on synthetic and widely used vision datasets to show its efficacy.",Deep learning for visual perception; aerial systems; perception and autonomy; perception for grasping and manipulation
"We consider the field of three-dimensional technical vision and in particular three-dimensional recognition. The problems of three-dimensional vision are singled out, and methods for obtaining and presenting three-dimensional data, as well as applications of three-dimensional vision, are reviewed. Deep learning methods in 3D recognition problems are surveyed. The main modern trends in this field are revealed. So far, quite a few neural network architectures, convolutional layers, sampling, pooling, and aggregation operations, and methods for representing and processing three-dimensional input data have been proposed. The field is under active development, with the greatest variety of methods being presented for point clouds.",3D recognition; deep learning; computer vision
"The view planning (VP) problem in robotic active vision enables a robot system to automatically perform object reconstruction tasks. Lacking prior knowledge, next-best-view (NBV) methods are typically used to plan a view sequence, with the goal of covering as many object surface areas as possible in an unknown environment. However, such methods have two problems: (1) they are unable to perform global path planning; and (2) the reconstruction process is inefficient because of time-consuming ray casting and high movement cost. We propose a neural network, SCVP, to pre-learn prior knowledge via set covering (SC) based training so as to achieve one-shot view planning. The SCVP network takes the volumetric occupancy grid as input and directly predicts a small (ideally minimum) number of views that cover all surface areas. Given object 3D models as a priori geometric knowledge, the training dataset is automatically labeled by the set covering optimization method. We propose a global path planning method to reconstruct objects without redundant movement. Comparative experiments on multiple datasets of 3D models show that, under the condition of similar or better surface coverage, the proposed method can outperform state-of-the-art NBV methods in terms of movement cost and inference time. Real-world experiments confirm that the proposed method can achieve faster object reconstruction than other methods.",Autonomous agents; deep learning for visual perception; motion and path planning; data sets for robotic vision; computer vision for automation
"Human face image analysis using machine learning is an important element in computer vision. The human face image conveys information such as age, gender, identity, emotion, race, and attractiveness to both human and computer systems. Over the last ten years, face analysis methods using machine learning have received immense attention due to their diverse applications in various tasks. Although several methods have been reported in the last ten years, face image analysis still represents a complicated challenge, particularly for images obtained from 'in the wild' conditions. This survey paper presents a comprehensive review focusing on methods in both controlled and uncontrolled conditions. Our work illustrates both merits and demerits of each method previously proposed, starting from seminal works on face image analysis and ending with the latest ideas exploiting deep learning frameworks. We show a comparison of the performance of the previous methods on standard datasets and also present some promising future directions on the topic.",face analysis; computer vision; deep learning
"Correspondence-based point cloud registration is a cornerstone in robotics perception and computer vision, which seeks to estimate the best rigid transformation aligning two point clouds from the putative correspondences. However, due to the limited robustness of 3D keypoint matching approaches, outliers, probably in large numbers, are prone to exist among the correspondences, which makes robust registration methods imperative. Unfortunately, existing robust methods have their own limitations (e.g. high computational cost or limited robustness) when facing high or extreme outlier ratios, probably unsuitable for practical use. In this letter, we present a novel, fast, deterministic and guaranteed robust solver, named TriVoC (Triple-layered Voting with Consensus maximization), for the robust registration problem. We decompose the selecting of the minimal 3-point sets into 3 consecutive layers, and in each layer we design an efficient voting and correspondence sorting framework on the basis of the pairwise equal-length constraint. In this manner, the 3-point sets can he selected independently from the reduced correspondence sets according to the sorted sequence, which can significantly lower the computational cost and meanwhile provide a strong guarantee to achieve the largest consensus set (as the final inlier set) as long as a probabilistic termination condition is fulfilled. Varied experiments show that our solver TriVoC is robust against up to 99% outliers, highly accurate, time-efficient even with extreme outlier ratios, and also practical for real-world applications, showing performance superior to other state-of-the-art competitors.",Computer vision for automation; RGB-D perception; point cloud registration; robust estimation
"Images taken under low light or dim backlight conditions usually have insufficient brightness, low contrast, and poor visual quality of the image, which leads to increased difficulty in computer vision and human recognition of images. Therefore, low illumination enhancement is very important in computer vision applications. We mainly provide an overview of existing deep learning enhancement algorithms in the low-light field. First, a brief overview of the traditional enhancement algorithms used in early low-light images is given. Then, according to the neural network structure used in deep learning and its learning algorithm, the enhancement methods are introduced. In addition, the datasets and common performance indicators used in the deep learning enhancement technology are introduced. Finally, the problems and future development of the deep learning enhancement method for low-light images are described. (C) 2022 Society of Photo-Optical Instrumentation Engineers (SPIE)",low-light; deep learning; image enhancement
"Infrared image target detection technology has been one of the essential research topics in computer vision, which has promoted the development of automatic driving, infrared guidance, infrared surveillance, and other fields. However, traditional target detection algorithms for infrared images have difficulty adapting to the target's multiscale characteristics. In addition, the accuracy of the detection algorithm is significantly reduced when the target is occluded. The corresponding solutions are proposed in this paper to solve these two problems. The final experiments show that this paper's infrared image target detection model improves significantly.",infrared image; deep learning; neural network; target detection; transfer learning; multiscale characteristics; context analysis
"Recent advances in deep learning have shown remarkable performance in road segmentation from remotely sensed images. However, these methods based on convolutional neural networks (CNNs) cannot obtain long-range dependency and global contextual information because of the intrinsic inductive biases. Motivated by the success of Transformer in computer vision (CV), excellent models based on Transformer are emerging endlessly. However, patches with a fixed scale limit the further improvement of the model performance. To address this problem, a dual-resolution road segmentation network (DCS-TransUperNet) with a features fusion module (FFM) was proposed for road segmentation. Firstly, the encoder of DCS-TransUperNet was designed based on CSwin Transformer, which uses dual subnetwork encoders of different scales to obtain the coarse and fine-grained feature representations. Secondly, a new FFM was constructed to build enhanced feature representation with global dependencies, using different scale features from the subnetwork encoders. Thirdly, a mixed loss function was designed to avoid the local optimum caused by the imbalance between road and background pixels. Experiments using the Massachusetts dataset and DeepGlobe dataset showed that the proposed DCS-TransUperNet could effectively solve the discontinuity problem and preserve the integrity of the road segmentation results, achieving a higher IoU (65.36% on Massachusetts dataset and 56.74% on DeepGlobe) of road segmentation compared to other state-of-the-art methods. The considerable performance also proves the powerful generation ability of our method.",remote sensing image; road segmentation; CSwin Transformer; dual scales; long-range contextual dependencies
"Applying advanced technologies such as computer vision is highly desirable in seed testing. Among testing needs, computer vision is a feasible technology for conducting seed and seedling classification used in purity analysis and in germination tests. This review focuses on seed identification that currently encounters extreme challenges due to a shortage of expertise, time-consuming training and operation, and the need for large numbers of reference specimens. The reviewed computer vision techniques and application strategies also apply to other methods in seed testing. The review describes the development of machine learning-based computer vision in automating seed identification and their limitations in feature extraction and accuracy. As a subset of machine learning techniques, deep learning has been applied successfully in many agricultural domains, which presents potential opportunities for its application in seed identification and seed testing. To facilitate application in seed testing, the challenges of deep learning-based computer vision systems are summarised through analysing their application in other agricultural domains. It is recommended to accelerate the application in seed testing by optimising procedures or approaches in image acquisition technologies, dataset construction and model development. A concept flow chart for using computer vision systems is proposed to advance computer-assisted seed identification.",artificial intelligence; computer vision; dataset construction; deep learning; image analysis; machine learning; seed identification; seed testing
"The development of computer vision technology is rapid, which supports the automatic quality control of precision components efficiently and reliably. This paper focuses on the application of computer vision technology in manufacturing quality control. A new deep learning algorithm is presented, Multi-angle projective Generative Adversarial Networks (MapGANs), to automatically generate 3D visualization models of products and components. The generated 3D visualization models can intuitively and accurately display the product parameters and indicators. Based on these indicators, our model can accurately determine whether the product meets the standard. The working principle of the MapGANs algorithm is to automatically infer the basic three-dimensional shape distribution through the product's projection module, while using multiple angles and multiple views to improve the fineness and accuracy of the three-dimensional visualization model. The experimental results prove that MapGANs can effectively reconstruct two-dimensional images into three-dimensional visualization models, and meanwhile accurately predict whether the quality of the product meets the standard.",3D visualization model; Neural network; Generation adversarial network; Precision components
"Human gaze estimation plays a major role in many applications in human-computer interaction and computer vision by identifying the users' point-of-interest. Revolutionary developments of deep learning have captured significant attention in gaze estimation literature. Gaze estimation techniques have progressed from single-user constrained environments to multi-user unconstrained environments with the applicability of deep learning techniques in complex unconstrained environments with extensive variations. This paper presents a comprehensive survey of the single-user and multi-user gaze estimation approaches with deep learning. State-of-the-art approaches are analyzed based on deep learning model architectures, coordinate systems, environmental constraints, datasets and performance evaluation metrics. A key outcome from this survey realizes the limitations, challenges and future directions of multi-user gaze estimation techniques. Furthermore, this paper serves as a reference point and a guideline for future multi-user gaze estimation research.",Computer vision; Gaze estimation; Deep learning; Eye tracking
"HRNet (High-Resolution Networks) as reported by Sun et al. (in: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (CVPR), 2019) has been the state-of-the-art human pose estimation method, benefitting from its parallel high-resolution designed network structures. However, HRNet is still a typical CNN (Convolutional Neural Networks) architecture, with local convolution operations. Recently, Transformers have been successfully applied in many computer vision areas. The main mechanism in Transformers is self-attention, which can learn global or long-range dependencies among different parts. In this paper, we propose a human pose estimation framework built upon High-Resolution Multi-scale Transformers, termed MTPose. We combine the two advantages of high-resolution and Transformers together to improve the performance. Specifically, we design a sub-network, MTNet (Multi-scale Transformers-based high-resolution Networks), which consists of two parallel branches. One is high-resolution with convolutional local operations, named as local branch. The other is the global branch utilizing multi-scale Transformer encoders to learn long-range dependencies of the whole body keypoints. At the end of the networks, the two branches are integrated together to predict the final keypoint heatmaps. Experiments on two benchmark datasets, the MSCOCO keypoint detection dataset and MPII human pose dataset, demonstrate that our method can significantly improve the state-of-the-art human pose estimation methods. Code will be available at: https://github.com/fudiGeng/MTPose.",Human pose estimation; High-resolution networks; Multi-scale transformers; Multi-scale self-attention
"The discipline of computer vision is becoming more popular as a research subject. In a surveillance-based computer vision application, item identification and tracking are the core procedures. They consist of segmenting and tracking an object of interest from a sequence of video frames, and they are both performed using computer vision algorithms. In situations when the camera is fixed and the backdrop remains constant, it is possible to detect items in the background using more straightforward methods. Aerial surveillance, on the other hand, is characterized by the fact that the target, as well as the background and video camera, are all constantly moving. It is feasible to recognize targets in the video data captured by an unmanned aerial vehicle (UAV) using the mean shift tracking technique in combination with a deep convolutional neural network (DCNN). It is critical that the target detection algorithm maintains its accuracy even in the presence of changing lighting conditions, dynamic clutter, and changes in the scene environment. Even though there are several approaches for identifying moving objects in the video, background reduction is the one that is most often used. An adaptive background model is used to create a mean shift tracking technique, which is shown and implemented in this work. In this situation, the background model is provided and updated frame-by-frame, and therefore, the problem of occlusion is fully eliminated from the equation. The target tracking algorithm is fed the same video stream that was used for the target identification algorithm to work with. In MATLAB, the works are simulated, and their performance is evaluated using image-based and video-based metrics to establish how well they operate in the real world.",
"Progress in the digitization of cultural assets leads to online databases that become too large for a human to analyze. Moreover, some analyses might be challenging, even for experts. In this paper, we explore two applications of computer vision to analyze historical data: watermark recognition and one-shot repeated pattern detection in artwork collections. Both problems present computer vision challenges which we believe to be representative of the ones encountered in cultural heritage applications: limited supervision is available, the tasks are fine-grained recognition, and the data comes in several different modalities. Both applications are also highly practical, as recognizing watermarks makes it possible to date and locate documents, while detecting repeated patterns allows exploring visual links between artworks. We demonstrate on both tasks the benefits of relying on deep mid-level features. More precisely, we define an image similarity score based on geometric verification of mid-level features and show how spatial consistency can be used to fine-tune out-of-the-box features for the target dataset with weak or no supervision. This paper relates and extends our previous works (Shen et al. in Discovering visual patterns in art collections with spatially-consistent feature learning, 2019; Shen et al. in Large-scale historical watermark recognition dataset and a new consistency-based approach, 2020). Our code and data are available at http://imagine.enpc.fr/similar to shenx/HisImgAnalysis/.",Feature learning; Self-supervised learning; Artwork analysis; Watermark recognition
"In computer vision, edge and object contour detection is essential for higher-level vision tasks, such as shape matching, visual salience, image segmentation, and object recognition. It has attracted much atten-tion during the past several decades, and many excellent methods have been proposed. In this paper, we make a comprehensive introduction to representative edge and object contour detection methods in the past two decades. Based on the development of these methods, we mainly classify them into two cate-gories: traditional methods and learning-based methods. We further divide traditional methods into local pattern methods, edge grouping methods, active contour models, and bio-inspired methods. Further, we divide learning-based methods into classical learning-based methods and deep learning-based methods. At the same time, we introduce the most popular benchmarks and evaluation measures and quantita-tively compare the performances of these promising methods. Moreover, we discuss current challenges in edge and object contour detection and suggest some future trends to bridge gaps with human vision. We believe that this overview will benefit newcomers and promote the development of edge and object contour detection.(c) 2022 Elsevier B.V. All rights reserved.",Edge detection; Contour detection; Convolutional neural network; Non-classical receptive field; Edge grouping; Simple cells
"With the rapid development of social economy and the extensive and in-depth development of national fitness activities, national physical fitness monitoring and research work has achieved rapid development. In recent years, the application of deep learning technology has also achieved research breakthroughs in the field of computer vision. How deep learning technology can effectively capture motion information in sample data and use it to realize the recognition and classification of human actions is currently a research hot spot. Today's popularization of various shooting devices such as mobile phones and portable action cameras has contributed to the vigorous growth of image data. Therefore, through computer vision technology, image data is widely used in practical application scenarios of human feature recognition. This paper proposes a deep learning network based on the recognition of human body feature changes in sports, improves the recognition method, and compares the recognition accuracy with the original method. The experimental results of this paper show that the result of this paper is 1.68% higher than the original recognition method, the accuracy rate of the improved motion history image is increased by 14.8%, and the overall recognition rate is higher. It can be seen from the above experimental results that this method has achieved good results in human body action recognition.",
"This paper outlines a novel advanced framework that combines structurized knowledge and visual models-Computational Knowledge Vision. In advanced studies of image and visual perception, a visual model's understanding and reasoning ability often determines whether it works well in complex scenarios. This paper presents the state-of-the-art mainstream of vision models for visual perception. This paper then proposes a concept and basic framework of Computational Knowledge Vision that extends the knowledge engineering methodology to the computer vision field. In this paper, we first retrospect prior work related to Computational Knowledge Vision in the light of the connectionist and symbolist streams. We discuss neural network models, meta-learning models, graph models, and Transformer models in detail. We then illustrate a basic framework for Computational Knowledge Vision, whose essential techniques include structurized knowledge, knowledge projection, and conditional feedback. The goal of the framework is to enable visual models to gain the ability of representation, understanding, and reasoning. We also describe in-depth works in Computational Knowledge Vision and its extensions in other fields.",Computer vision; Knowledge engineering; Deep learning; Graph learning; Meta-learning; Transformer; Artificial intelligence (AI)
"With the development of the Internet of Things, the application of computer vision on mobile phones is becoming more and more extensive and people have higher and higher requirements for the timeliness of the recognition results returned and the processing capabilities of the mobile phone for image recognition. However, the processing capability and storage capability of the user terminal equipment cannot meet the needs of identifying and storing a large number of pictures, and the data transmission process will cause high energy consumption of the terminal equipment. At the same time, multisource deep transfer learning has outstanding performance in computer vision and image classification. However, due to the huge amount of calculation of the deep network model, it is impossible to use the existing excellent network model to realize image recognition and classification on the mobile terminal. In order to solve the abovementioned problems, we propose a multisource mobile transfer learning algorithm based on dynamic model compression, this algorithm considers the realization of multisource transfer learning computing in the case of multiple mobile device computing source domains, and the method also guarantees data privacy and security for each device (origin domain). Meanwhile, extensive experiments show that our method can achieve remarkable results in popular image classification datasets.",
"Aiming at the problems of low detection accuracy and long detection time of existing image edge detection technologies, an image edge detection method of human-computer interaction interface based on machine vision technology is proposed. Based on machine vision technology, the image weight is calculated by iterative repeated weighted least square method, the image is Gaussian filtered by improved Canny algorithm, and the optimal threshold is calculated by iterative method to judge the effective edge. Through comparative experiments, it is proved that the maximum detection accuracy of the man-machine interface image edge enhancement detection method based on machine vision technology proposed in this paper is 100%, the detection time is always kept below 0.2S, and the fastest detection time is 0.1 s, which has wide applicability.",Machine vision technology; Image edge enhancement detection; Canny algorithm; Least square method; Gaussian filtering
"Automatic recognition of the eye states is essential for diverse computer vision applications related to drowsiness detection, facial emotion recognition (FER), human-computer interaction (HCI), etc. Existing solutions for eye state detection are either parameter intensive or suffer from a low recognition rate. This paper presents the design and implementation of a vision-based system for real-time eye state recognition on a resource-constrained embedded platform to tackle these issues. The designed system uses an ensemble of two lightweight convolutional neural networks (CNN), each trained to extract relevant information from the eye patches. We adopted transfer-learning-based fine-tuning to overcome the over-fitting issues when training the CNNs on small sample eye state datasets. Once trained, these CNNs are integrated and jointly fine-tuned to achieve enhanced performance. Experimental results manifest the effectiveness of the proposed eye state recognizer that is robust and computationally efficient. On the ZJU dataset, the proposed DCNNE model delivered the state-of-the-art recognition accuracy of 97.99% and surpassed the prior best recognition accuracy of 97.20% by 0.79%. The designed model also achieved competitive results on the CEW and MRL datasets. Finally, the designed CNNs are optimized and ported on two different embedded platforms for real-world applications with real-time performance. The complete system runs at 62 frames per second (FPS) on an Nvidia Xavier device and 11 FPS on a low-cost Intel NCS2 embedded platform using a frame size of 640 x 480 pixels resolution.",Eye state recognition; Deep learning; Ensemble; Transfer learning; CNN; Real-time embedded implementation
"Transformers have recently lead to encouraging progress in computer vision. In this work, we present new baselines by improving the original Pyramid Vision Transformer (PVT v1) by adding three designs: (i) a linear complexity attention layer, (ii) an overlapping patch embedding, and (iii) a convolutional feed-forward network. With these modifications, PVT v2 reduces the computational complexity of PVT v1 to linearity and provides significant improvements on fundamental vision tasks such as classification, detection, and segmentation. In particular, PVT v2 achieves comparable or better performance than recent work such as the Swin transformer. We hope this work will facilitate state-of-the-art transformer research in computer vision. Code is available at https://github.com/whai362/PVT.",transformers; dense prediction; image classification; object detection; semantic segmentation
"Global encoding of visual features in video captioning is important for improving the description accuracy. In this paper, we propose a video captioning method that combines Vision Transformer (ViT) and reinforcement learning. Firstly, Resnet-152 and ResNeXt-101 are used to extract features from videos. Secondly, the encoding block of the ViT network is applied to encode video features. Thirdly, the encoded features are fed into a Long Short-Term Memory (LSTM) network to generate a video content description. Finally, the accuracy of video content description is further improved by fine-tuning reinforcement learning. We conducted experiments on the benchmark dataset MSR-VTT used for video captioning. The results show that compared with the current mainstream methods, the model in this paper has improved by 2.9%, 1.4%, 0.9% and 4.8% under the four evaluation indicators of LEU-4, METEOR, ROUGE-L and CIDEr-D, respectively.",Video captioning; Vision transformer; Reinforcement learning; Long short-term memory network; Computer vision; Natural language processing; Attention mechanism; Encode-decode; Deep learning
"Humans can naturally and effectively find salient regions in complex scenes. Motivated by this observation, attention mechanisms were introduced into computer vision with the aim of imitating this aspect of the human visual system. Such an attention mechanism can be regarded as a dynamic weight adjustment process based on features of the input image. Attention mechanisms have achieved great success in many visual tasks, including image classification, object detection, semantic segmentation, video understanding, image generation, 3D vision, multimodal tasks, and self-supervised learning. In this survey, we provide a comprehensive review of various attention mechanisms in computer vision and categorize them according to approach, such as channel attention, spatial attention, temporal attention, and branch attention; a related repository https://github.com/MenghaoGuo/Awesome-Vision-Attentions is dedicated to collecting related work. We also suggest future directions for attention mechanism research.",attention; transformer; computer vision; deep learning; salience
"Single-image depth estimation represents a longstanding challenge in computer vision and although it is an ill-posed problem, deep learning enabled astonishing results leveraging both supervised and self-supervised training paradigms. State-of-the-art solutions achieve remarkably accurate depth estimation from a single image deploying huge deep architectures, requiring powerful dedicated hardware to run in a reasonable amount of time. This overly demanding complexity makes them unsuited for a broad category of applications requiring devices with constrained resources or memory consumption. To tackle this issue, in this paper a family of compact, yet effective CNNs for monocular depth estimation is proposed, by leveraging self-supervision from a binocular stereo rig. Our lightweight architectures, namely PyD-Net and PyD-Net2, compared to complex state-of-the-art trade a small drop in accuracy to drastically reduce the runtime and memory requirements by a factor ranging from 2x to 100x. Moreover, our networks can run real-time monocular depth estimation on a broad set of embedded or consumer devices, even not equipped with a GPU, by early stopping the inference with negligible (or no) loss in accuracy, making it ideally suited for real applications with strict constraints on hardware resources or power consumption.",Estimation; Feature extraction; Computer architecture; Cameras; Real-time systems; Hardware; Decoding; Computer vision; deep learning; deep architectures; unsupervised lea
"Underwater object detection is an essential step in image processing and it plays a vital role in several applications such as the repair and maintenance of sub-aquatic structures and marine sciences. Many computer vision-based solutions have been proposed but an optimal solution for underwater object detection and species classification does not exist. This is mainly because of the challenges presented by the underwater environment which mainly include light scattering and light absorption. The advent of deep learning has enabled researchers to solve various problems like protection of the subaquatic ecological environment, emergency rescue, reducing chances of underwater disaster and its prevention, underwater target detection, spooring, and recognition. However, the advantages and shortcomings of these deep learning algorithms are still unclear. Thus, to give a clearer view of the underwater object detection algorithms and their pros and cons, we proffer a state-of-the-art review of different computer vision-based approaches that have been developed as yet. Besides, a comparison of various state-of-the-art schemes is made based on various objective indices and future research directions in the field of underwater object detection have also been proffered.",Object detection; Light absorption; Light scattering; Deep-learning
"Artificial intelligence (AI) and computer vision (CV) methods become reliable to extract features from radiological images, aiding COVID-19 diagnosis ahead of the pathogenic tests and saving critical time for disease management and control. Thus, this review article focuses on cascading numerous deep learningbased COVID-19 computerized tomography (CT) imaging diagnosis research, providing a baseline for future research. Compared to previous review articles on the topic, this study pigeon-holes the collected literature very differently (i.e., its multi-level arrangement). For this purpose, 71 relevant studies were found using a variety of trustworthy databases and search engines, including Google Scholar, IEEE Xplore, Web of Science, PubMed, Science Direct, and Scopus. We classify the selected literature in multi-level machine learning groups, such as supervised and weakly supervised learning. Our review article reveals that weak supervision has been adopted extensively for COVID-19 CT diagnosis compared to supervised learning. Weakly supervised (conventional transfer learning) techniques can be utilized effectively for realtime clinical practices by reusing the sophisticated features rather than over-parameterizing the standard models. Few-shot and self-supervised learning are the recent trends to address data scarcity and model efficacy. The deep learning (artificial intelligence) based models are mainly utilized for disease management and control. Therefore, it is more appropriate for readers to comprehend the related perceptive of deep learning approaches for the in-progress COVID-19 CT diagnosis research.(c) 2022 Elsevier B.V. All rights reserved.",COVID-19 CT detection; COVID-19 CT diagnosis; Supervised learning; Weakly supervised learning; COVID-19 CT deep learning
"Drowsiness is a feeling of sleepiness before the sleep onset and has severe implications from a safety perspective for the individuals involved in industrial activities, mining, and driving. The state-of-the-art computer vision (CV) based drowsiness detection methods generally utilize multiple deep convolutional neural networks (DCNN) without investigating deep feature aggregation techniques for the drowsiness detection task. More importantly, the reported results are mostly based on acted drowsy data, making the utilization of models trained on such data highly arguable for detecting drowsiness in real-life situations. Towards ameliorating this, we first present a comprehensive real drowsy data curated from 50 subjects, where subjects are labeled as fresh or drowsy. Further, four DCNN models: Xception, ResNet101, InceptionV4, and ResNext101, are trained on our dataset using transfer learning to select a baseline model for our drowsiness detection method. Moreover, an experimental study is performed using five different pooling methods: global max, global average, generalized mean, region of interest, and Weibull activation, to compute a robust and discriminative global descriptor. Our results reveal that the parametric Weibull activation pooling is the best suited for aggregating deep convolutional features. Additionally, a low complexity model based on the MobileNetV2 is proposed for a deployable drowsiness detection solution in mobile devices. The detection accuracy of 93.80% and 90.50% is achieved using our proposed Weibull-based ResNext101 and MobileNetV2 models, respectively. Moreover, our results show that the proposed non-invasive method outperforms the polysomnography signals-based invasive drowsiness detection approach.",Drowsiness detection; Fatigue detection; Deep convolutional neural network; Parametric aggregation; CNN
"Dynamic compaction method (DCM) is currently one of the most commonly used foundation reinforcement techniques. However, manual monitoring is still the mainstream way of DCM tamping counting with low efficiency and high cost. This paper focuses on the tamping times of DCM, and proposes a non-contact Intelligent Monitoring Method for Tamping Times ((IMT2)-T-2) based on machine vision and pattern recognition technology. The hammer detection methods based on cooperative targets, YOLOv4 and YOLOv4-tiny are compared, and then the motion model based on the hammer position of construction image series is proposed and the vision-based full automatic measurement of tamping times is realized. Moreover, a field test was carried out to verify the applicability of above method. The results of the tamping times measurement indicate that the proposed method can measure the count of tamping under general working conditions with quite high accuracy.",Dynamic compaction; Intelligent monitoring method for tamping times ((IMT2)-T-2); Machine vision; Pattern recognition
"Automatic classification of different species of fish is important for the comprehension of marine ecology, fish behaviour analysis, aquaculture management, and fish health monitoring. In recent years, many automatic classification methods have been developed, among which machine vision-based classification methods are widely used with the advantages of being fast and non-destructive. In addition, the successful application of rapidly emerging deep learning techniques in machine vision has brought new opportunities for fish classification. This paper provides an overview of machine vision models applied in the field of fish classification, followed by a detailed discussion of specific applications of various classification methods. Furthermore, the challenges and future research directions in the field of fish classification are discussed. This paper would help researchers and practitioners to understand the applicability of machine vision in fish classification and encourage them to develop advanced algorithms and models to address the complex problems that exist in fish classification practice.",computer vision; fish classification; machine learning
"Computer vision-based crack detection is an effective technique for evaluating the structural safety of concrete building structures. Currently, the existing crack-detection methods based on machine learning often require pre-training or/and re-training the model, which is an experiential and complex task. In this study, based on sparse representation, we cast the crack damage detection problem by determining the outlier in the sparse correlation coefficients between the selected crack and the testing image regions. Specifically, by dividing one concrete image to be detected, we can obtain multiple testing image regions. Then, the spatial variation features of these image region contents are computed via discrete cosine transformation and are further used as the dictionary set. Considering that only a fraction of the dictionary set belongs to the cracks, the correlation coefficients of the selected known crack regions in the dictionary set should be sparse. Furthermore, a fast iterative shrinkage-thresholding algorithm (FISTA) was utilized to obtain the optimum sparse correlation coefficients. Finally, for the dictionary set, the atoms (i.e., regions) that have larger values in the sparse correlation coefficients are treated as outliers (i.e., cracks), and the 3 delta principle is exploited to identify these outliers. Experiments on a practical concrete image set show that the proposed algorithm is more accurate and efficient than traditional crack-detection methods.",crack detection; feature extraction; sparse representation; computer vision; FISTA
"The generation of natural language descriptions for a video has been reported by many researchers till now. But, it is still the most interesting research topic among the researchers due to the emerging interdisciplinary problem of Computer Vision (CV), Natural Language Processing (NLP) and Deep Learning (DL). The results of a video description are still not convincing due to the redundancy of a large number of similar frames in a video. In this paper, we propose dual-stage based text generation approach in which the first stage is for reducing redundancy due to the similar frames by processing selected sets of frames and keyframe from the shots of a video and in the second stage, the text generator module will generate relevant text for a video using the selected sets of frames and keyframes of each shot. In the first stage, a flexible novel shot boundary detection (SBD or temporal boundaries) approach is proposed which will segment the video into shots and then keyframe and set of frames are selected from each shot using frame selection policy. Then, the spatio-temporal features for each segment and 2D features for each keyframe are extracted respectively using the 3D convolutional network and VGG19. These features are passed to the next stage where these features are embedded with semantic concepts related to video and then text generation will take place using Long Short Term Memory (LSTM) recurrent network. The proposed approach is the amalgamation of classical and modern computer vision techniques. In the first stage, the Noise-Resistant Local Binary Pattern (NRLBP) feature is used for detecting illumination and motion invariant temporal boundaries in a video and processing keyframes and sets of frames for the further text generation. TRECVid 2001 and 2007 datasets are used to validate the exactness of the proposed SBD approach and MSR-VTT (Microsoft Research Video to Text ) and YouTube2text (MSVD) datasets are applied to analyze and validate the performance of proposed video to text generation approach.",Shot boundary detection; Illumination; Motion effect; Abrupt transition; Video captioning
"Background: Maintaining a healthy diet is vital to avoid health-related issues, e.g., undernutrition, obesity and many non-communicable diseases. An indispensable part of the health diet is dietary assessment. Traditional manual recording methods are not only burdensome but time-consuming, and contain substantial biases and errors. Recent advances in Artificial Intelligence (AI), especially computer vision technologies, have made it possible to develop automatic dietary assessment solutions, which are more convenient, less time-consuming and even more accurate to monitor daily food intake.Scope and approach: This review presents Vision-Based Dietary Assessment (VBDA) architectures, including multi-stage architecture and end-to-end one. The multi-stage dietary assessment generally consists of three stages: food image analysis, volume estimation and nutrient derivation. The prosperity of deep learning makes VBDA gradually move to an end-to-end implementation, which applies food images to a single network to directly estimate the nutrition. The recently proposed end-to-end methods are also discussed. We further analyze existing dietary assessment datasets, indicating that one large-scale benchmark is urgently needed, and finally highlight critical challenges and future trends for VBDA.Key findings and conclusions: After thorough exploration, we find that multi-task end-to-end deep learning approaches are one important trend of VBDA. Despite considerable research progress, many challenges remain for VBDA due to the meal complexity. We also provide the latest ideas for future development of VBDA, e.g., fine-grained food analysis and accurate volume estimation. This review aims to encourage researchers to propose more practical solutions for VBDA.",Dietary assessment; Computer vision; Deep learning; Food recognition; Food segmentation; Volume estimation
"The application of deep learning techniques to the detection and automated classification of Alzheimer's disease (AD) has recently gained considerable attention. The rapid progress in neuroimaging and sequencing techniques has enabled the generation of large-scale imaging genetic data for AD research. In this study, we developed a deep learning approach, IGnet, for automated AD classification using both magnetic resonance imaging (MRI) data and genetic sequencing data. The proposed approach integrates computer vision (CV) and natural language processing (NLP) techniques, with a deep three-dimensional convolutional network (3D CNN) being used to handle the three-dimensional MRI input and a Transformer encoder being used to manage the genetic sequence input. The proposed approach has been applied to the Alzheimer's Disease Neuroimaging Initiative (ADNI) data set. Using baseline MRI scans and selected single-nucleotide polymorphisms on chromosome 19, it achieved a classification accuracy of 83.78% and an area under the receiver operating characteristic curve (AUC-ROC) of 0.924 with the test set. The results demonstrate the great potential of using multi-disciplinary AI approaches to integrate imaging genetic data for the automated classification of AD.",Alzheimer's disease diagnosis; imaging genetics; deep learning; CNN; transformer; classification
"In the last decade, there has been a surge of interest in addressing complex Computer Vision (CV) problems in the field of face recognition (FR). In particular, one of the most difficult ones is based on the accurate determination of the ethnicity of mankind. In this regard, a new classification method using Machine Learning (ML) tools is proposed in this paper. Specifically, a new Deep Learning (DL) approach based on a Deep Convolutional Neural Network (DCNN) model is developed, which outperforms a reliable determination of the ethnicity of people based on their facial features. However, it is necessary to make use of specialized high-performance computing (HPC) hardware to build a workable DCNN-based FR system due to the low computation power given by the current central processing units (CPUs). Recently, the latter approach has increased the efficiency of the network in terms of power usage and execution time. Then, the usage of field-programmable gate arrays (FPGAs) was considered in this work. The performance of the new DCNN-based FR method using FPGA was compared against that using graphics processing units (GPUs). The experimental results considered an image dataset composed of 3141 photographs of citizens from three distinct countries. To our knowledge, this is the first image collection gathered specifically to address the ethnicity identification problem. Additionally, the ethnicity dataset was made publicly available as a novel contribution to this work. Finally, the experimental results proved the high performance provided by the proposed DCNN model using FPGAs, achieving an accuracy level of 96.9 percent and an F1 score of 94.6 percent while using a reasonable amount of energy and hardware resources.",face recognition; ethnicity identification; deep learning; real-time; HPC; FPGA; GPU
"How to automatically predict people's gaze has attracted attention in the field of computer vision and machine learning. Previous studies on this topic set many constraints, such as restricted scenarios and strict and complex inputs. To mitigate these constraints to predict the gaze of people in more general scenarios, we propose a three-pathway network (TPNet) to estimate gaze via the joint modeling of multiple cues. Specifically, we first design a human-centric relationship inference (HCRI) module to learn the object-level relationship between the target person and the surrounding persons/objects in a scene. To the best of our knowledge, this is the first time that the object-level relationship is introduced into the gaze estimation task. Then, we construct a novel deep network with three pathways to fuse multiple cues, including scene saliency, object-level relationships and head information, to predict the gaze target. In addition, to extract the multilevel features during network training, we build and embed a micropyramid module in TPNet. The performance of TPNet is evaluated on two gaze estimation datasets: GazeFollow and DLGaze. A large number of quantitative and qualitative experimental results verify that TPNet can obtain robust results and significantly outperform the existing state-of-the-art gaze estimation methods. The code of TPNet will be released later.",Estimation; Visualization; Task analysis; Feature extraction; Faces; Semantics; Saliency detection; Gaze estimation; three-pathway network; multiple cues fusion; human-centric relationship inference; image understanding; computer vision
"The rapid development of machine learning technologies in recent years has led to the emergence of CNN-based sensors or ML-enabled smart sensor systems, which are intensively used in medical analytics, unmanned driving of cars, Earth sensing, etc. In practice, the accuracy of CNN-based sensors is highly dependent on the quality of the training datasets. The preparation of such datasets faces two fundamental challenges: data quantity and data quality. In this paper, we propose an approach aimed to solve both of these problems and investigate its efficiency. Our solution improves training datasets and validates it in several different applications: object classification and detection, depth buffer reconstruction, panoptic segmentation. We present a pipeline for image dataset augmentation by synthesis with computer graphics and generative neural networks approaches. Our solution is well-controlled and allows us to generate datasets in a reproducible manner with the desired distribution of features which is essential to conduct specific experiments in computer vision. We developed a content creation pipeline targeted to create realistic image sequences with highly variable content. Our technique allows rendering of a single 3D object or 3D scene in a variety of ways, including changing of geometry, materials and lighting. By using synthetic data in training, we have improved the accuracy of CNN-based sensors compared to using only real-life data.",CNN-based sensors; synthetic training data; dataset augmentation; content creation pipeline
"Textures contain a wealth of image information and are widely used in various fields such as computer graphics and computer vision. With the development of machine learning, the texture synthesis and generation have been greatly improved. As a very common element in everyday life, wallpapers contain a wealth of texture information, making it difficult to annotate with a simple single label. Moreover, wallpaper designers spend significant time to create different styles of wallpaper. For this purpose, this paper proposes to describe wallpaper texture images by using multi-label semantics. Based on these labels and generative adversarial networks, we present a framework for perception driven wallpaper texture generation and style transfer. In this framework, a perceptual model is trained to recognize whether the wallpapers produced by the generator network are sufficiently realistic and have the attribute designated by given perceptual description; these multi-label semantic attributes are treated as condition variables to generate wallpaper images. The generated wallpaper images can be converted to those with well-known artist styles using CycleGAN. Finally, using the aesthetic evaluation method, the generated wallpaper images are quantitatively measured. The experimental results demonstrate that the proposed method can generate wallpaper textures conforming to human aesthetics and have artistic characteristics.",Semantics; Generative adversarial networks; Training; Task analysis; Image color analysis; Generators; Computer vision; Texture generation; multi-label semantics; style transfer
"Fine-grained vehicle categorization has evolved into a significant subject of study due to its importance in the Intelligent Transportation System. A highly accurate and real-time vehicle categorization system will help to support many applications not only in the security aspect but also many walks of life. In this paper, facing the growing importance of this study, we present an image dataset named Frontal-103 to promote the development of the vision-based research on the vehicle, and particularly for the task of fine-grained vehicle categorization. This paper provides a detailed analysis of Frontal-103 in its current state: 1,759 fine-grained vehicle models in 103 vehicle makes and 65,433 web-nature images in total. Apart from the specific viewpoint and vehicle hierarchy, Frontal-103 is superior to the other state-of-the-art vehicle image datasets not only in the scale and diversity but also the accuracy and fine-grained level. We further discuss the peculiar challenges and issues lies in the task of fine-grained vehicle categorization and illustrate the usefulness of our dataset in addressing those problems. We hope Frontal-103 will be beneficial to the vision-based vehicle analysis and contribute to the computer vision community.",Task analysis; Automobiles; Computational modeling; Cameras; Three-dimensional displays; Surveillance; Image Dataset; fine-grained; vehicle categorization
"Depth estimation is crucial in several computer vision applications, and a recent trend in this field aims at inferring such a cue from a single camera. Unfortunately, despite the compelling results achieved, state-of-the-art monocular depth estimation methods are computationally demanding, thus precluding their practical deployment in several application contexts characterized by low-power constraints. Therefore, in this paper, we propose a lightweight Convolutional Neural Network based on a shallow pyramidal architecture, referred to as mu PyD-Net, enabling monocular depth estimation on microcontrollers. The network is trained in a peculiar self-supervised manner leveraging proxy labels obtained through a traditional stereo algorithm. Moreover, we propose optimization strategies aimed at performing computations with quantized 8-bit data and map the high-level description of the network to low-level layers optimized for the target microcontroller architecture. Exhaustive experimental results on standard datasets and an in-depth evaluation with a device belonging to the popular Arm CortexM family confirm that obtaining sufficiently accurate monocular depth estimation on microcontrollers is feasible. To the best of our knowledge, our proposal is the first one enabling such remarkable achievement, paving the way for the deployment of monocular depth cues onto the tiny end-nodes of distributed sensor networks.",Computer vision; depth estimation; deep learning; optimization methods; edge computing; IoT; micro-controllers
"In underwater scenes, degraded underwater images caused by wavelength-dependent light absorption and scattering present huge challenges to vision tasks. Underwater image enhancement has attracted much attention due to the significance of vision-based applications in marine engineering and underwater robotics. Numerous underwater image enhancement algorithms have been proposed in the last few years. However, almost all existing approaches focus only on the enhancement of independent images. Considering that images photographed in the same underwater scene usually share similar degradation, related images can provide rich complementary information for each other's enhancement. In this paper, we propose an Underwater Image Co-enhancement Network (UICoE-Net) based on an encoder-decoder Siamese architecture. For joint learning, we introduced correlation feature matching units into the multiple layers of our Siamese encoder-decoder structure in order to communicate the mutual correlation of the two branches. Extensive experiments using the Underwater Image Enhancement Benchmark (UIEB), Underwater Image Co-enhancement Dataset (UICoD) collected from an underwater video dataset with ground-truth reference and Stereo Quantitative Underwater Image Dataset (SQUID) dataset demonstrate the effectiveness of our method.",Image enhancement; Correlation; Task analysis; Computer vision; Degradation; Deep learning; Visualization; Underwater image enhancement; underwater image co-enhancement; deep learning; convolutional neural network; Siamese structure; correlation feature matching
"Intelligent video surveillance systems are rapidly being introduced to public places. The adoption of computer vision and machine learning techniques enables various applications for collected video features; one of the major is safety monitoring. The efficacy of violent event detection is measured by the efficiency and accuracy of violent event detection. In this paper, we present a novel architecture for violence detection from video surveillance cameras. Our proposed model is a spatial feature extracting a U-Net-like network that uses MobileNet V2 as an encoder followed by LSTM for temporal feature extraction and classification. The proposed model is computationally light and still achieves good results-experiments showed that an average accuracy is 0.82 +/- 2% and average precision is 0.81 +/- 3% using a complex real-world security camera footage dataset based on RWF-2000.",violence detection; violent behavior; intelligent video surveillance; computer vision; U-Net; LSTM; deep learning
"Under the dual effects of the rapid growth of tunnel mileage and operating years, the application and research of tunnel crack identification based on machine vision are increasing with the vigorous development of machine vision. However, due to the complex environment in tunnels, it is difficult to quickly obtain tunnel lining cracks via computer visions in the tunnel. Therefore, this paper presents the design of a fast acquisition system with the geometric feature analysis for tunnel lining cracks, which has been integrated into a tunnel fast inspection vehicle with a machine vision module. Through the research on the image acquisition system of the tunnel lining, the parameter selection of the crack shooting hardware system is determined, and the fast calculation method of shooting parameters is proposed. The geometric characteristic analysis of the tunnel lining crack image is employed to calculate crack width and determine the optimal gray value of crack extraction. Field tests have been conducted in the highway tunnels in Zhejiang and Yunnan provinces in China and the result indicates that the proposed approach yields much better performance in the detection efficiency, whose time of detection is only 1%, and the number of personnel required is only 40% of the traditional pure manual method. Compared with similar systems, it also has significant advantages in crack resolution and detection speed. This research provides a means of rapid acquisition of tunnel cracks and laying a foundation for the evaluation of the service performance of the tunnel.",tunnel detection; machine vision; lining cracks; image processing; image acquisition; system development
"It is one of the most critical technologies for unmanned electric locomotives to detect the obstacles in front of their operation quickly and accurately, which is of great significance for the safe operation of electric locomotives Aiming at the problems of current computer vision detection methods, such as error warning, low detection accuracy, and slow detection speed, an obstacle intelligent detection method for unmanned electric locomotives based on an improved YOLOv3 (YOLOv3-4L) algorithm is proposed. The obstacle image data set of the electric locomotive running area is constructed to provide a testing environment for various obstacle detection algorithms. In the network structure, the darknet-53 feature extraction network is simplified, and the four-scale detection structure is formed by adding the shallow layer detection scale to the detection layer, which can improve the detection speed and accuracy of the algorithm for obstacles in front of the locomotive. Distance intersection over union loss function and Focal loss function are adopted to redesign the loss function of the target detector to further improve the detection accuracy of the algorithm. Traditional computer vision techniques such as perspective transformation, sliding window, and least square cubic polynomial are used to detect the track lines. By finding the area where the track was located and extending a certain distance to the outside of the track, the dangerous area of electric locomotive running is obtained. The improved YOLOv3 algorithm is utilized to detect obstacles, and only the types and positions of obstacles coincident with dangerous areas are output. The experimental results show that the traditional computer vision techniques such as perspective transformation, sliding window, and least square cubic polynomial can detect not only straight track but also curved track, which makes up for the shortcomings of the Hough transforms in detecting curved tracks. Compared with the original YOLOv3 algorithm, the YOLOv3-4L algorithm improves the mean average precision by 5.1%, and the detection speed increases by 7 fps. YOLOv3-4L detection model has high detection accuracy and speed, which can meet the actual working conditions and provide technical reference for unmanned driving of electric locomotives in underground coal mines. (C) 2022 SPIE and IS&T",computer vision; feature extraction network; feature scale; loss function; dangerous area
"The automatic classification and retrieval of images is a challenging task, especially when dealing with low-quality and faded inks images, such as the historical manuscripts. Therefore, in this study we develop a reinforcement learning agent that is capable of interacting with an environment including historical Arabic manuscript images and retrieve the most similar images to a query image. First, the deep visual features of the images are extracted utilizing the pre-trained VGG19 convolutional neural network. Then, the associated deep textual features of the images are also extracted utilizing the attentional BiLSTM deep learning model. Both features are fused using the concatenation merge layer and hashed to reduce the dimensionality among the fused feature vectors for better image classification and retrieval. The proposed method tested on a manually collected dataset and recorded a promising high accuracy proved that the computer vision could be better performing than the humans' vision.",Reinforcement learning; Image retrieval; Deep features fusion; Locality-sensitive hashing
"Leukocytes are a critical component of the human immune system. Many diseases can be diagnosed by analyzing the morphology and number of leukocytes. Due to the extensive application of convolutional neural networks (CNNs) in computer vision (CV), computer-aided automated methods have become the preferred methods for medical image diagnoses. Recently, Transformer has emerged in CV with performance comparable to CNN. Assisted diagnoses are often performed on resource-limited computing devices. The deployments of deep learning (DL) models are limited by the number of parameters and the computation. This study provides a DL training framework that introduces a model compression method of knowledge distillation (KD) in the classification of leukocytes, using small models instead of large ones, to achieve accurate results. Firstly, large models with CNN or Transformer structure are pre-trained on the mixed leukocyte dataset with 25,830 original images. Then, the dark knowledge of the pre-trained large models is extracted by KD, and the small models are trained. Finally, the best performing small model is selected as the final prediction model, which achieves 98.31% testing accuracy on the mixed dataset. The proposed framework on the enhanced BCCD dataset achieves 99.88% testing accuracy, which is better than other methods. It effectively combines the advantages of large and small models to meet the requirements of low resource consumption and high accuracy.",Knowledge distillation; Deep learning; Leukocyte classification; CNN; Transformer
"As everyone knows that in today's time Artificial Intelligence, Machine Learning and Deep Learning are being used extensively and generally researchers are thinking of using them everywhere. At the same time, we are also seeing that the second wave of corona has wreaked havoc in India. More than 4 lakh cases are coming in 24 h. In the meantime, news came that a new deadly fungus has come, which doctors have named Mucormycosis (Black fungus). This fungus also spread rapidly in many states, due to which states have declared this disease as an epidemic. It has become very important to find a cure for this life-threatening fungus by taking the help of our today's devices and technology such as artificial intelligence, data learning. It was found that the CT-Scan has much more adequate information and delivers greater evaluation validity than the chest X-Ray. After that the steps of Image processing such as pre-processing, segmentation, all these were surveyed in which it was found that accuracy score for the deep features retrieved from the ResNet50 model and SVM classifier using the Linear kernel function was 94.7%, which was the highest of all the findings. Also studied about Deep Belief Network (DBN) that how easy it can be to diagnose a life-threatening infection like fungus. Then a survey explained how computer vision helped in the corona era, in the same way it would help in epidemics like Mucormycosis.",Mucormycosis; Computer vision; Black fungus; Artificial intelligence; Deep learning
"Acute lymphoblastic leukemia (ALL) is the most common childhood cancer worldwide, and it is characterized by the production of immature malignant cells in the bone marrow. Computer vision techniques provide automated analysis that can help specialists diagnose this disease. Microscopy image analysis is the most economical method for the initial screening of patients with ALL, but this task is subjective and time-consuming. In this study, we propose a hybrid model using a genetic algorithm (GA) and a residual convolutional neural network (CNN), ResNet-50V2, to predict ALL using microscopy images available in ALL-IDB dataset. However, accurate prediction requires suitable hyperparameters setup, and tuning these values manually still poses challenges. Hence, this paper uses GA to find the best hyperparameters that lead to the highest accuracy rate in the models. Also, we compare the performance of GA hyperparameter optimization with Random Search and Bayesian optimization methods. The results show that GA optimization improves the accuracy of the classifier, obtaining 98.46% in terms of accuracy. Additionally, our approach sheds new perspectives on identifying leukemia based on computer vision strategies, which could be an alternative for applications in a real-world scenario.",Leukemia classification; Convolutional neural networks; Genetic algorithm; Hyperparameter optimization; Fine-tuning
"Hurricanes are tropical storms that cause immense damage to human life and property. Rapid assessment of damage caused by hurricanes is extremely important for the first responders. But this process is usually slow, expensive, labor intensive and prone to errors. The advancements in remote sensing and computer vision help in observing Earth at a different scale. In this paper, a new Convolutional Neural Network model has been designed with the help of satellite images captured from the areas affected by hurricanes. The model will be able to assess the damage by detecting damaged and undamaged buildings based upon which the relief aid can be provided to the affected people on an immediate basis. The model is composed of five convolutional layers, five pooling layers, one flattening layer, one dropout layer and two dense layers. Hurricane Harvey dataset consisting of 23,000 images of size 128 x 128 pixels has been used in this paper. The proposed model is simulated on 5750 test images at a learning rate of 0.00001 and 30 epochs with the Adam optimizer obtaining an accuracy of 0.95 and precision of 0.97. The proposed model will help the emergency responders to determine whether there has been damage or not due to the hurricane and also help those to provide relief aid to the affected people.",Natural disaster; Damage; Hurricane; Remote sensing; Satellite imagery; Computer vision; Deep learning; Convolutional neural network
"Deep learning techniques help computer vision automatically learn the intrinsic patterns within complex data. This research mainly concentrates on creating a mobile application based on augmented reality for elderly mobile users that helps in identifying the traffic signals and other signboards in real-time using deep learning techniques. TensorFlow serves as an implementation platform to build the object detection system with deep learning. The single shot multibox detector (SSD) model and the two-stage faster-regional convolutional neural network (RCNN) models from TensorFlow's object detection application programming interface (API) are compared in this study. The SSD model with MobileNet as a backbone network serves well for this study as it is faster than the RCNN model with comparable accuracy. However, unconstrained environments like occlusions can be an obstacle to the effective performance of an object detection system. This research provides a solution to handle occlusions by developing a robust object detection system through image segmentation techniques. The model introduced is based on the SSD MobileNet model which enables it to be deployable on mobile devices for real-time offline detection. The developed model exhibits faster performance than the state-of-the-art instance segmentation model, Mask RCNN with comparable accuracy. Elaborated implementation of this system and results are presented in further sections.",computer vision; neural networks; object recognition; occlusion handling; SSD MobileNet; TensorFlow
Using a computer vision approach we have extracted the Haralick's texture features of randomly oriented electrospun nanomaterials in order to predict the proliferative behavior of cells which were subsequently seeded onto the nanosurfaces.,
"Production of gestures does not require vision. Everyone-even those who are blind since birth-produces hand gestures while interacting with others. In our previous study, a gesture-based technique called Dactylology has been developed for the visually impaired to help them interact with computers. Cognitive load is an important indicator and a critical research issue while designing and adopting such new techniques. Hence, a study was conducted to compare the performance on a task between two computer input techniques (i.e., Dactylology and Braille) under varying levels of cognitive load (low, medium, and high), introduced by manipulating the task complexity. For the purpose of the study, 14 visually impaired participants were trained on Dactylology and Braille techniques to interact with the computer. The task performance was measured through the response time and false response (FR). The results confirm that the participants had significantly lower response time and committed fewer FRs in the Dactylology technique than the Braille under all cognitive load conditions. Altogether, these results render sufficient support to consider gesture-based Dactylology as a potential technique for the visually impaired to interact with the computer.",Task analysis; Particle measurements; Atmospheric measurements; Visualization; Time factors; Keyboards; Standards; Braille; cognitive load; dactylology; human-computer interaction (HCI); model human processor (MHP); visually impaired
"Recent state-of-the-art Face Detection algorithms in the field of Computer Vision focus greatly on real-time processing and results. The applications using these algorithms deal with low quality video feeds having less Pixels Per Inch (ppi) and/or low frame rate. The algorithms perform well with such video feeds, but their performance deteriorates towards high quality, high data-per-frame videos. Such video files mostly exist in offline mode, that could be used for post processing by the Computer Vision applications. This paper focuses on developing such an algorithm that gives faster results on high quality videos, at par with the algorithms working on live low quality video feeds. The proposed algorithm uses Convolutional-MTCNN as base algorithm, and speeds it up for high definition videos. This paper also presents a novel solution to the problem of occlusion and detecting partial or fully hidden faces in the videos. This is achieved by using probabilistic approaches, given that the face has been identified in first few frames, to give the algorithm an estimate of where the face should be in the occluded region.",Face detection; Feature tracking; Interpolation
"Aims lmmunohistochemistry (IHC) assessment of tissue is a central component of the modern pathology workflow, but quantification is challenged by subjective estimates by pathologists or manual steps in semi-automated digital tools. This study integrates various computer vision tools to develop a fully automated workflow for quantifying Ki-67, a standard IHC test used to assess cell proliferation on digital whole slide images (WSIs). Methods We create an automated nuclear segmentation strategy by deploying a Mask R-CNN classifier to recognise and count 3,3'-diaminobenzidine positive and negative nuclei. To further improve automation, we replaced manual selection of regions of interest (ROIs) by aligning Ki-67 WSIs with corresponding H&E-stained sections, using scale-invariant feature transform (SIFT) and a conventional histomorphological convolutional neural networks to define tumour-rich areas for quantification. Results The Mask R-CNN was tested on 147 images generated from 34 brain tumour Ki-67 WSIs and showed a high concordance with aggregate pathologists' estimates (n = 3 assessors; y = 0.9712x - 1.945, r=0.9750). Concordance of each assessor's Ki-67 estimates was higher when compared with the Mask R-CNN than between individual assessors (r(avg)=0.9322 vs 0.8703; p=0.0213). Coupling the Mask R-tNN with SIFT-CNN workflow demonstrated ROls can be automatically chosen and partially sampled to improve automation and dramatically decrease computational time (average: 88.55-19.28min; p<0.0001). Conclusions We show how innovations in computer vision can be serially compounded to automate and improve implementation in clinical workflows. Generalisation of this approach to other ancillary studies has significant implications for computational pathology.",
"In the past ten years, deep learning technology has achieved a great success in many fields, like computer vision and speech recognition. Recently, large-scale geometry data become more and more available, and the learned geometry priors have been successfully applied to 3D computer vision and computer graphics fields. Different from the regular representation of images, surface meshes have irregular structures with different vertex numbers and topologies. Therefore, the traditional convolution neural networks used for images cannot be directly used to handle surface meshes, and thus, many methods have been proposed to solve this problem. In this paper, we provide a comprehensive survey of existing geometric deep learning methods for mesh processing. We first introduce the relevant knowledge and theoretical background of geometric deep learning and some basic mesh data knowledge, including some commonly used mesh datasets. Then, we review various deep learning models for mesh data with two different types: graph-based methods and mesh structure-based methods. We also review the deep learning-based applications for mesh data. In the final, we give some potential research directions in this field.",Geometric deep learning; Non-Euclidean space; Mesh; Convolution; Spectral domain; Spatial domain
"The massive addition of data to the internet in text, images, and videos made computer vision-based tasks challenging in the big data domain. Recent exploration of video data and progress in visual information captioning has been an arduous task in computer vision. Visual captioning is attributable to integrating visual information with natural language descriptions. This paper proposes an encoder-decoder framework with a 2D-Convolutional Neural Network (CNN) model and layered Long Short Term Memory (LSTM) as the encoder and an LSTM model integrated with an attention mechanism working as the decoder with a hybrid loss function. Visual feature vectors extracted from the video frames using a 2D-CNN model capture spatial features. Specifically, the visual feature vectors are fed into the layered LSTM to capture the temporal information. The attention mechanism enables the decoder to perceive and focus on relevant objects and correlate the visual context and language content for producing semantically correct captions. The visual features and GloVe word embeddings are input into the decoder to generate natural semantic descriptions for the videos. The performance of the proposed framework is evaluated on the video captioning benchmark dataset Microsoft Video Description (MSVD) using various well-known evaluation metrics. The experimental findings indicate that the suggested framework outperforms state-of-the-art techniques. Compared to the state-of-the-art research methods, the proposed model significantly increased all measures, B@1, B@2, B@3, B@4, METEOR, and CIDEr, with the score of 78.4, 64.8, 54.2, and 43.7, 32.3, and 70.7, respectively. The progression in all scores indicates a more excellent grasp of the context of the inputs, which results in more accurate caption prediction.",Attention; Computer vision; Convolutional neural network; LSTM; Video captioning
"The attention mechanism of computer vision represented by a non-local network improves the performance of numerous vision tasks while bringing computational burden for deployment Wang et al. (2018). In this work, we explore to release the inference computation for non-local network by decoupling the training/inference procedure. Specifically, we propose the implicit non-local network (iNL). During training, iNL models the dependency between features across long-range affinities like original non-local blocks; during inference, iNL could be reformulated as only two convolution layers but can rival non-local network. In this way, the computation complexity and the memory costs are reduced. In addition, we take a further step and extend our iNL into a more generalized form, which covers the attentions of different orders in computer vision tasks. iNL brings steady improvements on multiple benchmarks of different vision tasks including classification, detection, and instance segmentation. In the meantime, it provides a brand-new perspective to understand the attention mechanism in deep neural networks. (c) 2022 Elsevier B.V. All rights reserved.",Attention; Computation cost; Implicit method; Generalized form
"Objective This study aims to develop and validate a convolutional neural network (CNN)-based algorithm for automatic selection of informative frames in flexible laryngoscopic videos. The classifier has the potential to aid in the development of computer-aided diagnosis systems and reduce data processing time for clinician-computer scientist teams. Methods A dataset of 22,132 laryngoscopic frames was extracted from 137 flexible laryngostroboscopic videos from 115 patients. 55 videos were from healthy patients with no laryngeal pathology and 82 videos were from patients with vocal fold polyps. The extracted frames were manually labeled as informative or uninformative by two independent reviewers based on vocal fold visibility, lighting, focus, and camera distance, resulting in 18,114 informative frames and 4018 uninformative frames. The dataset was split into training and test sets. A pre-trained ResNet-18 model was trained using transfer learning to classify frames as informative or uninformative. Hyperparameters were set using cross-validation. The primary outcome was precision for the informative class and secondary outcomes were precision, recall, and F1-score for all classes. The processing rate for frames between the model and a human annotator were compared. Results The automated classifier achieved an informative frame precision, recall, and F1-score of 94.4%, 90.2%, and 92.3%, respectively, when evaluated on a hold-out test set of 4438 frames. The model processed frames 16 times faster than a human annotator. Conclusion The CNN-based classifier demonstrates high precision for classifying informative frames in flexible laryngostroboscopic videos. This model has the potential to aid researchers with dataset creation for computer-aided diagnosis systems by automatically extracting relevant frames from laryngoscopic videos.",artificial intelligence; computer vision; computer-aided diagnosis; laryngology; machine learning; vocal fold polyp
"This paper investigates the extension of ImageNet and its millions of English-labeled images to Arabic using Arabic WordNet. The primary finding is the identification of Arabic synsets for 1219 of the 21,841 synsets used in ImageNet, which represents 1.1 million images. By leveraging the parent-child structure of synsets in ImageNet, this dataset is extended to 10,462 synsets (and 7.1 million images) that have an Arabic label, which is either a match or a direct hypernym, and to 17,438 synsets (and 11 million images) when a hypernym of a hypernym is included. Samples evaluated suggest that generating Arabic labels for images in ImageNet using hypernyms does indeed produce meaningful results. The precision values for seven evaluated samples exceeded 90%. Moreover, when all the images in the samples were combined, the precision value equaled 93%. For the entire ImageNet, when all hypernyms for a node are considered, an Arabic synset is found for all but four synsets. This represents the major contribution of this work: a dataset of 14,195,756 images that have Arabic labels. The resulting dataset presents Arabic labels for 99.9% of the images in ImageNet.",ImageNet; Computer vision; Arabic WordNet; Arabic computer vision; Language and computer vision; Linked data
"Roadside LiDAR (light detection and ranging) is a solution to fill in the gaps for connected vehicles (CV) by detecting the status of global road users at transportation facilities. It relies greatly on the clustering algorithm for accurate and rapid data processing so as to ensure effectiveness and reliability. To contribute to better roadside LiDAR-based transportation facilities, this paper presents a fast-spherical-projection-based clustering algorithm (FSPC) for real-time LiDAR data processing with higher clustering accuracy and noise handling. The FSPC is designed to work on a spherical map which could be directly derived from the instant returns of a LiDAR sensor. A 2D-window searching strategy is specifically designed to accelerate the computation and alleviate the density variation impact in the LiDAR point cloud. The test results show the proposed algorithm can achieve a high processing efficiency with 24.4 ms per frame, satisfying the real-time requirement for most common LiDAR applications (100 ms per frame), and it also ensures a high accuracy in object clustering, with 96%. Additionally, it is observed that the proposed FSPC allows a wider detection range and is more stable, tackling the surge in foreground points that frequently occurs in roadside LiDAR applications. Finally, the generality of the proposed FSPC indicates the proposed algorithm could also be implemented in other areas such as autonomous driving and remote sensing.",data and data science; automatic vehicle detection and identification systems; computer vision; data science; remote sensing; vehicle detection
"Vision-based hardware driver assistance systems are the most important systems in the world because of their low cost and ability to provide information on driving environments. Improving safety and reducing accidents are the two main objectives of these systems. For this, in this paper a new Vision-based Hardware Advanced Driver Assistance System (VH-ADAS) based machine learning incorporating the hybridization of Support Vector Machine (SVM)-Histogram of Oriented Gradient (HOG) classifier and Particle Swarm Optimization (PSO) technique is proposed for traffic scenes from both video and captured images. First, the proposed system uses a feature extraction method based on the HOG. Then, the Particle Swarm Optimization technique is used for selection and so to optimize the features. The SVM method is applied to obtain fast detection and high accuracy. Finally, a hardware synthesizable architecture of the complete system was developed and then co-simulation validity was succeeded using the Matlab-Vivado System Generator (VSG) and a Field Programmable Gate Array (FPGA). The results show that the proposed new system supports real-time detection in both images and video. Also, they show that the proposed vehicle detection method is competitive in terms of parallel run time with only 1.483 ms and in terms of accuracy rate with only 97.84%.",Real-time VH-ADAS; Machine learning; Particle swarm optimization; Vivado system generator
"Vehicle detection plays an important role in the development of an autonomous driving system. Fast processing and accurate detection are two major aspects of generating the autonomous vehicle detection system. This paper proposes a novel computer vision-based cost-effective vehicle detection system. Here, a Gentle Adaptive Boosting algorithm is trained with Haar-like features to generate the hypothesis of vehicles. Haar-like feature generates hypotheses very fast but may detect false vehicle candidates. The support vector machine algorithm is trained with the histogram of oriented gradient features to filter out the generated false hypothesis. The histogram of oriented gradients descriptor utilizes the shape and outlines of the vehicles, hence detects vehicles more accurately. Haar-Likes features and histogram of oriented gradients features are organized to accomplish the aspects of autonomous driving. The performance of the proposed vehicle detector is evaluated for day time and night time captured images and compared with three different existing vehicle detectors. The average precision of the proposed system for day time captured image is 0.97 and for night time captured image is 0.94. The proposed system requires 15 times less training time as compared to the existing technique for the same number of image data and on the same CPU.",vehicle detection system; Haar-like feature; histogram of oriented gradients features; Adaboost classifier; support vector machine; autonomous driving system
"Recent advances in image processing and machine learning methods have greatly enhanced the ability of object classification from images and videos in different applications. Classification of human activities is one of the emerging research areas in the field of computer vision. It can be used in several applications including medical informatics, surveillance, human computer interaction, and task monitoring. In the medical and healthcare field, the classification of patients' activities is important for providing the required information to doctors and physicians for medication reactions and diagnosis. Nowadays, some research approaches to recognize human activity from videos and images have been proposed using machine learning (ML) and soft computational algorithms. However, advanced computer vision methods are still considered promising development directions for developing human activity classification approach from a sequence of video frames. This paper proposes an effective automated approach using feature fusion and ML methods. It consists of five steps, which are the preprocessing, feature extraction, feature selection, feature fusion, and classification steps. Two available public benchmark datasets are utilized to train, validate, and test ML classifiers of the developed approach. The experimental results of this research work show that the accuracies achieved are 99.5% and 99.9% on the first and second datasets, respectively. Compared with many existing related approaches, the proposed approach attained high performance results in terms of sensitivity, accuracy, precision, and specificity evaluation metric.",
"Graph Convolutional Networks (GCNs) are general deep representation learning models for graph-structured data. In this paper, we propose a simple Plug-in Attention Module (PLAM) to improve the rep-resentation power of GCNs, inspired by the recent success of the query-key mechanism in computer vision and natural language processing. With this module, our network is able to adaptively learn the weights from a node towards its neighbors. Different from existing attention-based GCNs, the proposed PLAM has several important properties. First, the parameter space for the attention module is isolated from that for feature learning. This ensures that the proposed approach can be conveniently applied to existing GCNs as a plug-in module. Second, the anchor node and neighbor nodes are treated separately when learning the attention weights, which further enhances the flexibility of our structure. Third, our attention module extracts higher-level information by computing the inner product of the features between the anchor node and neighbor nodes, leading to significantly increased representation power. Last, we take a step forward and propose a novel structural encoding technique for the graph attention module to inject local and global structure information. Although being simple, our PLAM models have achieved state-of-the-art performances on graph-structured datasets under both the transductive and inductive settings. Additionally, experiments on image and point cloud datasets show potential applica-tions of PLAM on several computer vision tasks. (c) 2022 Published by Elsevier B.V.",Graph-based learning; Graph convolutional networks; Self-attention; Semi-supervised node classification
"The vision-based smart driving technologies for road safety are the popular research topics in computer vision. The precise moving object detection with continuously tracking capability is one of the most important vision-based technologies nowadays. In this paper, we propose an improved object detection system, which combines a typical object detector and long short-term memory (LSTM) modules, to further improve the detection performance for smart driving. First, starting from a selected object detector, we combine all vehicle classes and bypassing low-level features to improve its detection performance. After the spatial association of the detected objects, the outputs of the improved object detector are then fed into the proposed double-layer LSTM (dLSTM) modules to successfully improve the detection performance of the vehicles in various conditions, including the newly-appeared, the detected and the gradually-disappearing vehicles. With stage-by-stage evaluations, the experimental results show that the proposed vehicle detection system with dLSTM modules can precisely detect the vehicles without increasing computations.",Vehicle detection; LSTM-based object refiner; Spatial priority order; Adaptive miss-time threshold; Adaptive confidence threshold
"Rain streaks are one of the main factors that degrade the performance of computer vision algorithms. Therefore, a preprocessing method is needed to remove rain streaks from rainy images. The main issue of the rain removal task is to prevent over (or under) de-raining. Over de-raining means that the background details are removed along with rain streaks in light rain, and under de-raining means that the rain streaks are not completely removed in heavy rain. These occur as the density of rain and intensity of rain streaks vary. In order to solve this, this paper proposes a two-step rain removal method. The proposed system first estimates the rain streaks image redefined with a simple operation from an input rainy image. The proposed rain streaks image contains rain density and rain streak intensity for the rainy image. By using this, the proposed system can adaptively remove rain streaks from images captured in various rain conditions. In addition, we propose a novel architectural unit, the elementwise attentive gating block, which is an optimized block used to deal with high frequency rain streaks. The proposed block selectively passes the desired components from the input feature maps by applying different weights to each element. It helps to clearly extract the rain streaks, and as a result, there are no traces of rain streaks on the restored image. The proposed method outperforms previous rain removal algorithms for both synthetic and real-world images.",Rain; Image restoration; Task analysis; Feature extraction; Degradation; Computer vision; Learning systems; Rain removal; max channel simplification; deep learning; elementwise attention block
"Image captioning is a very important task, which is on the edge between natural language processing (NLP) and computer vision (CV). The current quality of the captioning models allows them to be used for practical tasks, but they require both large computational power and considerable storage space. Despite the practical importance of the image-captioning problem, only a few papers have investigated model size compression in order to prepare them for use on mobile devices. Furthermore, these works usually only investigate decoder compression in a typical encoder-decoder architecture, while the encoder traditionally occupies most of the space. We applied the most efficient model-compression techniques such as architectural changes, pruning and quantization to several state-of-the-art image-captioning architectures. As a result, all of these models were compressed by no less than 91% in terms of memory (including encoder), but lost no more than 2% and 4.5% in metrics such as CIDEr and SPICE, respectively. At the same time, the best model showed results of 127.4 CIDEr and 21.4 SPICE, with a size equal to only 34.8 MB, which sets a strong baseline for compression problems for image-captioning models, and could be used for practical applications.",image captioning; model compression; pruning; quantization
"Neuromorphic vision sensors such as the Dynamic and Active-pixel Vision Sensor (DAVIS) using silicon retina are inspired by biological vision, they generate streams of asynchronous events to indicate local log-intensity brightness changes. Their properties of high temporal resolution, low-bandwidth, lightweight computation, and low-latency make them a good fit for many applications of motion perception in the intelligent vehicle. However, as a younger and smaller research field compared to classical computer vision, neuromorphic vision is rarely connected with the intelligent vehicle. For this purpose, we present three novel datasets recorded with DAVIS sensors and depth sensor for the distracted driving research and focus on driver drowsiness detection, driver gaze-zone recognition, and driver hand-gesture recognition. To facilitate the comparison with classical computer vision, we record the RGB, depth and infrared data with a depth sensor simultaneously. The total volume of this dataset has 27360 samples. To unlock the potential of neuromorphic vision on the intelligent vehicle, we utilize three popular event-encoding methods to convert asynchronous event slices to event-frames and adapt state-of-the-art convolutional architectures to extensively evaluate their performances on this dataset. Together with qualitative and quantitative results, this work provides a new database and baseline evaluations named NeuroIV in cross-cutting areas of neuromorphic vision and intelligent vehicle.",Neuromorphics; Vision sensors; Intelligent sensors; Intelligent vehicles; Cameras; Neuromorphic vision; distracted driving; advanced driver assistance system; database and baseline evaluations; event encoding; deep learning
"Vision-based semantic segmentation of waterbodies and nearby related objects provides important information for managing water resources and handling flooding emergency. However, the lack of large-scale labeled training and testing datasets for water-related categories prevents researchers from studying water-related issues in the computer vision field. To tackle this problem, we present ATLANTIS, a new benchmark for semantic segmentation of waterbodies and related objects. ATLANTIS consists of 5,195 images of waterbodies, as well as high quality pixel-level manual annotations of 56 classes of objects, including 17 classes of man-made objects, 18 classes of natural objects and 21 general classes. We analyze ATLANTIS in detail and evaluate several state-of-the-art semantic segmentation networks on our benchmark. In addition, a novel deep neural network, AQUANet, is developed for waterbody semantic segmentation by processing the aquatic and non-aquatic regions in two different paths. AQUANet also incorporates low-level feature modulation and cross-path modulation for enhancing feature representation. Experimental results show that the proposed AQUANet outperforms other state-of-the-art semantic segmentation networks on ATLANTIS. We claim that ATLANTIS is the largest waterbody image dataset for semantic segmentation providing a wide range of water and water-related classes and it will benefit researchers of both computer vision and water resources engineering.",Water resources engineering; Computer vision; Waterbody image dataset; Semantic segmentation; Deep learning models
"Non-Destructive Testing (NDT) is one of the inspection techniques used in industrial tool inspection for quality and safety control. It is performed mainly using X-ray Computed Tomography (CT) to scan the internal structure of the tools and detect the potential defects. In this paper, we propose a new toolbox called the CT-Based Integrity Monitoring System (CTIMS-Toolbox) for automated inspection of CT images and volumes. It contains three main modules: first, the database management module, which handles the database and reads/writes queries to retrieve or save the CT data; second, the pre-processing module for registration and background subtraction; third, the defect inspection module to detect all the potential defects (missing parts, damaged screws, etc.) based on a hybrid system composed of computer vision and deep learning techniques. This paper explores the different features of the CTIMS-Toolbox, exposes the performance of its modules, compares its features to some existing CT inspection toolboxes, and provides some examples of the obtained results.",computerized tomography (CT); defect inspection; computer vision; image processing; deep learning; toolbox; image classification
"The application of deep architectures inspired by the fields of artificial intelligence and computer vision has made a significant impact on the task of crack detection. As the number of studies being published in this field is growing fast, it is important to categorize the studies at deeper levels. In this paper, a comprehensive literature review of deep learning-based crack detection studies and the contributions they have made to the field is presented. The studies are categorised according to the computer vision aspect and at deeper levels to facilitate exploring the studies that utilised similar approaches to address the crack detection problem. Moreover, the authors perform a comparison between the studies which use the same publicly available data sets, in order to find the most promising crack detection approaches. Critical future directions for research are proposed, based on these reviewed studies as well as on trends and developments in areas similar to the crack detection area.",structural health monitoring; crack detection; deep learning; image classification; object recognition; semantic segmentation
"This article presents a systematic overview of artificial intelligence (AI) and computer vision strategies for diagnosing the coronavirus disease of 2019 (COVID-19) using computerized tomography (CT) medical images. We analyzed the previous review works and found that all of them ignored classifying and categorizing COVID-19 literature based on computer vision tasks, such as classification, segmentation, and detection. Most of the COVID-19 CT diagnosis methods comprehensively use segmentation and classification tasks. Moreover, most of the review articles are diverse and cover CT as well as X-ray images. Therefore, we focused on the COVID-19 diagnostic methods based on CT images. Well-known search engines and databases such as Google, Google Scholar, Kaggle, Baidu, IEEE Xplore, Web of Science, PubMed, ScienceDirect, and Scopus were utilized to collect relevant studies. After deep analysis, we collected 114 studies and reported highly enriched information for each selected research. According to our analysis, AI and computer vision have substantial potential for rapid COVID-19 diagnosis as they could significantly assist in automating the diagnosis process. Accurate and efficient models will have real-time clinical implications, though further research is still required. Categorization of literature based on computer vision tasks could be helpful for future research; therefore, this review article will provide a good foundation for conducting such research.",COVID-19; COVID-19 diagnosis; COVID-19 classification; Image segmentation; COVID-19 detection
"There is a large growth in hardware and software systems capable of producing vast amounts of image and video data. These systems are rich sources of continuous image and video streams. This motivates researchers to build scalable computer vision systems that utilize data-streaming concepts for processing of visual data streams. However, several challenges exist in building large-scale computer vision systems. For example, computer vision algorithms have different accuracy and speed profiles depending on the content, type, and speed of incoming data. Also, it is not clear how to adaptively tune these algorithms in large-scale systems. These challenges exist because we lack formal frameworks for building and optimizing large-scale visual processing. This paper presents formal methods and algorithms that aim to overcome these challenges and improve building and optimizing large-scale computer vision systems. We describe a formal algebra framework for the mathematical description of computer vision pipelines for processing image and video streams. The algebra naturally describes feedback control and provides a formal and abstract method for optimizing computer vision pipelines. We then show that a general optimizer can be used with the feedback-control mechanisms of our stream algebra to provide a common online parameter optimization method for computer vision pipelines.",Streaming media; Computer vision; Algebra; Pipelines; Approximation algorithms; Optimization; Buildings; Stream algebra; online vision algorithms; stream processing; large scale computer vision systems; parameter tuning; performance optimization
"In this article, we propose a novel model for facial micro-expression (FME) recognition. The proposed model basically comprises a transformer, which is recently used for computer vision and has never been used for FME recognition. A transformer requires a huge amount of data compared to a convolution neural network. Then, we use motion features, such as optical flow and late fusion to complement the lack of FME dataset. The proposed method was verified and evaluated using the SMIC and CASME II datasets. Our approach achieved state-of-the-art (SOTA) performance of 0.7447 and 73.17% in SMIC in terms of unweighted F1 score (UF1) and accuracy (Acc.), respectively, which are 0.31 and 1.8% higher than previous SOTA. Furthermore, UF1 of 0.7106 and Acc. of 70.68% were shown in the CASME II experiment, which are comparable with SOTA.",deep learning; image processing; facial micro-expression; emotion recognition; vision transformer
"Computer-vision-based space circular target detection has a wide range of applications in visual measurement, object detection, and other fields. The space circular target is projected into an ellipse in the camera for localization. Traditional methods based on monocular vision use a precise calculation model to calculate the center coordinate and normal vector of the space circular target according to the image's elliptic parameters. However, this accurate calculation method has the disadvantage of poor anti-interference ability in practical application. Aiming at the shortcomings of the above traditional calculation method, this paper proposes an optimization method for fitting the circular target in 3D space, where the image ellipse is projected back into 3D space and then detects the center coordinate and normal vector of the space circular target. Unlike the traditional method, this approach is not sensitive to the image's elliptic parameters; it has stronger noise resistance performance and notable application value. The feasibility and effectiveness of the proposed method were verified by both simulation and practical experimental results.",computer vision; monocular vision; space circular target detection; noise resistance performance
"Computer vision has shown potential for assisting post-earthquake inspection of buildings through automatic damage detection in images. However, assessing the safety of an earthquake-damaged building requires considering this damage in the context of its global impact on the structural system. Thus, an inspection must consider the expected damage progression of the associated component and the component's contribution to structural system performance. To address this issue, a digital twin framework is proposed for post-earthquake building evaluation that integrates unmanned aerial vehicle (UAV) imagery, component identification, and damage evaluation using a Building Information Model (BIM) as a reference platform. The BIM guides selection of optimal sets of images for each building component. Then, if damage is identified, each image pixel is assigned to a specific BIM component, using a GrabCut-based segmentation method. In addition, 3D point cloud change detection is employed to identify nonstructural damage and associate that damage with specific BIM components. Two example applications are presented. The first develops a digital twin for an existing reinforced concrete moment frame building and demonstrates BIM-guided image selection and component identification. The second uses a synthetic graphics environment to demonstrate 3D point cloud change detection for identifying damaged nonstructural masonry walls. In both examples, observed damage is tied to BIM components, enabling damage to be considered in the context of each component's known design and expected earthquake performance. The goal of this framework is to combine component-wise damage estimates with a pre-earthquake structural analysis of the building to predict a building's post-earthquake safety based on an external UAV survey.",unmanned aerial vehicles; building information modeling; digital twin; computer vision; post-earthquake evaluation; automated inspection
"The inspection of welding surface quality is an important task for welding work. With the development of product quality inspection technology, automated and machine vision-based inspection have been applied to more industrial application fields because of its non-contact, convenience, and high efficiency. However, challenging material and optical phenomena such as high reflective surface areas often present on welding seams tend to produce artifacts such as holes in the reconstructed model using current visual sensors, hence leading to insufficiency or even errors in the inspection result. This paper presents a 3D reconstruction technique for highly reflective welding surfaces based on binocular style structured light stereo vision. The method starts from capturing a fully lit image for identifying highly reflective regions on a welding surface using conventional computer vision models, including gray-scale, binarization, dilation, and erosion. Then, fringe projection profilometry is used to generate point clouds on the interested area. The mapping and alignment from 2D image to 3D point cloud is then established to highlight features that are vital for eliminating holes-large featureless areas-caused by high reflections such as the specular mirroring effect. A two-way slicing method is proposed to operate on the refined point cloud, following the concept of dimensionality reduction to project the sliced point cloud onto different image planes before a Smoothing Spline model is applied to fit the discrete point formed by projection. The 3D coordinate values of points in the hole region are estimated according to the fitted curves and appended to the original point cloud using iterative algorithms. Experiment results verify that the proposed method can accurately reconstruct a wide range of welding surfaces with significantly improved precision.",3D reconstruction; structured light; high reflect; point cloud
"Simple Summary Ticks are ectoparasites of humans, livestock, and wild animals and, as such, they are a nuisance, as well as vectors for disease transmission. Since the risk of tick-borne disease varies with the tick species, tick identification is vitally important in assessing threats. Standard taxonomic approaches are time-consuming and require skilled microscopy. Computer vision may provide a tenable solution to this problem. The emerging field of computer vision has many practical applications already, such as medical image analyses, facial recognition, and object detection. This tool may also help with the identification of ticks. To train a computer vision model, a substantial number of images are required. In the present study, tick images were obtained from a tick passive surveillance program that receives ticks from public individuals, partnering agencies, or veterinary clinics. We developed a computer vision method to identify common tick species and our results indicate that this tool could provide accurate, affordable, and real-time solutions for discriminating tick species. It provides an alternative to the present tick identification strategies. A wide range of pathogens, such as bacteria, viruses, and parasites can be transmitted by ticks and can cause diseases, such as Lyme disease, anaplasmosis, or Rocky Mountain spotted fever. Landscape and climate changes are driving the geographic range expansion of important tick species. The morphological identification of ticks is critical for the assessment of disease risk; however, this process is time-consuming, costly, and requires qualified taxonomic specialists. To address this issue, we constructed a tick identification tool that can differentiate the most encountered human-biting ticks, Amblyomma americanum, Dermacentor variabilis, and Ixodes scapularis, by implementing artificial intelligence methods with deep learning algorithms. Many convolutional neural network (CNN) models (such as VGG, ResNet, or Inception) have been used for image recognition purposes but it is still a very limited application in the use of tick identification. Here, we describe the modified CNN-based models which were trained using a large-scale molecularly verified dataset to identify tick species. The best CNN model achieved a 99.5% accuracy on the test set. These results demonstrate that a computer vision system is a potential alternative tool to help in prescreening ticks for identification, an earlier diagnosis of disease risk, and, as such, could be a valuable resource for health professionals.",medical entomology; ticks; computer vision
"Computer vision for large scale building detection can be very challenging in many environments and settings even with recent advances in deep learning technologies. Even more challenging is modeling to detect the presence of specific buildings (in this case schools) in satellite imagery at a global scale. However, despite the variation in school building structures from rural to urban areas and from country to country, many school buildings have identifiable overhead signatures that make them possible to be detected from high-resolution imagery with modern deep learning techniques. Our hypothesis is that a Deep Convolutional Neural Network (CNN) could be trained for successful mapping of school locations at a regional or global scale from high-resolution satellite imagery. One of the key objectives of this work is to explore the possibility of having a scalable model that can be used to map schools across the globe. In this work, we developed AI-assisted rapid school location mapping models in eight countries in Asia, Africa, and South America. The results show that regional models outperform country-specific models and the global model. This indicates that the regional model took the advantage of having been exposed to diverse school location structure and features and generalized better, however, the global model was the worst performer due to the difficulty of generalizing the significant variability of school location features across different countries from different regions.",computer vision; deep learning; school mapping; high resolution satellite imagery
"The egocentric action recognition EAR field has recently increased its popularity due to the affordable and lightweight wearable cameras available nowadays such as GoPro and similars. Therefore, the amount of egocentric data generated has increased, triggering the interest in the understanding of egocentric videos. More specifically, the recognition of actions in egocentric videos has gained popularity due to the challenge that it poses: the wild movement of the camera and the lack of context make it hard to recognise actions with a performance similar to that of third-person vision solutions. This has ignited the research interest on the field and, nowadays, many public datasets and competitions can be found in both the machine learning and the computer vision communities. In this survey, we aim to analyse the literature on egocentric vision methods and algorithms. For that, we propose a taxonomy to divide the literature into various categories with subcategories, contributing a more fine-grained classification of the available methods. We also provide a review of the zero-shot approaches used by the EAR community, a methodology that could help to transfer EAR algorithms to real-world applications. Finally, we summarise the datasets used by researchers in the literature. (c) 2021 The Author(s). Published by Elsevier B.V. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).",Deep learning; Computer vision; Human action recognition; Egocentric vision; Few-shot learning
"Video tracking involves detecting previously designated objects of interest within a sequence of image frames. It can be applied in robotics, unmanned vehicles, and automation, among other fields of interest. Video tracking is still regarded as an open problem due to a number of obstacles that still need to be overcome, including the need for high precision and real-time results, as well as portability and low-power demands. This work presents the design, implementation and assessment of a low-power embedded system based on an SoC-FPGA platform and the honeybee search algorithm (HSA) for real-time video tracking. HSA is a meta-heuristic that combines evolutionary computing and swarm intelligence techniques. Our findings demonstrated that the combination of SoC-FPGA and HSA reduced the consumption of computational resources, allowing real-time multiprocessing without a reduction in precision, and with the advantage of lower power consumption, which enabled portability. A starker difference was observed when measuring the power consumption. The proposed SoC-FPGA system consumed about 5 Watts, whereas the CPU-GPU system required more than 200 Watts. A general recommendation obtained from this research is to use SoC-FPGA over CPU-GPU to work with meta-heuristics in computer vision applications when an embedded solution is required.",heterogeneous computing; meta-heuristic; video tracking; system-on-chip; field-programmable gate array; evolutionary computing; swarm intelligence; embedded system design; graphics processing unit; computer vision
"Reduction in chemical usage for crop management due to the environmental and health issues is a key area in achieving sustainable agricultural practices. One area in which this can be achieved is through the development of intelligent spraying systems which can identify the target for example crop disease or weeds allowing for precise spraying reducing chemical usage. Artificial intelligence and computer vision has the potential to be applied for the precise detection and classification of crops. In this paper, a study is presented that uses instance segmentation for the task of leaf and rust disease detection in apple orchards using Mask R-CNN. Three different Mask R-CNN network backbones (ResNet-50, MobileNetV3-Large and MobileNetV3-Large-Mobile) are trained and evaluated for the tasks of object detection, segmentation and disease detection. Segmentation masks on a subset of the Plant Pathology Challenge 2020 database are annotated by the authors, and these are used for the training and evaluation of the proposed Mask R-CNN based models. The study highlights that a Mask R-CNN model with a ResNet-50 backbone provides good accuracy for the task, particularly in the detection of very small rust disease objects on the leaves.",computer vision; instance segmentation; sustainable agriculture
"Friction stir welding (FSW) is an environmentally friendly, solid-state welding technique. In this research work, we analyze the microstructure of a new type of FSW weld applying a two- stage framework based on image processing algorithms containing a segmentation step and microstructure analysis of objects occurring in different layers. A dual-speed tool as used to prepare the tested weld. In this paper, we present the segmentation method for recognizing areas containing particles forming bands in the microstructure of a dissimilar weld of aluminum alloys made by FSW technology. A digital analysis was performed on the images obtained using an Olympus GX51 light microscope. The image analysis process consisted of basic segmentation methods in conjunction with domain knowledge and object detection located in different layers of a weld using morphological operations and point transformations. These methods proved to be effective in the analysis of the microstructure images corrupted by noise. The segmentation parts as well as single objects were separated enough to analyze the distribution on different layers of the specimen and the variability of shape and size of the underlying microstructures, which was not possible without computer vision support.",FSW; image processing; computer vision; segmentation; microstructure analysis; dual-speed tool
"6G network enables the rapid connection of autonomous vehicles, the generated internet of vehicles establishes a large-scale point cloud, which requires automatic point cloud analysis to build an intelligent transportation system in terms of the 3D object detection and segmentation. Recently, a great variety of deep convolution networks have been proposed for 3D data analysis, making significant progress in the application of deep learning in 3D computer vision. Inspired by the application of transformer network in 2D computer visual tasks, and in order to increase the expression ability of local fine-grained features, we propose an effective local feature transformer network to learn local feature information and correlations between point clouds. Our network is adaptive to the arrangement of set elements through transformer module, so it is suitable for the feature extraction of local point clouds. In addition, experimental results demonstrate that our LFT-network outperforms the state-of-the-art in 3D model classification tasks on ModelNet40 dataset and segmentation tasks on S3DIS dataset.",Point cloud compression; Transformers; Three-dimensional displays; Task analysis; Feature extraction; Convolution; Semantics; 6G; point cloud; 3D computer vision; transfomer; classification; segmentation
"Aims Dynamic retinal vessel analysis (DVA) provides a non-invasive way to assess microvascular function in patients and potentially to improve predictions of individual cardiovascular (CV) risk. The aim of our study was to use untargeted machine learning on DVA in order to improve CV mortality prediction and identify corresponding response alterations. Methods and results We adopted a workflow consisting of noise reduction and extraction of independent components within DVA signals. Predictor performance was assessed in survival random forest models. Applying our technique to the prediction of all-cause mortality in a cohort of 214 haemodialysis patients resulted in the selection of a component which was highly correlated to maximal venous dilation following flicker stimulation (vMax), a previously identified predictor, confirming the validity of our approach. When fitting for CV mortality as the outcome of interest, a combination of three components derived from the arterial signal resulted in a marked improvement in predictive performance. Clustering analysis suggested that these independent components identified groups of patients with substantially higher CV mortality. Conclusion Our results provide a machine learning workflow to improve the predictive performance of DVA and identify groups of haemodialysis patients at high risk of CV mortality. Our approach may also prove to be promising for DVA signal analysis in other CV disease states.",Machine learning; Retinal vessels; Microcirculation; Haemodialysis; Myocardial infarction and cardiac death
"Video analysis of human motion has been widely used in intelligent monitoring, sports analysis, and virtual reality as a research hotspot in computer vision. It is necessary to decompose and track the movements in the process of movement in order to improve the training quality in dance training. The traditional motion tracking decomposition method, on the other hand, is unable to calculate the visual changes of adjacent key nodes, and the contour of 3D visual motion tracking remains ambiguous. This paper applies the human posture estimation algorithm in computer vision to the detection of key points of rectangular objects and obtains the heat map of key points of rectangular objects by adding a lightweight feature extraction network and a feature pyramid layer integrating multilayer semantic information, on the basis of summarizing and analyzing related research work at home and abroad. Because of the fusion of multilayer information, the network's design not only reduces the amount of calculation and parameters but also improves the accuracy of the final detection result. The test results show that the proposed algorithm's recognition accuracy has improved.",
"High-speed industrial machine-vision (MV) applications such as surface inspection of steel sheets necessitate synchronous operation of multiple high-resolution cameras. Synchronization of cameras in the microsecond band is necessary to ensure accurate frame matching while melding images together. Existing approaches for synchronization employ dedicated electronic circuits or network-time-protocol (NTP) whose accuracies are in the millisecond band. Conversely, IEEE-1508 precision-time-protocol (PTP) synchronizes computers in highly accurate industrial measurement and control networks. Synchronization algorithms using PTP involve synchronizing computers connected to cameras. Although the computers synchronize in the microsecond band, the cameras synchronize in the millisecond band. Moreover, PTP is practically not used for synchronizing multiple devices due to the high bandwidth utilization of the network. This paper proposes a temporal synchronization algorithm and framework with two-way communication with timestamps and estimates mean path delays. Unicast transmission forms the basis of the synchronization framework, so that the network utilization is minimal, thereby ensuring the necessary bandwidth is available for image transmission. Experimental results show that the proposed approach outperforms the existing methodologies with synchronization accuracies in the microsecond band.",Frame synchronization; Machine vision; Manufacturing automation
"In the semiconductor industry, automated visual inspection aims to improve the detection and recognition of manufacturing defects by leveraging the power of artificial intelligence and computer vision systems, enabling manufacturers to profit from an increased yield and reduced manufacturing costs. Previous domain-specific contributions often utilized classical computer vision approaches, whereas more novel systems deploy deep learning based ones. However, a persistent problem in the domain stems from the recognition of very small defect patterns which are often in the size of only a few mu m and pixels within vast amounts of high-resolution imagery. While these defect patterns occur on the significantly larger wafer surface, classical machine and deep learning solutions have problems in dealing with the complexity of this challenge. This contribution introduces a novel hybrid multistage system of stacked deep neural networks (SH-DNN) which allows the localization of the finest structures within pixel size via a classical computer vision pipeline, while the classification process is realized by deep neural networks. The proposed system draws the focus over the level of detail from its structures to more task-relevant areas of interest. As the created test environment shows, our SH-DNN-based multistage system surpasses current approaches of learning-based automated visual inspection. The system reaches a performance (F1-score) of up to 99.5%, corresponding to a relative improvement of the system's fault detection capabilities by 8.6-fold. Moreover, by specifically selecting models for the given manufacturing chain, runtime constraints are satisfied while improving the detection capabilities of currently deployed approaches.",Computer vision; Pattern and image recognition; Deep learning; Semiconductor manufacturing; Factory automation; Fault inspection
"Controllable image caption, which belongs to the intersection of Computer Vision (CV) and Natural Language Process (NLP), is an important part of applying artificial intelligence to many life scenes. We adopt an encoder-decoder structure, which considers visual models as the encoder and regards language models as the decoder. In this work, we introduce a new feature extraction model, namely FVC R-CNN, to learn both the salient features and the visual commonsense features. Furthermore, a novel MT-LSTM neural network for sentence generation is proposed, which is activated by m-tanh and is superior to the traditional Long Short-term memory Network (LSTM) by a significant margin. Finally, we put forward a multi-branch decision strategy to optimize the output. The experimental results are conducted on the widely used COCO Entities dataset, which demonstrates that the proposed method simultaneously outperforms the baseline, surpassing the state-of-the-art methods under a wide range of evaluation metrics. There are CIDEr and SPICE respectively achieves 206.3 and 47.6, yield state-of-the-art (SOTA) performance.",Controllable image caption; M-tanh activation function; MT-LSTM neural network; FVC R-CNN model
"The computer-assisted rehabilitation environment (CAREN) system plays an important role in the training of rehabilitation patients, where the capture of the patient's 3-D pose and gait is critical for assessing the patient's requirements for effective training. Vision-based methods are highly effective for this task due to their low cost, high speed, and noninterference. Although various general vision-based pose estimation methods were developed recently, their performance is limited in the CAREN system due to the specific environment. To address these problems, we propose an improved framework for accurate 2-D and 3-D pose estimation for the CAREN system through using multiview videos. First, for 2-D pose estimation, we propose a coarse-to-fine heatmap shrinking (CFHS) strategy that gradually reduces the kernel size of the heatmap of joints during training to improve the performance. Second, to further obtain 3-D pose estimations, we propose a novel spatial-temporal perception network that fuses the 2-D results from multiple views and multiple moments; multiview early fusion uses complementary spatial information from different views, and multimoment late fusion leverages temporal information from the sequential input for higher accuracy. The experimental results, based on CAREN videos of 225 orthopedic patients, showed that the accuracy of 2-D human pose estimations with the CFHS training strategy reached 99.85% PCKh@0.5. For 3-D results, the mean per joint position error was 25.22 mm, and the 3DPCK reached 98.71%, which outperformed existing general video-based methods. The results showed that the proposed system is capable of estimating human poses with high accuracy for clinical applications.",Heating systems; Training; Pose estimation; Kernel; Videos; Hospitals; Deep learning; Computer-assisted rehabilitation environment (CAREN); gait analysis; human pose estimation; motion capture; orthopedic patients
"Visual relationship detection (VRD) is one newly developed computer vision task, aiming to recognize relations or interactions between objects in an image. It is a further learning task after object recognition, and is important for fully understanding images even the visual world. It has numerous applications, such as image retrieval, machine vision in robotics, visual question answer (VQA), and visual reasoning. However, this problem is difficult since relationships are not definite, and the number of possible relations is much larger than objects. So the complete annotation for visual relationships is much more difficult, making this task hard to learn. Many approaches have been proposed to tackle this problem especially with the development of deep neural networks in recent years. In this survey, we first introduce the background of visual relations. Then, we present categorization and frameworks of deep learning models for visual relationship detection. The high-level applications, benchmark datasets, as well as empirical analysis are also introduced for comprehensive understanding of this task.",Visualization; Task analysis; Deep learning; Cognition; Object detection; Training; Semantics; Deep learning; detection; neural networks; visual relation
A green revolution has accelerated over the recent decades with a look to replace existing transportation power solutions through the adoption of greener electrical alternatives. In parallel the digitisation of manufacturing has enabled progress in the tracking and traceability of processes and improvements in fault detection and classification. This paper explores electrical machine manufacture and the challenges faced in identifying failures modes during this life cycle through the demonstration of state-of-the-art machine vision methods for the classification of electrical coil winding defects. We demonstrate how recent generative adversarial networks can be used to augment training of these models to further improve their accuracy for this challenging task. Our approach utilises pre-processing and dimensionality reduction to boost performance of the model from a standard convolutional neural network (CNN) leading to a significant increase in accuracy.,Electrical machines; Machine learning; Computer vision; Manufacturing; Coil winding
"Image semantic segmentation is an important part of fundamental in image interpretation and computer vision. With the development of convolutional neural network technology, deep learning-based image semantic segmentation methods have received more and more attention and research. At present, many excellent semantic segmentation methods have been proposed and applied in the field of remote sensing. In this paper, we summarized the semantic segmentation methods used for remote sensing image, including the traditional remote sensing image semantic segmentation methods and the methods based on deep learning, we emphasize on summarizing the remote sensing image semantic segmentation algorithms based on deep learning and classify them into different categories, and then we introduce the datasets that commonly used and data preparation methods including pre-processing and augmentation techniques. Finally, the challenges and future directions of research in this domain are analyzed and prospected. It is hoped that this study can widen the frontiers of knowledge and provide useful literature for researchers interested in advancing this field of research.",Deep learning; Image semantic segmentation; Remote sensing image; Computer vision
"Person re-identification (Person re-ID) is a challenging task in the field of computer vision. Generally, re-ID is regarded as an image retrieval problem; this task aims at searching a query person across multiple non-overlapping cameras. Re-ID is an important research direction after facial recognition that has many application scenarios, such as intelligent security, intelligent homing system, and unmanned supermarket. In practice, the identification would be deteriorated by the pedestrian postures, occlusions, and lighting conditions. Cropped images of pedestrians are often obtained from automatic detectors; this type of detection introduces two types of errors: part missing and excessive background. In previous works, fine-grained information has been proved to be useful for pedestrian retrieval. In this paper, we observe that multiple attention blocks can perform long-range multi-hop communication. To tackle the problem of excessive background, multistage cascaded attention blocks are introduced to put more focus on the information about human body. On the other hand, the part-level features have been proven to be effective against high quality of feature representation, but considering traditional vertical segmentation brings part inconsistency and lost some detailed semantic clues; the concept of overlapping features has been proposed to overcome this problem. Experiments on two large-scale re-ID datasets show that our method improves the learned representation of the feature embeddings and achieved competitive results.",Computer vision; Deep learning; Person re-identification; Attention mechanisms; Image retrieval
"The depth of an asphalt pavement structure is an important index used to reflect the anti-skid performance of the pavement, which directly affects the driving safety of vehicles. In the current specifications (e.g., China, European and American standards), the mean texture depth (MTD) of an asphalt pavement is generally measured by the sand-patch method (SPM). However, SPM is easily affected by the operator's subjective experience and experimental environment, resulting in low accuracy and high scatter in the data. In view of such shortcomings, a fast and accurate method to obtain the texture depth of asphalt pavement was proposed by designing a new test tool and a computer-aided calculation method. The multiocular vision theory was adopted to reconstruct a threedimensional (3D) point cloud model of pavement texture. To reflect the concavity and convexity of pavement texture and eliminate the influence of pavement slope, the iterative closest point (ICP) algorithm was employed to obtain the datum reference plane (DRP). On this basis, the pentahedral volume calculation method suitable for evaluating the texture depth of an asphalt pavement was proposed. Laboratory experiment results using SPM were inverted by equal volume calculation to obtain the texture reference plane (TRP) and MTD'. Meanwhile, a 3D average maximum elevation (Avg.EL) algorithm was obtained by the evolution of two-dimensional mean profile depth (MPD). Furthermore, three parameters, namely MTD, MTD', and Avg.EL, were used to measure the texture depth of two field pavement sections in service. The experimental results showed that the correlation between MTD' and MTD was better than that between Avg.EL and MTD, and the degree of scatter was smaller. MTD' could be used directly as a reference value for MTD. In the present work, the 3D volume calculation method of asphalt pavement texture depth was proposed, which integrates the technical flow of image acquisition, model reconstruction, and algorithm analysis. The developed method provides a technical reference for the application of computer vision technology in the analysis of pavement texture depth.",Mean texture depth; Multiocular vision; Structure from motion; 3D reconstruction; Iterative closest point; Sand-patch method
"Due to the need for increased security measures for monitoring and safeguarding the activities, video anomaly detection is considered as one of the significant research aspects in the domain of computer vision. Assigning human personnel to continuously check the surveillance videos for finding suspicious activities such as violence, robbery, wrong U-turns, to mention a few, is a laborious and error-prone task. It gives rise to the need for devising automated video surveillance systems ensuring security. Motivated by the same, this paper addresses the problem of detection and localization of anomalies from surveillance videos using pipelined deep autoencoders and one-class learning. Specifically, we used a convolutional autoencoder and a sequence-to-sequence long short-term memory autoencoder in a pipelined fashion for spatial and temporal learning of the videos, respectively. The authors followed the principle of one-class classification for training the model on normal data and testing it on anomalous testing data. The authors achieved a reasonably significant performance in terms of an equal error rate and the time required for anomaly detection and localization comparable to standard benchmarked approaches, thus, qualifies to work in a near-real-time manner for anomaly detection and localization.",computer vision; video surveillance
"Herein, we propose a framework for the generation of photorealistic synthetic datasets using High Dynamic Range Imaging (HDRI) that include all kinds of information computer vision algorithms need. Furthermore, by utilizing the proposed framework, we demonstrate cross-domain knowledge transfer in a semantic segmentation scenario. We found that deep neural networks trained with our synthetic images or with a mix of real and synthetic perform equal to or in cases better than those trained solely on real images. To our knowledge, this is the first work that uses HDRI to successfully transfer knowledge from the synthetic domain to the real world.",Artificial neural networks; knowledge transfer; robot vision; semantic segmentation; synthetic data
"Pedestrian detection is a critical task in the field of computer vision, and it has made considerable progress with the help of Convnets. However, a persistent crucial problem is that small-scale pedestrians are notoriously difficult to detect because of the introduction of weak contrast and blurred boundaries in real-world scenarios. In this paper, we present a simple and compact detection method for detecting multi-scale pedestrians, which is especially suitable for detecting small-scale pedestrians that are not easily recognized in images or videos. We first interpret convolutional neural network (CNN) channel features, explore the detection performance of different feature fusion methods, and propose a novel two-level feature fusion strategy specially designed for small-scale pedestrians. Moreover, a sub-network named prediction module is injected into the framework to improve the general performance without any bells and whistles. In addition, we propose an adaptive loss that adds an adaptive adjustment coefficient to the Smooth L1 loss function to enhance its robustness to pedestrian detection tasks. Using these methods synthetically, we achieve state-of-the-art detection performance on the Caltech pedestrian dataset under three evaluation protocols; particularly, the performance of small-scale pedestrians under Far evaluation setting is improved (miss rate decreases from 70.97% to 60.09%). Further, the proposed method achieves a competitive speed-accuracy trade-off with 0.31 second per image of 1024x2048 pixels on the CityPersons dataset.",Computer vision; Convnets; Pedestrian detection; Feature fusion; Loss function
"Human-computer interaction (HCI) is a prominent development that provides autonomous and pervasive service broadcasting for in-person communications. Computer vision techniques are assimilated with HCI for human detection and object classification in autonomous communication environments. This article introduces an application dependable interaction module (ADIM) that is optimal in recognizing humans and other autonomous systems for communication. This recognition helps to precisely detect the application demands of the user/system for flawless service broadcast. In this process, a deep belief network is used for the persistent analysis of the behavior of the system/human in the interacting end. The behavior is characterized using touch, voice, and commands/requests for identifying the object at the initial stage. The metrics sharing delay, response latency, interaction failures, recognition ratio, and error are employed to verify the proposed module's performance.",Computer vision; Deep belief networks; Human-computer interaction; Interaction behavior; Multimodal processing
"Many of the research problems in robot vision involve the detection of keypoints, areas with salient information in the input images and the generation of local descriptors, that encode relevant information for such keypoints. Computer vision solutions have recently relied on Deep Learning techniques, which make extensive use of the computational capabilities available. In autonomous robots, these capabilities are usually limited and, consequently, images cannot be processed adequately. For this reason, some robot vision tasks still benefit from a more classic approach based on keypoint detectors and local descriptors. In 2D images, the use of binary representations for visual tasks has shown that, with lower computational requirements, they can obtain a performance comparable to classic real-value techniques. However, these achievements have not been fully translated to 3D images, where research is mainly focused on real-value approaches. Thus, in this paper, we propose a keypoint detector and local descriptor based on 3D binary patterns. The experimentation demonstrates that our proposal is competitive against state-of-the-art techniques, while its processing can be performed more efficiently.",Shape binary patterns; Point clouds; Local descriptor; Keypoint detector
"There have been few anatomical structure segmentation studies using deep learning. Numbers of training and ground truth images applied were small and the accuracies of which were low or inconsistent. For a surgical video anatomy analysis, various obstacles, including a variable fast-changing view, large deformations, occlusions, low illumination, and inadequate focus occur. In addition, it is difficult and costly to obtain a large and accurate dataset on operational video anatomical structures, including arteries. In this study, we investigated cerebral artery segmentation using an automatic ground-truth generation method. Indocyanine green (ICG) fluorescence intraoperative cerebral videoangiography was used to create a ground-truth dataset mainly for cerebral arteries and partly for cerebral blood vessels, including veins. Four different neural network models were trained using the dataset and compared. Before augmentation, 35,975 training images and 11,266 validation images were used. After augmentation, 260,499 training and 90,129 validation images were used. A Dice score of 79% for cerebral artery segmentation was achieved using the DeepLabv3+ model trained using an automatically generated dataset. Strict validation in different patient groups was conducted. Arteries were also discerned from the veins using the ICG videoangiography phase. We achieved fair accuracy, which demonstrated the appropriateness of the methodology. This study proved the feasibility of operating field view of the cerebral artery segmentation using deep learning, and the effectiveness of the automatic blood vessel ground truth generation method using ICG fluorescence videoangiography. Using this method, computer vision can discern blood vessels and arteries from veins in a neurosurgical microscope field of view. Thus, this technique is essential for neurosurgical field vessel anatomy-based navigation. In addition, surgical assistance, safety, and autonomous surgery neurorobotics that can detect or manipulate cerebral vessels would require computer vision to identify blood vessels and arteries.",semantic segmentation; neural network; blood vessel; indocyanine green; neurosurgical field; computer vision; deep learning; cerebral artery
"High-quality clear image can not only bring a good subjective feeling, but also provide good performance guarantee for subsequent computer vision tasks in practical industrial applications. How to improve the low-light image quality and obtain clear image is a challenging task in computer vision. In order to ensure that clear images can be obtained under harsh lighting conditions, we propose a new low-light image enhancement network with decomposition and adaptive information fusion strategy. It firstly decomposes the image by decomposition network, which can obtain a reflection map with more details. Next, a brightness perception network is used to obtain the global and local brightness features of the input image. In addition, we employ an adaptive information fusion module (AIFM) to deal with the redundant information and noise in the multiple features. The experimental results show that the proposed network can not only restore the visually satisfactory image brightness, but also effectively remove the noise and get clear enhancement results. Specifically, the proposed method can achieve 22.20dB PSNR and 0.8380 SSIM gain on LOL dataset, which is the best performance and significantly improved compared with the state-of-the-art methods. We also illustrate the performance by NIQE scores with the proposed method and other comparable algorithms on several other real-world low-light benchmarks datasets including NPE, DICM and LIME, which also indicate that the proposed method has good generalization ability and superiority.",Low-light image enhancement; Image decomposition; Adaptive information fusion; Brightness perception; Convolutional neural network
"Computer vision systems are commonly used to design touch-less human-computer interfaces (HCI) based on dynamic hand gesture recognition (HGR) systems, which have a wide range of applications in several domains, such as, gaming, multimedia, automotive, home automation. However, automatic HGR is still a challenging task, mostly because of the diversity in how people perform the gestures. In addition, the number of publicly available hand gesture datasets is scarce, often the gestures are not acquired with sufficient image quality, and the gestures are not correctly performed. In this data article, we propose a dataset of 27 dynamic hand gesture types acquired at full HD resolution from 21 different subjects, which were carefully instructed before performing the gestures and monitored when performing the gesture; the subjects had to repeat the movement in case the performed hand gesture was not correct, i.e., the authors of this paper that were observing the gesture found that it did not correspond to the exact expected movement and/or the camera recorded a viewpoint did not allow for a plain visualizing of the gesture. Each subject performed 3 times the 27 hand gestures for a total of 1701 videos collected and corresponding 204,120 video frames.",Dynamic hand gesture recognition; Hand gesture dataset; Human-computer interface; Touch-less screen; Computer vision
"Surface cracks in concrete structures are an important indicator of the soundness of a structure. Stereo vision, consisting of two identical cameras, has been suggested to quantify crack characteristics using derived depth information. However, because high measurement resolution is required, zoom lenses are often used, making simultaneously crack localization and characterization difficult. This study presents a framework for the use of stereo vision employing one wide-angle lens and one telephoto lens, enabling accurate crack quantification as well as efficient 3D reconstruction. Furthermore, a robust depth estimation strategy is proposed for planar surfaces, such as are found in most concrete bridges. The performance of the proposed approach is field validated using an in-service concrete bridge. The 3D reconstruction model generated by a set of wide-angle images, including crack information extracted from the telephoto images using deep learning, can enable the improved inspection of concrete structures.",Computer vision; Concrete crack; Crack evaluation; Deep learning; Stereo vision
"Varietal control to avoid unwanted varietal mixtures is an important objective for the nursery plant industry. In this study, we have developed and analyzed the capabilities of a computer vision system based on deep learning for the control of plant varieties in the nursery plant industry and for evaluating its capabilities. For this purpose, three datasets of nursery plant images were compared. The datasets came from two varieties of almond trees (Prunus dulcis) named Soleta and Pentacebas. Each dataset contained images with three different scales: whole plant, leaf, and venation. The Gradient-weighted Class Activation Mapping (Grad-CAM) technique was used to unveil the most important features to discriminate between both varieties. The three datasets provided classification accuracies above 97% in the test set, being the leaf dataset, with a 98.8% accuracy, the one providing the best results. Concerning the most important features of the plants, the Grad-CAM showed that they are located in the center of the leaf, that is, the venation. In conclusion, we have shown that computer vision is a promising technique for the control of plant varietal mixtures.",computer vision; convolutional neural network; deep learning; nursery plant; varietal mixture
"Chest radiography (X-ray) is the most common diagnostic method for pulmonary disorders. A trained radiologist is required for interpreting the radiographs. But sometimes, even experienced radiologists can misinterpret the findings. This leads to the need for computer-aided detection diagnosis. For decades, researchers were automatically detecting pulmonary disorders using the traditional computer vision (CV) methods. Now the availability of large annotated datasets and computing hardware has made it possible for deep learning to dominate the area. It is now the modus operandi for feature extraction, segmentation, detection, and classification tasks in medical imaging analysis. This paper focuses on the research conducted using chest X-rays for the lung segmentation and detection/classification of pulmonary disorders on publicly available datasets. The studies performed using the Generative Adversarial Network (GAN) models for segmentation and classification on chest X-rays are also included in this study. GAN has gained the interest of the CV community as it can help with medical data scarcity. In this study, we have also included the research conducted before the popularity of deep learning models to have a clear picture of the field. Many surveys have been published, but none of them is dedicated to chest X-rays. This study will help the readers to know about the existing techniques, approaches, and their significance.",Deep convolutional neural network; Computer vision; Lung segmentation; Multiclass classification; Nodule; TB; COVID-19; Pneumothorax detection; GAN
"In this paper, one of the most novel topics in Deep Learning (DL) is explored: Visual Question Answering (VQA). This research area uses three of the most important fields in Artificial Intelligence (AI) to automatically provide natural language answers for questions that a user can ask about an image. These fields are: 1) Computer Vision (CV), 2) Natural Language Processing (NLP) and 3) Knowledge Representation & Reasoning (KR&R). Initially, a review of the state of art in VQA and our contributions to it are discussed. Then, we build upon the ideas provided by Pythia, which is one of the most outstanding approaches. Therefore, a study of the Pythia's architecture is carried out with the aim of presenting varied enhancements with respect to the original proposal in order to fine-tune models using a bag of tricks. Several training strategies are compared to increase the global accuracy and understand the limitations associated with VQA models. Extended results check the impact of the different tricks over our enhanced architecture, jointly with additional qualitative results.",Computer vision; Natural language processing; Knowledge representation & reasoning; Visual question answering; Artificial intelligence
"Establishing links between the processing, the microstructure and the properties of steel is of utmost importance for rational material design. Including information from the microstructure proves difficult as it requires quantifying the visual information included in microscopy images. The increasing performance of deep learning models in computer vision offers great potential in this regard. Herein, we investigate how features from deep learning models can help us to predict information about the hardness and the composition based on SEM images for a set of complex martensitic steels.(c) 2021 Acta Materialia Inc. Published by Elsevier Ltd. All rights reserved.",
"Although computer vision-based methods are emerging due to the advances in mobile imaging technologies, the current approach to assessing elastomeric-bearing conditions heavily relies on human visual inspection. Furthermore, few computer-vision efforts are found for automatic condition assessment for the bearings, partially due to the challenge in acquiring a specific and large-scale database in practice. Through developing a unique imagery database with engineering-meaningful condition labels, this paper first benchmarks the performance by utilizing the traditional computer vision framework using scale-invariant feature transform (SIFT) features and support vector machine (SVM) classifier. By adopting three different kinds of convolutional neural networks (CNN) architectures (AlexNet, VGG-11, and ResNet-18), this paper contributes by evaluating different CNN architectures on small size elastomeric bearing dataset. Also, transfer learning (TL) techniques are applied to improve the performance of CNN models. Furthermore, different training strategies including using pretrained weights as fixed feature extractors and fully finetune the network architecture are evaluated in this paper. The authors conclude that the CNN models equipped with fully fine-tuned TL techniques possess much satisfactory performance and hold promising to be used for real-world applications for automating elastomeric-bearing condition assessment.",Elastomeric bearings; Damage detection; Deep learning; Convolutional neural network; Small data
"Recognition of construction waste compositions using computer vision (CV) is increasingly explored to enable its subsequent management, e.g., determining chargeable levy at disposal facilities or waste sorting using robot arms. However, the applicability of existing CV-enabled construction waste recognition in real-life scenarios is limited by their relatively low accuracy, characterized by a failure to distinguish boundaries among different waste materials. This paper aims to propose a novel boundary-aware Transformer (BAT) model for fine-grained composition recognition of construction waste mixtures. First, a pre-processing workflow is devised to separate the hard-to-recognize edges from the background. Second, a Transformer structure with a self-designed cascade decoder is developed to segment different waste materials from construction waste mixtures. Finally, a learning-enabled edge refinement scheme is used to fine-tune the ignored boundaries, further boosting the segmentation precision. The performance of the BAT model was evaluated on a benchmark dataset comprising nine types of materials in a cluttered and mixture state. It recorded a 5.48% improvement of MIoU (mean intersection over union) and 3.65% of MAcc (Mean Accuracy) against the baseline. The research contributes to the body of interdisciplinary knowledge by presenting a novel deep learning model for construction waste material semantic segmentation. It can also expedite the applications of CV in construction waste management to achieve a circular economy.",Circular economy; Waste recycling; Construction and demolition waste; Artificial intelligence; Material recognition; Semantic segmentation
"Shadow removal is a fundamental and pivotal task to build the high-level cognition in the computer vision field. Due to the fact that the existing shadow removal methods cannot effectively remove shadows from the outdoor image and deal with shadow boundaries, we construct a convolutional neural network without the process of shadow detection for the shadow removal task. The constructed CNN avoids the risk of gradient vanishing by designing double-attention residual block and improves the performance of shadow removal by fusing the knowledge transfer idea. Specially, we design a hand-crafted feature, named brightness-gradient difference feature, to distinguish shadow boundary pixels from non-shadow boundary pixels, and the designed feature is fused into the loss function to dilute or even eliminate the existing shadow boundaries. Extensive experiments using three public shadow removal benchmarks with three measurable indicators are reported in this paper. The results of experiments demonstrate that the proposed method has an effective performance for the shadow removal task. The ablation studies validate the structural rationality of the proposed method. (c) 2021 Elsevier B.V. All rights reserved.",Computer vision; Shadow removal; Knowledge transfer; Brightness-gradient difference feature
"The seismic performance of a building must be evaluated after it has been affected by an earthquake load. In the evaluation process, building codes and standards require that the drift of the structure is determined to assess structural performance. This study provides an innovative method that helps engineers in measuring the deflection of reinforced concrete (RC) beams. An imagery deep learning model, called residual networks (ResNet), is used to classify the deflection based on observation by computer vision. However, determining the optimal values of the hyperparameters of this model is a challenge. Therefore, a hybrid model that integrates the bio-inspired optimization (i.e., jellyfish search [JS] algorithm) and ResNet is developed. The input data that are used to train the model are images that are collected in RC structural experiments. This experiment involved 29 cantilever beams with various RC designs. These specimen RC beams were tested under simulated seismic loads with lateral displacement control. After each load had been applied to the beam, four single-lens digital cameras captured images from the east, west, north, and south. Then, the performance of computer vision-based JS-ResNet was evaluated by comparing its accuracy with that of the original ResNet using default hyperparameters. The results of the analysis show that the proposed JS-ResNet model achieves higher accuracy than conventional ResNet. Therefore, the hybrid model can provide insights in similar visual surveillance tasks.",beam behavior; deep learning for computer vision; deflection detection; drift analysis; hybrid residual networks; jellyfish search optimizer; reinforced concrete structure; seismic loads; structural health monitoring
"Plants are often attacked by various pathogens during their growth, which may cause environmental pollution, food shortages, or economic losses in a certain area. Integration of high throughput phenomics data and computer vision (CV) provides a great opportunity to realize plant disease diagnosis in the early stage and uncover the subtype or stage patterns in the disease progression. In this study, we proposed a novel computational framework for plant disease identification and subtype discovery through a deep-embedding image-clustering strategy, Weighted Distance Metric and the t-stochastic neighbor embedding algorithm (WDM-tSNE). To verify the effectiveness, we applied our method on four public datasets of images. The results demonstrated that the newly developed tool is capable of identifying the plant disease and further uncover the underlying subtypes associated with pathogenic resistance. In summary, the current framework provides great clustering performance for the root or leave images of diseased plants with pronounced disease spots or symptoms.",plant; disease diagnosis; subtype discovery; deep learning; t-SNE; image clustering
"Image segmentation is a very important topic in the field of computer vision. We present a method for semantic segmentation of selected stuff classes from a superset of classes. We show that in situations where only select stuff classes are required if we group them as per a strategy then it can attain much higher accuracy than the models trained on the original dataset with all classes intact. The COCO-Stuff Dataset is used for demonstrating the aforesaid strategy. For training purposes, the DeepLabv3+ with Mobilenet-v2 architecture is used. We have achieved an 80.2 percent mean Intersection over Union (mIoU) on these selected classes. We also refine the masks using Learning/Computer Vision (CV) methods and hence obtain better visualization results as compared to the existing DeepLabv3+ results. (c) 2021 Elsevier Ltd. All rights reserved.",Image segmentation; Stuff classes; Deeplab
"Human Action Recognition (HAR) is a current research topic in the field of computer vision that is based on an important application known as video surveillance. Researchers in computer vision have introduced various intelligent methods based on deep learning and machine learning, but they still face many challenges such as similarity in various actions and redundant features. We proposed a framework for accurate human action recognition (HAR) based on deep learning and an improved features optimization algorithm in this paper. From deep learning feature extraction to feature classification, the proposed framework includes several critical steps. Before training fine-tuned deep learning models - MobileNet-V2 and Darknet53 - the original video frames are normalized. For feature extraction, pre-trained deep models are used, which are fused using the canonical correlation approach. Following that, an improved particle swarm optimization (IPSO)-based algorithm is used to select the best features. Following that, the selected features were used to classify actions using various classifiers. The experimental process was performed on six publicly available datasets such as KTH, UT-Interaction, UCF Sports, Hollywood, IXMAS, and UCF YouTube, which attained an accuracy of 98.3%, 98.9%, 99.8%, 99.6%, 98.6%, and 100%, respectively. In comparison with existing techniques, it is observed that the proposed framework achieved improved accuracy.",Action recognition; deep learning; features fusion; features selection; recognition
"Crowd density estimation is an important topic in computer vision due to its widespread applications in surveillance, urban planning, and intelligence gathering. Resulting from extensive analysis, crowd density estimation reflects many aspects such as similarity of appearance between people, background components, and inter-blocking in intense crowds. In this paper, we are interested to apply machine learning for crowd management in order to monitor populated area and prevent congestion situations. We propose a Single-Convolutional Neural Network with Three Layers (S-CNN3) model to count the number of people in a scene and conclude about the crowd estimation. Then, a comparative study for density counting establishes the performance of the proposed model against the convolutional neural networks with four layers (single-CNN4) and Switched Convolutional neural networks (SCNN). ShanghaiTech dataset, considered as the largest data base for crowd counting, is used in this work. The proposed model proves high effectiveness and efficiency for crowd density estimation with 99.88% of average test accuracy and 0.02 of average validation loss. These results achieve better performance than the existing state-of-the-art models.",Estimation; Convolutional neural networks; Monitoring; Neural networks; Computer vision; Feature extraction; Deep learning; Crowd counting; density estimation; GPU; switching convolutional neural network (SCNN)
"These days, deep learning and computer vision are much-growing fields in this modern world of information technology. Deep learning algorithms and computer vision have achieved great success in different applications like image classification, speech recognition, self-driving vehicles, disease diagnostics, and many more. Despite success in various applications, it is found that these learning algorithms face severe threats due to adversarial attacks. Adversarial examples are inputs like images in the computer vision field, which are intentionally slightly changed or perturbed. These changes are humanly imperceptible. But are misclassified by a model with high probability and severely affects the performance or prediction. In this scenario, we present a deep image restoration model that restores adversarial examples so that the target model is classified correctly again. We proved that our defense method against adversarial attacks based on a deep image restoration model is simple and state-of-the-art by providing strong experimental results evidence. We have used MNIST and CIFAR10 datasets for experiments and analysis of our defense method. In the end, we have compared our method to other state-ofthe-art defense methods and proved that our results are better than other rival methods.",Computer vision; deep learning; convolutional neural networks; adversarial examples; adversarial attacks; adversarial defenses
"The current development of artificial intelligence is largely based on deep Neural Networks (DNNs). Especially in the computer vision field, DNNs now occur in everything from autonomous vehicles to safety control systems. Convolutional Neural Network (CNN) is based on DNNs mostly used in different computer vision applications, especially for image classification and object detection. The CNN model takes the photos as input and, after training, assigns it a suitable class after setting traceable parameters like weights and biases. CNN is derived from Human Brain's Part Visual Cortex and sometimes performs even better than Haman visual system. However, recent research shows that CNN Models are much vulnerable against adversarial examples. Adversarial examples are input image huts that are deliberately modified, which are imperceptible to humans, but a CNN model strongly misrepresents them. This means that adversarial attacks or examples are a serious threat to deep learning models, especially for CNNs in the computer vision field. The methods which are used to create adversarial examples are called adversarial attacks. We have proposed an easy method that restores adversarial examples, which are created due to different adversarial attacks and misclassified by a CNN model. Our reconstructed adversarial examples are correctly classified by a model again with high probability and restore the prediction of a CNN model. We will also prove that our method is based on image arithmetic operations, simple, single-step, and has low computational complexity. Our method is to reconstruct all types of adversarial examples for correct classification. Therefore, we can say that our proposed method is universal or transferable. The datasets used for experimental evidence are MNIST, FASHION-MNIST, CIFAR10, and CALTECH-101. In the end, we have presented a comparative analysis with other state-of-the methods and proved that our results are better.",Computer vision; deep learning; convolutional neural network; adversarial attacks; adversarial examples; and adversarial defense methods
"Sign language recognition can be considered as an effective solution for disabled people to communicate with others. It helps them in conveying the intended information using sign languages without any challenges. Recent advancements in computer vision and image processing techniques can be leveraged to detect and classify the signs used by disabled people in an effective manner. Metaheuristic optimization algorithms can be designed in a manner such that it fine tunes the hyper parameters, used in Deep Learning (DL) models as the latter considerably impacts the classification results. With this motivation, the current study designs the Optimal Deep Transfer Learning Driven Sign Language Recognition and Classification (ODTL-SLRC) model for disabled people. The aim of the proposed ODTL-SLRC technique is to recognize and classify sign languages used by disabled people. The proposed ODTL-SLRC technique derives EfficientNet model to generate a collection of useful feature vectors. In addition, the hyper parameters involved in EfficientNet model are fine-tuned with the help of HGSO algorithm. Moreover, Bidirectional Long Short Term Memory (BiLSTM) technique is employed for sign language classification. The proposed ODTL-SLRC technique was experimentally validated using benchmark dataset and the results were inspected under several measures. The comparative analysis results established the superior performance of the proposed ODTL-SLRC technique over recent approaches in terms of efficiency.",Sign language; image processing; computer vision; disabled people; deep learning; parameter tuning
"The performance and accuracy of computer vision systems are affected by noise in different forms. Although numerous solutions and algorithms have been presented for dealing with every type of noise, a comprehensive technique that can cover all the diverse noises and mitigate their damaging effects on the performance and precision of various systems is still missing. In this paper, we have focused on the stability and robustness of one computer vision branch (i.e., visual object tracking). We have demonstrated that, without imposing a heavy computational load on a model or changing its algorithms, the drop in the performance and accuracy of a system when it is exposed to an unseen noise-laden test dataset can be prevented by simply applying the style transfer technique on the train dataset and training the model with a combination of these and the original untrained data. To verify our proposed approach, it is applied on a generic object tracker by using regression networks. This method's validity is confirmed by testing it on an exclusive benchmark comprising 50 image sequences, with each sequence containing 15 types of noise at five different intensity levels. The OPE curves obtained show a 40% increase in the robustness of the proposed object tracker against noise, compared to the other trackers considered.",Style transfer; visual object tracking; robustness; corruption
"Deep learning approaches have recently raised the bar in many fields, from Natural Language Processing to Computer Vision, by leveraging large amounts of data. However, they could fail when the retrieved information is not enough to fit the vast number of parameters, frequently resulting in overfitting and therefore in poor generalizability. Few-Shot Learning aims at designing models that can effectively operate in a scarce data regime, yielding learning strategies that only need few supervised examples to be trained. These procedures are of both practical and theoretical importance, as they are crucial for many real-life scenarios in which data is either costly or even impossible to retrieve. Moreover, they bridge the distance between current data-hungry models and human-like generalization capability. Computer vision offers various tasks that can be few-shot inherent, such as person re-identification. This survey, which to the best of our knowledge is the first tackling this problem, is focused on Few-Shot Object Detection, which has received far less attention compared to Few-Shot Classification due to the intrinsic challenge level. In this regard, this review presents an extensive description of the approaches that have been tested in the current literature, discussing their pros and cons, and classifying them according to a rigorous taxonomy.",Deep learning for few-shot object detection; dataset for object detection; benchmarks and metrics for object detection
"In recent years complex food security issues caused by climatic changes, limitations in human labour, and increasing production costs require a strategic approach in addressing problems. The emergence of artificial intelligence due to the capability of recent advances in computing architectures could become a new alternative to existing solutions. Deep learning algorithms in computer vision for image classification and object detection can facilitate the agriculture industry, especially in paddy cultivation, to alleviate human efforts in laborious, burdensome, and repetitive tasks. Optimal planting density is a crucial factor for paddy cultivation as it will influence the quality and quantity of production. There have been several studies involving planting density using computer vision and remote sensing approaches. While most of the studies have shown promising results, they have disadvantages and show room for improvement. One of the disadvantages is that the studies aim to detect and count all the paddy seedlings to determine planting density. The defective paddy seedlings' locations are not pointed out to help farmers during the sowing process. In this work we aimed to explore several deep convolutional neural networks (DCNN) models to determine which one performs the best for defective paddy seedling detection using aerial imagery. Thus, we evaluated the accuracy, robustness, and inference latency of one- and two-stage pretrained object detectors combined with state-of-the-art feature extractors such as EfficientNet, ResNet50, and MobilenetV2 as a backbone. We also investigated the effect of transfer learning with fine-tuning on the performance of the aforementioned pretrained models. Experimental results showed that our proposed methods were capable of detecting the defective paddy rice seedlings with the highest precision and an F1-Score of 0.83 and 0.77, respectively, using a one-stage pretrained object detector called EfficientDet-D1 EficientNet.",paddy seedlings; computer vision; object detection; deep learning; convolutional neural networks
"Automatic image aesthetics assessment is a computer vision problem dealing with categorizing images into different aesthetic levels. The categorization is usually done by analyzing an input image and computing some measure of the degree to which the image adheres to the fundamental principles of photography such as balance, rhythm, harmony, contrast, unity, look, feel, tone, and texture. Due to its diverse applications in many areas, automatic image aesthetic assessment has gained significant research attention in recent years. This article presents a comparative study of different automatic image aesthetics assessment techniques from the year 2005 to 2021. A number of conventional hand-crafted as well as modern deep learning-based approaches are reviewed and analyzed for their performance on various publicly available datasets. Additionally, critical aspects of different features and models have also been discussed to analyze their performance and limitations in different situations. The comparative analysis reveals that deep learning based approaches excel hand-crafted based techniques in image aesthetic assessment.",Feature extraction; Image color analysis; Deep learning; Computer vision; Convolutional neural networks; Support vector machines; Photography; Quality assessment; Image aesthetic assessment; aesthetic visual perception; image quality assessment; computer vision; convolutional neural networks; deep learning
"Design of a vision-based traffic analytic system for urban traffic video scenes has a great potential in context of Intelligent Transportation System (ITS). It offers useful traffic-related insights at much lower costs compared to their conventional sensor based counterparts. However, it remains a challenging problem till today due to the complexity factors such as camera hardware constraints, camera movement, object occlusion, object speed, object resolution, traffic flow density, and lighting conditions etc. ITS has many applications including and not just limited to queue estimation, speed detection and different anomalies detection etc. All of these applications are primarily dependent on sensing vehicle presence to form some basis for analysis. Moving cast shadows of vehicles is one of the major problems that affects the vehicle detection as it can cause detection and tracking inaccuracies. Therefore, it is exceedingly important to distinguish dynamic objects from their moving cast shadows for accurate vehicle detection and recognition. This paper provides an in-depth comparative analysis of different traffic paradigm-focused conventional and state-of-the-art shadow detection and removal algorithms. Till date, there has been only one survey which highlights the shadow removal methodologies particularly for traffic paradigm. In this paper, a total of 70 research papers containing results of urban traffic scenes have been shortlisted from the last three decades to give a comprehensive overview of the work done in this area. The study reveals that the preferable way to make a comparative evaluation is to use the existing Highway I, II, and III datasets which are frequently used for qualitative or quantitative analysis of shadow detection or removal algorithms. Furthermore, the paper not only provides cues to solve moving cast shadow problems, but also suggests that even after the advent of Convolutional Neural Networks (CNN)-based vehicle detection methods, the problems caused by moving cast shadows persists. Therefore, this paper proposes a hybrid approach which uses a combination of conventional and state-of-the-art techniques as a pre-processing step for shadow detection and removal before using CNN for vehicles detection. The results indicate a significant improvement in vehicle detection accuracies after using the proposed approach.",Vehicle detection; Feature extraction; Intelligent transportation systems; Streaming media; Classification algorithms; Computer vision; Lighting; Deep learning; Generative adversarial Networks; Computer vision; convolutional neural networks; deep learning; generative adversarial networks; intelligent transportation system; moving cast shadow removal; vehicle shadows; vehicle detection
"This article proposes a novel technique for trailer angle detection (TAD) for use in an advanced trailer backup assistance system (TBAS) to accomplish a semiautonomous or full-autonomous backup maneuver. TBAS incorporates a combined trailer-tow vehicle kinematic model that requires an accurate estimate of the hitch angle. The proposed computer vision (CV) and machine learning (ML) TAD model process the image frames, acquired from the rear-facing camera, to detect and track the trailer and estimate its orientation in relation to the tow vehicle. The technique is based on a deep learning object detection and CV tracking model to detect and track one or more identifiable objects on the trailer (the marker-lights on the front edges of the trailer in this work). The estimated positions of the detected marker-lights are used to perform the hitch angle estimation. The model detects the hitch angle within the specified limit with an acceptance rate of 98%. The model is implemented in real time with a processing rate of more than 30 frames/s.",Cameras; Computational modeling; Object detection; Lenses; Kinematics; Estimation; Feature extraction; Advanced driver assistance system (ADAS); deep-learning; fisheye camera; object detection; trailer angle detection (TAD)
"Attention mechanisms have been explored with CNNs across the spatial and channel dimensions. However, all the existing methods devote the attention modules to capture local interactions from a uni-scale. This paper tackles the following question: can one consolidate multi-scale aggregation while learning channel attention more efficiently? To this end, we avail channel-wise attention over multiple feature scales, which empirically shows its aptitude to replace the limited local and uni-scale attention modules. EMCA is lightweight and can efficiently model the global context further; it is easily integrated into any feed-forward CNN architectures and trained in an end-to-end fashion. We validate our novel architecture through comprehensive experiments on image classification, object detection, and instance segmentation with different backbones. Our experiments show consistent gains in performances against their counterparts, where our proposed module, named EMCA, outperforms other channel attention techniques in accuracy and latency trade-off. More specifically, compared to SENet, we boost the accuracy by 0.8 %, 0.6 %, and 1 % on ImageNet benchmark for ResNet-18, 34, and 50, respectively. For detection and segmentation tasks, MS-COCO are for benchmarking, Our EMCA module boost the accuracy by 0.5 % and 0.3 %, respectively. We also conduct experiments that probe the robustness of the learned representations. Our code will be published once the paper is accepted.",Feature extraction; Context modeling; Computer architecture; Channel estimation; Transformers; Image segmentation; Deep learning; Machine learning; Computer vision; Image processing; Classification algorithms; Channel attention module; deep learning; machine learning; computer vision; object classification; CNN backbones; CNN encoders; CNNs; convolutions; image processing
"Recent advancements in deep learning architecture have increased its utility in real-life applications. Deep learning models require a large amount of data to train the model. In many application domains, there is a limited set of data available for training neural networks as collecting new data is either not feasible or requires more resources such as in marketing, computer vision, and medical science. These models require a large amount of data to avoid the problem of overfitting. One of the data space solutions to the problem of limited data is data augmentation. The purpose of this study focuses on various data augmentation techniques that can be used to further improve the accuracy of a neural network. This saves the cost and time consumption required to collect new data for the training of deep neural networks by augmenting available data. This also regularizes the model and improves its capability of generalization. The need for large datasets in different fields such as computer vision, natural language processing, security, and healthcare is also covered in this survey paper. The goal of this paper is to provide a comprehensive survey of recent advancements in data augmentation techniques and their application in various domains.",Deep learning; data augmentation; transfer learning; cost sensitive learning; generalization; overfitting
"Recently, vision-language models based on transformers are gaining popularity for joint modeling of visual and textual modalities. In particular, they show impressive results when transferred to several downstream tasks such as zero and few-shot classification. In this article, we propose a visual question answering (VQA) approach for remote sensing images based on these models. The VQA task attempts to provide answers to image-related questions. In contrast, VQA has gained popularity in computer vision, in remote sensing, it is not widespread. First, we use the contrastive language image pretraining (CLIP) network for embedding the image patches and question words into a sequence of visual and textual representations. Then, we learn attention mechanisms to capture the intradependencies and interdependencies within and between these representations. Afterward, we generate the final answer by averaging the predictions of two classifiers mounted on the top of the resulting contextual representations. In the experiments, we study the performance of the proposed approach on two datasets acquired with Sentinel-2 and aerial sensors. In particular, we demonstrate that our approach can achieve better results with reduced training size compared with the recent state-of-the-art.",Transformers; Task analysis; Remote sensing; Visualization; Feature extraction; Head; Computer vision; Co-attention; remote sensing; self-attention; transformers; vision-language models; visual question answering (VQA)
"Video captioning is a highly challenging computer vision task that automatically describes the video clips using natural language sentences with a clear understanding of the embedded semantics. In this work, a video caption generation framework consisting of discrete wavelet convolutional neural architecture along with multimodal feature attention is proposed. Here global, contextual and temporal features in the video frames are taken into account and separate attention networks are integrated into the visual attention predictor network to capture multiple attentions from these features. These attended features with textual attention are employed in the visual-to-text translator for caption generation. The experiments are conducted on two benchmark video captioning datasets - MSVD and MSR-VTT. The results prove an improved performance of the method with a CIDEr score of 91.7 and 52.2, for the aforementioned datasets, respectively.",Feature extraction; Visualization; Discrete wavelet transforms; Semantics; Decoding; Transformers; Natural languages; Video coding; Computer vision; Video captioning; discrete wavelet convolutional model; multimodal feature extraction; visual attention predictor
"The development of accurate methods for multi-label scene classification (MLC) of remote sensing (RS) images is one of the most important research topics in RS. To address MLC problems, the use of deep neural networks that require a high number of reliable training images annotated by multiple land-cover class labels (multi-labels) has been found popular in RS. However, collecting such annotations is time consuming and costly. A common procedure to obtain annotations at zero labeling cost is to rely on thematic products or crowdsourced labels. As a drawback, these procedures come with the risk of label noise that can distort the learning process of the MLC algorithms. In the literature, most label noise robust methods are designed for single-label classification (SLC) problems in computer vision (CV), where each image is annotated by a single label. Unlike SLC, label noise in MLC can be associated with: 1) subtractive label noise (a land cover class label is not assigned to an image while that class is present in the image); 2) additive label noise (a land cover class label is assigned to an image, although that class is not present in the given image); and 3) mixed label noise (a combination of both). In this article, we investigate three different noise robust CV SLC methods (self-adaptive training (SAT), early-learning regularization, and joint co-regularized training) and adapt them to be robust for multi-label noise scenarios in RS. During experiments, we study the effects of different types of multi-label noise and evaluate the adapted methods rigorously. To this end, we also introduce a synthetic multi-label noise injection strategy that is more adequate to simulate operational scenarios compared to the uniform label noise injection strategy, in which the labels of absent and present classes are flipped at uniform probability. Further, we study the relevance of different evaluation metrics in MLC problems under noisy multi-labels. On the basis of the theoretical and experimental analyses, some guidelines for a proper design of label noise robust MLC methods are derived.",Training; Noise robustness; Noise measurement; Annotations; Additive noise; Task analysis; Prototypes; Multi-label noise; multi-label scene classification (MLC); noisy labels; remote sensing (RS)
"With the advent of self-driving cars and the push by large companies into fully driverless transportation services, monitoring passenger behaviour in vehicles is becoming increasingly important for several reasons, such as ensuring safety and comfort. Although several human action recognition (HAR) methods have been proposed, developing a true HAR system remains a very challenging task. If the dataset used to train a model contains a small number of actors, the model can become biased towards these actors and their unique characteristics. This can cause the model to generalise poorly when confronted with new actors performing the same actions. This limitation is particularly acute when developing models to characterise the activities of vehicle occupants, for which data sets are short and scarce. In this study, we describe and evaluate three different methods that aim to address this actor bias and assess their performance in detecting in-vehicle violence. These methods work by removing specific information about the actor from the model's features during training or by using data that is independent of the actor, such as information about body posture. The experimental results show improvements over the baseline model when evaluated with real data. On the Hanau03 Vito dataset, the accuracy improved from 65.33% to 69.41%. On the Sunnyvale dataset, the accuracy improved from 82.81% to 86.62%.",Three-dimensional displays; Solid modeling; Computer vision; Data models; Monitoring; Feature extraction; Deep learning; Autonomous vehicles; Autonomous vehicles; computer vision; deep learning; domain generalization
"On the road to making self-driving cars a reality, academic and industrial researchers are working hard to continue to increase safety while meeting technical and regulatory constraints Understanding the surrounding environment is a fundamental task in self-driving cars. It requires combining complex computer vision algorithms. Although state-of-the-art algorithms achieve good accuracy, their implementations often require powerful computing platforms with high power consumption. In some cases, the processing speed does not meet real-time constraints. FPGA platforms are often used to implement a category of latency-critical algorithms that demand maximum performance and energy efficiency. Since self-driving car computer vision functions fall into this category, one could expect to see a wide adoption of FPGAs in autonomous cars. In this paper, we survey the computer vision FPGA-based works from the literature targeting automotive applications over the last decade. Based on the survey, we identify the strengths and weaknesses of FPGAs in this domain and future research opportunities and challenges.",Autonomous automobile; computer vision; field programmable gate arrays; reconfigurable architectures
"This paper focuses on visual attention, a state-of-the-art approach for image captioning tasks within the computer vision research area. We study the impact that different hyperparemeter configurations on an encoder-decoder visual attention architecture in terms of efficiency. Results show that the correct selection of both the cost function and the gradient-based optimizer can significantly impact the captioning results. Our system considers the cross-entropy, Kullback-Leibler divergence, mean squared error, and negative log-likelihood loss functions; the adaptive momentum (Adam), AdamW, RMSprop, stochastic gradient descent, and Adadelta optimizers. Experimentation shows that a combination of cross-entropy with Adam is the best alternative returning a Top-5 accuracy value of 73.092 and a BLEU-4 value of 20.10. Furthermore, a comparative analysis of alternative convolutional architectures demonstrated their performance as an encoder. Our results show that ResNext-101 stands out with a Top-5 accuracy of 73.128 and a BLEU-4 of 19.80; positioning itself as the best option when looking for the optimum captioning quality. However, MobileNetV3 proved to be a much more compact alternative with 2,971,952 parameters and 0.23 Giga fixed-point Multiply-Accumulate operations per Second (GMACS). Consequently, MobileNetV3 offers a competitive output quality at the cost of lower computational performance, supported by values of 19.50 and 72.928 for the BLEU-4 and Top-5 accuracy, respectively. Finally, when testing vision transformer (ViT), and data-efficient image transformer (DeiT) models to replace the convolutional component of the architecture, DeiT achieved an improvement over ViT, obtaining a value of 34.44 in the BLEU-4 metric.",Transformers; Computer architecture; Task analysis; Decoding; Measurement; Computational modeling; Computer vision; Image captioning; visual attention; computer vision; supervised learning; artificial intelligence
"We propose a system for monitoring the driving maneuver at road intersections using rule-based reasoning and deep learning-based computer vision techniques. Along with detecting and classifying turning movements online, the system also detects violations such as ignoring STOP signs and failing to yield the right-of-way to other drivers. There is no distinction between temporarily and permanently stopped vehicles in the majority of frameworks proposed in the literature. Therefore, to conduct an accurate right-of-way study, permanently stopped vehicles should be excluded not to confound the results. Moreover, we also propose in this work a low-cost Convolutional Neural Network (CNN)-based object detection framework able to detect moving and temporally stopped vehicles. The detection framework combines the reasoning system with background subtraction and a CNN-based object detector. The obtained results are promising. Compared to the conventional CNN-based methods, the detection framework reduces the execution time of the object detection module by about 30% (i.e., 54.1 instead of 75ms/image) while preserving the same detection reliability. The accuracy of trajectory recognition is 95.32%, that of the zero-speed detection is 96.67%, and the right-of-way detection was perfect.",Trajectory; Roads; Vehicles; Cognition; Monitoring; Turning; Accidents; Monitoring; driving behavior; road intersection; AI~reasoning; maneuver classification; computer vision; deep learning
"Transformer-based Deep Neural Network architectures have gained tremendous interest due to their effectiveness in various applications across Natural Language Processing (NLP) and Computer Vision (CV) domains. These models are the de facto choice in several language tasks, such as Sentiment Analysis and Text Summarization, replacing Long Short Term Memory (LSTM) model. Vision Transformers (ViTs) have shown better model performance than traditional Convolutional Neural Networks (CNNs) in vision applications while requiring significantly fewer parameters and training time. The design pipeline of a neural architecture for a given task and dataset is extremely challenging as it requires expertise in several interdisciplinary areas such as signal processing, image processing, optimization and allied fields. Neural Architecture Search (NAS) is a promising technique to automate the architectural design process of a Neural Network in a data-driven way using Machine Learning (ML) methods. The search method explores several architectures without requiring significant human effort, and the searched models outperform the manually built networks. In this paper, we review Neural Architecture Search techniques, targeting the Transformer model and its family of architectures such as Bidirectional Encoder Representations from Transformers (BERT) and Vision Transformers. We provide an in-depth literature review of approximately 50 state-of-the-art Neural Architecture Search methods and explore future directions in this fast-evolving class of problems.",Transformers; Computer architecture; Convolutional neural networks; Computational modeling; Bit error rate; Search problems; Neural architecture search; NAS; transformers; BERT; vision transformers; multi-head self-attention; hardware-aware NAS
"Current infrastructure maintenance works face limitations for various reasons: insufficient budget, increasing number of infrastructure facilities requiring maintenance, shortage of labor, and rapidly increasing number of aged infrastructure facilities. To overcome these limitations, a new approach that is different from manual inspection methods under existing rules and regulations is required. In this context, we explored the efficiency of bridge inspection and maintenance using unmanned aerial vehicles (UAVs), which can observe inaccessible areas, be conveniently and easily controlled, and may offer high economic benefits. Various tests were performed on elevated bridges, and suitable UAV images were obtained. The obtained images were inspected using machine vision technology, thereby avoiding subjective evaluations by humans. We also discuss methods for enhancing the objectivity of inspections. Another aim of this study was to automate inspection work and improve work efficiency through computer vision technology. The UAV image analysis and classification technology in this study utilized existing computer vision technology, but the optimization process for each inspection item is described in detail so that it can be directly applied to the inspection task. This is to overcome limitations of current inspection tasks, which require the ability and experience of personnel. For this purpose, objectivity can be secured by optimizing the data acquisition and analysis process on a job-by-job basis. The test results showed that both the efficiency and objectivity of the proposed UAV-based method were superior to those of existing bridge maintenance and inspection methods.",bridge; maintenance and inspection; crack detection; water leak; white coating; UAV
"Studies on domain adaptation (DA) for remote sensing (RS) imagery analysis lack consistency in selection and description of evaluation scenarios. Without properly characterizing datasets, model assumptions, and evaluation scenarios, it is difficult to objectively compare DA methods and reach conclusions about their suitability across different applications. With this motivation, this work seeks to empirically assess to which extent the interaction between data characteristics and model assumptions influences the effectiveness of DA methods. Using the widely explored task of building footprint segmentation as a case study, we perform a large-scale study across over 200 DA scenarios that include variations across view angles, areas observed, and sensors used for data acquisition. Rather than adopting different model architectures or optimization criteria, we contrast the performances of two DA methods based on adversarial learning that differ only in their assumptions about source and target domains. Informed by metadata and data characteristics unveiled using traditional computer vision (CV) techniques as well as pretrained deep models, we provide a detailed meta-analysis of experiments highlighting the importance of accurately considering data assumptions for DA in RS segmentation tasks. As demonstrated by a cherry-picking exercise, different claims regarding which model is best could be made by selecting different subsets of evaluation scenarios. While well-calibrated assumptions can be beneficial, mismatching assumptions can lead to negative biases in DA applications. This study intends to motivate the community toward more consistent evaluation protocols while providing recommendations and insights toward creating novel benchmark datasets, documenting data characteristics, application-specific knowledge, and model assumptions.",Remote sensing; Buildings; Image segmentation; Task analysis; Adversarial machine learning; Adaptation models; Feature extraction; Adversarial learning; data characterization; domain adaptation (DA); evaluation protocols; image segmentation; remote sensing (RS)
"Center of pressure (CoP) metrics, including its path length, sway area, and position, are important measurements of postural and balance control in biomechanical studies. A computer-vision-based CoP metrics estimation system offers a portable solution to obtain these gold-standard metrics with 3D multi-joint coordination underlying body movements for real-time evaluation of balance control. In this paper, we propose an end-to-end framework for video-level estimation of CoP path length and sway area, as well as the frame-level estimation of CoP position, utilizing the spatial-temporal features and adaptive graph structure learned by graph convolution network. This work is the first step toward demonstrating that these gold-standard metrics can be obtained with a more comprehensive tool than current force plate technologies. We propose two single-task models for video-level and frame-level estimation, respectively, and a multi-task learning approach that jointly learns the two-temporal-level features. To facilitate this line of research, we release a novel computer-vision-based 3D body landmark dataset containing a wide variety of action patterns with synchronized CoP labels using pose estimation. We also adapt our framework on an existing kinematic dataset collected by wearable markers. The experiments on both datasets validate that our framework achieves state-of-the-art accuracies for all metric estimations, while the proposed multi-task approach yields the most accurate and robust performance on video-level estimation.(1)",Measurement; Estimation; Three-dimensional displays; Task analysis; Adaptation models; Computational modeling; Predictive models; Multi-task learning; center of pressure; computer vision; 3D body landmarks; balance control
"While humans can effortlessly transform complex visual scenes into simple words and the other way around by leveraging their high-level understanding of the content, conventional or the more recent learned image compression codecs do not seem to utilize the semantic meanings of visual content to their full potential. Moreover, they focus mostly on rate-distortion and tend to underperform in perception quality especially in low bitrate regime, and often disregard the performance of downstream computer vision algorithms, which is a fast-growing consumer group of compressed images in addition to human viewers. In this paper, we (1) present a generic framework that can enable any image codec to leverage high-level semantics and (2) study the joint optimization of perception quality and distortion. Our idea is that given any codec, we utilize high-level semantics to augment the low-level visual features extracted by it and produce essentially a new, semantic-aware codec. We propose a three-phase training scheme that teaches semantic-aware codecs to leverage the power of semantic to jointly optimize rate-perception-distortion (R-PD) performance. As an additional benefit, semantic-aware codecs also boost the performance of downstream computer vision algorithms. To validate our claim, we perform extensive empirical evaluations and provide both quantitative and qualitative results.",Semantics; Codecs; Distortion; Image coding; Visualization; Training; Bit rate; Image compression; high-level semantics; generative adversarial networks
"Reading power equipment meters often requires loads of manpower, which is a trivial, repetitive, and error-prone task. While conventional automated recognition methods using computer vision (CV) techniques are inflexible under diverse scenarios, in this article, we propose a lightweight meter recognition method that combines deep learning and traditional CV techniques for automated meter reading. For meter detection, an adaptive anchor and global context (GC) module are deployed to improve the feature extraction ability of lightweight backbone without increasing computational cost. Then, an feature pyramid network (FPN) and a path aggregation network (PANet) are developed to realize the information interaction between different feature layers and achieve multiscale prediction. Our method also includes a multitask segmented network to read the detected meters, accelerating the detection speed. Experiments demonstrate that our proposed method can achieve a detection speed of 123 frame per second (FPS) in GeForce GTX 1080 and can obtain an accuracy of 88.2% mean average precision (mAP)50:95. In the case of insufficient training samples, the method can still achieve an accuracy of 80.9% mAP50:95. In addition, we build a power meter images (PMIs) dataset, which contains 1800 images in real scene. The dataset and method we proposed can help with further upgrades of traditional substations. In the future, we also hope to extend the algorithm to edge computing cameras for substations. The newly developed dataset and code are available at https://github.com/zzfan3/electric_meter_detect_recognize.",Meters; Feature extraction; Substations; Object detection; Deep learning; Convolutional neural networks; Real-time systems; Meter recognition; power equipment meter; real-time detection; Yolov5
"Currently, the ability to automatically detect human behavior in image sequences is one of the most important challenges in the area of computer vision. Within this broad field of knowledge, the recognition of activities of people groups in public areas is receiving special attention due to its importance in many aspects including safety and security. This paper proposes a generic computer vision architecture with the ability to learn and recognize different group activities using mainly the local group's movements. Specifically, a multi-stream deep learning architecture is proposed whose two main streams correspond to a representation based on a descriptor capable of representing the trajectory information of a sequence of images as a collection of local movements that occur in specific regions of the scene. Additional information (e.g. location, time, etc.) to strengthen the classification of activities by including it as additional streams. The proposed architecture is capable of classifying in a robust way different activities of a group as well to deal with the one-class problems. Moreover, the use of a simple descriptor that transforms a sequence of color images into a sequence of two-image streams can reduce the curse of dimensionality using a deep learning approach. The generic deep learning architecture has been evaluated with different datasets outperforming the state-of-the-art approaches providing an efficient architecture for single and multi-class classification problems.",Deep learning; Computer architecture; Training; Task analysis; Proposals; Behavioral sciences; Streaming media; Neural network architecture; one-class classification; multi-class classification
"Person re-identification (ReID), as a sub-direction of computer vision, has attracted more and more attention. In recent years, we have witnessed significant progress of person ReID driven by deep neural network architectures. In this paper, we introduce the progress of person ReID based on deep learning in recent years, including representation learning methods, metric learning methods, part-based methods, GAN-based methods, and video-based methods. The class of methods are summarised and analysed, and then we introduce the image-based datasets and the video-based datasets. We further discuss some of the current challenges and introduce some potential solutions in person ReID. Finally, we present the possible future directions of person ReID, such as collecting more abundant pedestrian datasets, adopting semi-supervised or unsupervised methods in person ReID. The purpose of this paper is to provide insights for the research on the person ReID and to present different methods of person ReID based on deep learning.",person re-identification; computer vision; deep learning; review
"The increasing popularity of attention mechanisms in deep learning algorithms for computer vision and natural language processing made these models attractive to other research domains. In healthcare, there is a strong need for tools that may improve the routines of the clinicians and the patients. Naturally, the use of attention-based algorithms for medical applications occurred smoothly. However, being healthcare a domain that depends on high-stake decisions, the scientific community must ponder if these high-performing algorithms fit the needs of medical applications. With this motto, this paper extensively reviews the use of attention mechanisms in machine learning methods (including Transformers) for several medical applications based on the types of tasks that may integrate several works pipelines of the medical domain. This work distinguishes itself from its predecessors by proposing a critical analysis of the claims and potentialities of attention mechanisms presented in the literature through an experimental case study on medical image classification with three different use cases. These experiments focus on the integrating process of attention mechanisms into established deep learning architectures, the analysis of their predictive power, and a visual assessment of their saliency maps generated by post-hoc explanation methods. This paper concludes with a critical analysis of the claims and potentialities presented in the literature about attention mechanisms and proposes future research lines in medical applications that may benefit from these frameworks.",Biomedical imaging; Computer architecture; Transformers; Medical services; Deep learning; Artificial intelligence; Biomedical equipment; Computer vision; Artificial intelligence; attention mechanisms; computer vision; deep learning; medical applications; medical image analysis; transformers
"In recent years, there has been an unprecedented growth in computer vision and deep learning implementation owing to the exponential rise of computation infrastructure. The same was also reflected in retinal image analysis and successful artificial intelligence models were developed for various retinal disease diagnoses using a wide variety of visual markers obtained from eye fundus images. This article presents a comprehensive study of different deep learning strategies employed in recent times for the diagnosis of five major eye diseases, i.e., Diabetic retinopathy, Glaucoma, age-related macular degeneration, Cataract, and Retinopathy of prematurity. This article is organized according to the deep learning implementation process pipeline, where commonly used datasets, evaluation metrics, image pre-processing techniques, and deep learning backbone models are first illustrated followed by an extensive review of different strategies for each of the five mentioned retinal diseases is presented. Finally, this article summarizes eight major research directions available in the field of retinal disease diagnosis and outlines key challenges and future scope for the present research community.",Retina; Medical diagnosis; Task analysis; Retinopathy; Deep learning; Measurement; Diabetes; Computer vision; deep learning; fundus image; retinal disease diagnosis; artificial intelligence; diabetic retinopathy; glaucoma; AMD; cataract; ROP
"In this study, deep learning techniques and algorithms used in point cloud processing have been analysed. Methods, technical properties and algorithms developed for 3D Object Classification and Segmentation, 3D object detection and tracking and 3D scene flow of point cloud data have been also analysed. 3D point cloud sensing techniques have been grouped as Multi-view, Volumetric approach and raw point cloud processing and mathematical models of them have been analysed. In 3D Object Classification and Segmentation, algorithms are given by analysing it in different categories as Convolutional Neural Network (CNN) based, Graph based, Hierarchical Data Structure-Based Methods and Others. 3D object detection and tracking, Segmentation-based, Frustum-based, Discretization-based analysed as point based and other methods. In each section, deep learning algorithms are compared with respect to applicability to real-time processing, amount of points, and relevance to large-scale or small-scale areas. In the last section, comparisons of point cloud processing methods are made and their advantages and disadvantages are given in the form of a table.",Deep learning; point cloud processing; 3D image processing; 3D computer vision; 3D object classification and segmentation
"Human face is one of the most widely used biometrics based on computer-vision to derive various useful information such as gender, ethnicity, age, and even identity. Facial age estimation has received great attention during the last decades because of its influence in many applications, like face recognition and verification, which may be affected by aging changes and signs which appear on human face along with age progression. Thus, it becomes a prominent challenge for many researchers. One of the most influential factors on age estimation is the type of features used in the model training process. Computer-vision is characterized by its superior ability to extract traditional facial features such as shape, size, texture, and deep features. However, it is still difficult for computers to extract and deal with semantic features inferred by human-vision. Therefore, we need somehow to bridge the semantic gap between machines and humans to enable utilization of the human brain capabilities of perceiving and processing visual information in semantic space. Our research aims to exploit human-vision in semantic facial feature extraction and fusion with traditional computer-vision features to obtain integrated and more informative features as an initial study paving the way to further augment the outperforming state-of-the-art age estimation models. A hierarchical automatic age estimation is achieved upon two consecutive stages: classification to predict (high-level) age group, followed by regression to estimate (low-level) exact age. The results showed noticeable performance improvements, when fusing semantic-based features with traditional vision-based features, surpassing the performance of traditional features alone.",Semantic features; semantic face analysis; feature-level fusion; computer-vision features; hierarchical age estimation; age group classification; facial aging
"Human motion analysis has been a common thread across modern and early medicine. While medicine evolves, analysis of movement disorders is mostly based on clinical presentation and trained observers making subjective assessments using clinical rating scales. Currently, the field of computer vision has seen exponential growth and successful medical applications. While this has been the case, neurology, for the most part, has not embraced digital movement analysis. There are many reasons for this including: the limited size of labeled datasets, accuracy and nontransparent nature of neural networks, and potential legal and ethical concerns. We hypothesize that a number of opportunities are made available by advancements in computer vision that will enable digitization of human form, movements, and will represent them synthetically in 3D. Representing human movements within synthetic body models will potentially pave the way towards objective standardized digital movement disorder diagnosis and building sharable open-source datasets from such processed videos. We provide a hypothesis of this emerging field and describe how clinicians and computer scientists can navigate this new space. Such digital movement capturing methods will be important for both machine learning-based diagnosis and computer vision-aided clinical assessment. It would also supplement face-to-face clinical visits and be used for longitudinal monitoring and remote diagnosis.",Artificial intelligence; computer-assisted diagnosis; computer-assisted image processing; neural networks (computer); movement disorders; Parkinson's disease
"In recent years, deep learning-based finger vein (FV) authentication has attracted the attention of biometric researchers and achieved breakthrough results. Previously, convolutional neural networks (CNNs) were the most commonly used deep learning-based methods for FV authentication. Recently, the vision Transformer (ViT)-based method has started getting attention from the research community due to its excellent performance in many computer vision tasks. In this article, we delve into ViTs and propose a novel model, FV Transformer (FVT), for FV authentication. The FVT consists of four key modules: 1) the conditional position embedding, which is capable of dynamically generating position codes according to the input FV tokens; 2) the weight-shared expanded multilayer perceptron (EMLP), which helps to extract richer and more robust token information; 3) the local information-enhanced feedforward network (FFN), which enhances the ability of local information extraction; and 4) the expansion-less mechanism (ELM) for aggregating adjacent FV tokens, which implements the pyramid structure, and hence, the multilevel feature extraction capability is introduced to the Transformer architecture, which originally focuses on global information. To fully validate the performance and generalization of FVT, experiments were conducted on nine publicly available FV datasets. The effectiveness of each key module of FVT is demonstrated in the ablation experiments. Also, the comparative experiments show that the FVT outperforms several baseline Transformer models and achieves competitive performance when compared with the state-of-the-art (SOTA) FV authentication methods.",Authentication; Transformers; Feature extraction; Task analysis; Veins; Computational modeling; Biological system modeling; Authentication; biometrics; computer vision; deep learning; finger vein (FV); Transformer
"Deep learning has enabled the rapid expansion of computer vision tasks from image frames to video segments. This paper focuses on the review of the latest research in the field of computer vision tasks in general and on object localization and identification of their associated pixels in video frames in particular. After performing a systematic analysis of the existing methods, the challenges related to computer vision tasks are presented. In order to address the existing challenges, a hybrid framework is proposed, where deep learning methods are coupled with domain knowledge. An additional feature of this survey is that a review of the currently existing approaches integrating domain knowledge with deep learning techniques is presented. Finally, some conclusions on the implementation of hybrid architectures to perform computer vision tasks are discussed.",Task analysis; Computer vision; Image segmentation; Proposals; Deep learning; Convolutional neural networks; Object detection; Computer vision; object detection; deep learning; theory-guided data science
"There is tremendous scope for improving the energy efficiency of embedded vision systems by incorporating programmable region-of-interest (ROI) readout in the image sensor design. In this work, we study how ROI programmability can be leveraged for vision applications by anticipating where the ROI will be located in future frames and switching pixels off outside of this region. We refer to this process of ROI prediction and corresponding sensor configuration as adaptive subsampling. Our adaptive subsampling algorithms comprise an object detector and an ROI predictor (Kalman filter) which operate in conjunction to optimize the energy efficiency of the vision pipeline with the end task being object tracking. To further facilitate the implementation of our adaptive algorithms in real systems, we select a candidate algorithm and map it onto an FPGA. Leveraging Xilinx Vitis AI tools, we designed and accelerated a YOLO object detector-based adaptive subsampling algorithm. In order to further improve the algorithm post-deployment, we evaluated several competing baselines on the OTB100 and LaSOT datasets. We found that coupling the ECO tracker with the Kalman filter has a competitive AUC score of 0.4568 and 0.3471 on the OTB100 and LaSOT datasets respectively. Further, the power efficiency of this algorithm is on par with, and in a couple of instances superior to, the other baselines. The ECO-based algorithm incurs a power consumption of approximately 4 W averaged across both datasets while the YOLO-based approach requires power consumption of approximately 6 W (as per our power consumption model). In terms of accuracy-latency tradeoff, the ECO-based algorithm provides near-real-time performance (19.23 FPS) while managing to attain competitive tracking precision.",Field programmable gate arrays; Kalman filters; Energy efficiency; Embedded systems; Power demand; Image sensors; Visualization; Computer vision; Hardware design languages; Object tracking; Adaptive subsampling; embedded computer vision; FPGA acceleration; hardware; software co-design; single object tracking; vision applications
"Significant advancements and progress made in recent computer vision research enable more effective processing of various objects in high-resolution overhead imagery obtained by various sources from drones, airplanes, and satellites. In particular, overhead images combined with computer vision allow many real-world uses for economic, commercial, and humanitarian purposes, including assessing economic impact from access crop yields, financial supply chain prediction for company's revenue management, and rapid disaster surveillance system (wildfire alarms, rising sea levels, weather forecast). Likewise, object detection in overhead images provides insight for use in many real-world applications yet is still challenging because of substantial image volumes, inconsistent image resolution, small-sized objects, highly complex backgrounds, and nonuniform object classes. Although extensive studies in deep learning-based object detection have achieved remarkable performance and success, they are still ineffective yielding a low detection performance, due to the underlying difficulties in overhead images. Thus, high-performing object detection in overhead images is an active research field to overcome such difficulties. This survey paper provides a comprehensive overview and comparative reviews on the most up-to-date deep learning-based object detection in overhead images. Especially, our work can shed light on capturing the most recent advancements of object detection methods in overhead images and the introduction of overhead datasets that have not been comprehensively surveyed before.",Object detection; Head; Detectors; Transformers; Satellites; Feature extraction; Remote sensing; Object detection; satellites; synthetic aperture radar; unmanned aerial vehicles
"This article shows one of the applications of computer vision methods in the problem of assessing the quality of the production of ceramic bricks. The tasks of quality assessment include the selection of chips, cracks, scratches, as well as colour changes. The article provides a method of colour difference for assessing the quality of industrial products. Real images of the production process are taken as a basis, for which methods for highlighting bricks have been developed, and for each an assessment of the colour difference from the sample is carried out. The proposed solution to the problem allows you to automate and speed up the process of assessing the quality of brick products in contrast to typical schemes for solving the same problem using visual observation.",image; computer vision; pattern comparison; histogram; production output
"The development of autonomous, fast, agile small Unmanned Aerial Vehicles (UAVs) brings up fundamental challenges in dynamic environments with fast and agile maneuvers, unreliable state estimation, imperfect sensing, coupling action, and perception in real-time under severe resource constraints. However, autonomous drone racing is a challenging research problem at the intersection of computer vision, planning, state estimation, and control. To bridge this, we propose an approach in the context of autonomous, perception-action aware vision-based drone racing in a photorealistic environment. Our approach integrates a deep convolutional neural network (CNN) with state-of-the-art path planning, state estimation, and control algorithms. The developed deep learning method is based on computer vision approaches to detecting the gates and estimating the flyable area. The planner and controller then use this information to generate a short, minimum-snap trajectory segment and send corresponding motor commands to reach the desired goal. A thorough evaluation of our proposed methodology has been carried out using the Gazebo and FlightGoggles (photorealistic sensor) environments. Extensive experiments demonstrate that the proposed approach outperforms state-of-the-art methods and flies the drone more consistently than many human pilots. Moreover, we demonstrated that our proposed system successfully guided the drone through tight race courses, reaching speeds up to 7m/s of the 2019 AlphaPilot Challenge.",Drones; Logic gates; Task analysis; Cameras; Artificial intelligence; Propellers; Computer vision; Control; computer vision; drone race; deep learning; path planning; photo-realistic environment; state estimation
"Astounding results from Transformer models on natural language tasks have intrigued the vision community to study their application to computer vision problems. Among their salient benefits, Transformers enable modeling long dependencies between input sequence elements and support parallel processing of sequence as compared to recurrent networks, e.g., Long short-term memory. Different from convolutional networks, Transformers require minimal inductive biases for their design and are naturally suited as set-functions. Furthermore, the straightforward design of Transformers allows processing multiple modalities (e.g., images, videos, text, and speech) using similar processing blocks and demonstrates excellent scalability to very large capacity networks and huge datasets. These strengths have led to exciting progress on a number of vision tasks using Transformer networks. This survey aims to provide a comprehensive overview of the Transformer models in the computer vision discipline. We start with an introduction to fundamental concepts behind the success of Transformers, i.e., self-attention, large-scale pre-training, and bidirectional feature encoding. We then cover extensive applications of transformers in vision including popular recognition tasks (e.g., image classification, object detection, action recognition, and segmentation), generative modeling, multi-modal tasks (e.g., visual-question answering, visual reasoning, and visual grounding), video processing (e.g., activity recognition, video forecasting), low-level vision (e.g., image super-resolution, image enhancement, and colorization), and three-dimensional analysis (e.g., point cloud classification and segmentation). We compare the respective advantages and limitations of popular techniques both in terms of architectural design and their experimental value. Finally, we provide an analysis on open research directions and possible future works. We hope this effort will ignite further interest in the community to solve current challenges toward the application of transformer models in computer vision.",Self-attention; transformers; bidirectional encoders; deep neural networks; convolutional networks; self-supervision; literature survey
"A holistic understanding of dynamic scenes is of fundamental importance in real-world computer vision problems such as autonomous driving, augmented reality and spatio-temporal reasoning. In this paper, we propose a new computer vision benchmark: Video Panoptic Segmentation (VPS). To study this important problem, we present two datasets, Cityscapes-VPS and VIPER together with a new evaluation metric, video panoptic quality (VPQ). We also propose VPSNet++, an advanced video panoptic segmentation network, which simultaneously performs classification, detection, segmentation, and tracking of all identities in videos. Specifically, VPSNet++ builds upon a top-down panoptic segmentation network by adding pixel-level feature fusion head and object-level association head. The former temporally augments the pixel features while the latter performs object tracking. Furthermore, we propose panoptic boundary learning as an auxiliary task, and instance discrimination learning which learns spatio-temporally clustered pixel embedding for individual thing or stuff regions, i.e., exactly the objective of the video panoptic segmentation problem. Our VPSNet++ significantly outperforms the default VPSNet, i.e., FuseTrack baseline, and achieves state-of-the-art results on both Cityscapes-VPS and VIPER datasets. The datasets, metric, and models are publicly available at https://github.com/mcahny/vps.",Task analysis; Image segmentation; Measurement; Electron tubes; Semantics; Head; Benchmark testing; Video panoptic segmentation; panoptic segmentation; video instance segmentation; video semantic segmentation; scene parsing
"Glaucoma is a type of visual impairment that is caused due to damage in the optic nerve. The vision loss increases from the peripheral vision towards the central vision, leading to blindness if untreated. The proposed approach is a Computer-Aided Detection (CADe) system using deep learning to screen visual field loss in glaucoma patients while performing different day-to-day activities such as searching objects, viewing photographs, etc. Incorporating an eye-tracking device helps to identify eye movements of glaucoma patients while performing different activities. Different day-to-day activities are depicted in the form of visual exploration tasks. CADe system fuses performance parameters and eye gaze parameters during visual exploration tasks onto images, to guide health care professionals of primary eye care centers in glaucoma screening. The pertinent eye gaze and performance parameters are visualized in the form of three fusion maps: Gaze Fusion Map (GFM), Gaze Fusion Reaction Time (GFRT) map, Gaze Convex Hull Map (GCHM), which are the outcomes of different visual exploration tasks. In addition, the explainability techniques applied in CADe generated Gaze Exploration - index (GE-i) that discriminates glaucoma and normal.",Visualization; Task analysis; Particle measurements; Behavioral sciences; Atmospheric measurements; Computational modeling; Indexes; Glaucoma; eye gaze; visual attention; explainable computer aided detection; visual field loss
"Although railway transportation has rapidly evolved in recent times, the existing manual maintenance of rail infrastructure presents several challenges, such as labor intensity and tediousness. Computer vision, as one of the automatic inspection methods, shows promising prospects for railway detection tasks. In this study, we proposed a novel rail surface defect detection method based on a self-reference template and similarity evaluation. Firstly, a self-reference template was generated by extracting the domain information together with the longitudinal direction of the rail image. We further defined an image structural similarity index, and subsequently compared all the transversal rows with the background template. A coarse-to-fine segmentation method was further proposed to locate the defect. In the first step of the segmentation procedure, rows with large differences were selected using the Otsu algorithm adaptively. The exact position of the defect was then determined by utilizing both gradient magnitude and grayscale information. Our method was evaluated on a public rail surface defects dataset, which included two types of data. The experiment results showed that our method detected type-I and type-II defects with 89.04% and 89.61% recall, and 98.46% and 97.87% precision, respectively. This shows that our method achieved higher accuracy than the established detection algorithms.",background template generation; computer vision; defect detection; rail inspection; structural similarity index
"This study focuses on the over-fitting problem in the training process of the deep convolutional neural network model and the problem of poor robustness when the model is applied in an occlusion environment. We propose a unique data augmentation method, In-and-Out. First, the information variance is enhanced through dynamic local operation while maintaining the overall geometric structure of the training image; compared with the global data augmentation method, our method effectively alleviates the overfitting problem of model training and significantly improves the generalization ability of the model. Then through the dynamic information removal operation, the image is hidden according to the dynamic patch generated by multiple parameters. Compared with other information removal methods, our method can better simulate the real-world occlusion environment, thus improving the robustness of the model in various occlusion scenes. This method is simple and easy to implement and can be integrated with most CNN-based computer vision tasks. Our extensive experiments show that our method surpasses previous methods on the Canadian Institute for Advanced Research dataset for image classification, the PASCAL Visual Object Classes dataset for object detection, and the Cityscapes dataset for semantic segmentation. In addition, our robustness experiments show that our method has good robustness to occlusion in various scenes. (C) 2022 SPIE and IS&T",data augmentation; information variance; dynamic local operation; information removal; robustness
"Pedestrian detection is an important branch of computer vision, and has important applications in the fields of autonomous driving, artificial intelligence and video surveillance. With the rapid development of deep learning and the proposal of large-scale datasets, pedestrian detection has reached a new stage and has achieved better performance. However, the performance of state-of-the-art methods is far behind expectations, especially when occlusion and scale variance exist. Therefore, many works focused on occlusion and scale variance have been proposed in the past few years. The purpose of this article is to make a detailed review of recent progress in pedestrian detection. First, a brief progress of pedestrian detection in the past two decades is summarized. Second, recent deep learning methods focusing on occlusion and scale variance are analyzed. Moreover, the popular datasets and evaluation methods for pedestrian detection are introduced. Finally, the development trends in pedestrian detection are discussed.",Feature extraction; Deep learning; Proposals; Object detection; Detectors; Real-time systems; Lighting; Deep learning; pedestrian detection; occlusion handling; scale variance
"Acne is a skin issue that plagues many young people and adults. Even if it is cured, it leaves acne spots or acne scars, which drives many individuals to use skincare products or undertake medical treatment. On the contrary, the use of inappropriate skincare products can exacerbate the condition of the skin. In view of this, this work proposes the use of computer vision (CV) technology to realize a new business model of facial skincare products. The overall framework is composed of a finger vein identification system, skincare products' recommendation system, and electronic payment system. A finger vein identification system is used as identity verification and personalized service. A skincare products' recommendation system provides consumers with professional skin analysis through skin type classification and acne detection to recommend skincare products that finally improve skin issues of consumers. An electronic payment system provides a variety of checkout methods, and the system will check out by finger-vein connections according to membership information. Experimental results showed that the equal error rate (EER) comparison of the FV-USM public database on the finger-vein system was the lowest and the response time was the shortest. Additionally, the comparison of the skin type classification accuracy was the highest.",electronics payment system; finger-vein; skincare; skin type classification; acne detection
"Wheat head detection can measure wheat traits such as head density and head characteristics. Standard wheat breeding largely relies on manual observation to detect wheat heads, yielding a tedious and inefficient procedure. The emergence of affordable camera platforms provides opportunities for deploying computer vision (CV) algorithms in wheat head detection, enabling automated measurements of wheat traits. Accurate wheat head detection, however, is challenging due to the variability of observation circumstances and the uncertainty of wheat head appearances. In this work, we propose a simple but effective idea-dynamic color transform (DCT)-for accurate wheat head detection. This idea is based on an observation that modifying the color channel of an input image can significantly alleviate false negatives and therefore improve detection results. DCT follows a linear color transform and can be easily implemented as a dynamic network. A key property of DCT is that the transform parameters are data-dependent such that illumination variations can be corrected adaptively. The DCT network can be incorporated into any existing object detectors. Experimental results on the Global Wheat Detection Dataset (GWHD) 2021 show that DCT can achieve notable improvements with negligible overhead parameters. In addition, DCT plays an important role in our solution participating in the Global Wheat Challenge (GWC) 2021, where our solution ranks the first on the initial public leaderboard, with an Average Domain Accuracy (ADA) of 0:821, and obtains the runner-up reward on the final private testing set, with an ADA of 0:695.",
"Accurate localization is an important component of the vehicle's autonomous navigation. The appearance of the moving objects may lead to a feature-matching error with the map features, thereby causing a serious decline in localization accuracy. A neuromorphic vision (NeuroIV) sensor is a kind of dynamic vision sensor with the properties of high temporal resolution, movement capture, and lightweight computation. In view of this, this research proposes to combine the NeuroIV and LiDAR points to acquire the static landmark features and robust navigation localization. However, as a younger and smaller research field compared to RGB computer vision, NeuroIV vision is rarely associated with the intelligent vehicle. For this purpose, we built a novel dataset recorded by NeuroIV sensor, and a state-of-the-art YOLO-small network is designed to detect the moving objects with the dataset. In order to completely deduct the whole dynamic zones, a sensors' novel fusion model is built by the zones' segmentation and matching, so the LiDAR's static environment is obtained completely by the remained points. By evaluating different types of LiDAR points, the feature-matching error can be alleviated further, making the localization more accurate. Together with qualitative and quantitative results, this work provides a moving objects' detection improvement of 14.13% mAP with the new NeuroIV dataset and an obvious localization accuracy improvement with LiDAR points' evaluation.",Laser radar; Location awareness; Sensors; Feature extraction; Cameras; Vehicle dynamics; Laser modes; LiDAR features' evaluation; localization improvement; movement detection; neuromorphic vision (NeuroIV) sensor; sensors' fusion model
"Ripening is a very important process that contributes to cheese quality, as its characteristics are determined by the biochemical changes that occur during this period. Therefore, monitoring ripening time is a fundamental task to market a quality product in a timely manner. However, it is difficult to accurately determine the degree of cheese ripeness. Although some scientific methods have also been proposed in the literature, the conventional methods adopted in dairy industries are typically based on visual and weight control. This study proposes a novel approach aimed at automatically monitoring the cheese ripening based on the analysis of cheese images acquired by a photo camera. Both computer vision and machine learning techniques have been used to deal with this task. The study is based on a dataset of 195 images (specifically collected from an Italian dairy industry), which represent Pecorino cheese forms at four degrees of ripeness. All stages but the one labeled as day 18 , which has 45 images, consist of 50 images. These images have been handled with image processing techniques and then classified according to the degree of ripening, i.e., 18, 22, 24, and 30 days. A 5-fold cross-validation strategy was used to empirically evaluate the performance of the models. During this phase, each training fold was augmented online. This strategy allowed to use 624 images for training, leaving 39 original images per fold for testing. Experimental results have demonstrated the validity of the approach, showing good performance for most of the trained models.",Cheese ripening; image analysis; image processing; machine learning; image classification; deep learning
"The good condition of railway rails is crucial to ensuring the safe operation of the railway network. At present, the rail flaw detectors are widely used in rail flaw detection, they are typically based on the principle of ultrasonic detection. However, the rail detection results analysis process involves huge manual work and the associated labor costs, with low levels of efficiency. In order to improve the efficiency, accuracy of results analysis and also reduce the labor costs, it is necessary to employ classification of ultrasonic flaw detection B-scan image, based on an artificial intelligence algorithm. Inspired by transformer models, with excellent performance in the field of natural language processing (NLP), some deep learning models differ from traditional convolutional neural networks (CNN), gradually emerge in the field of computer image processing. In order to explore the practicality of this model in the field of computer image processing (vision), in the paper, the Vision Transformer (ViT) is employed to train with rail defect B-scan images data and produce a rail defect classification. The model accuracy is more than 90% with the highest accuracy reaching 98.92%.",railway; rail defect; artificial intelligence; vision transformer
"Feature representation is highly important for many computer vision tasks. A broad range of prior studies have been proposed to strengthen representation ability of architectures via built-in blocks. However, during the forward propagation, the reduction in feature map scales still leads to the lack of representation ability. In this paper, we focus on boosting the representational power of a convolutional network by the multi-branch framework that we term the BranchNet. Each branch is directly supervised by label information to enrich the hierarchy features in BranchNet. Based on this framework, we further propose a collaborative learning loss and a soft target loss to transfer knowledge from deeper layers to shallow layers. BranchNet is an efficient training framework without extra parameters introduced in inference and can be integrated in existing networks, e.g., VGG, ResNet, and DenseNet. We evaluate BranchNet on all of these models and find that our method outperforms the baseline models on the widely-used CIFAR and ImageNet datasets. In particular, on the CIFAR-100 dataset, the classification error of ResNet-164 with BranchNet decreases by 4.51 percent. We also conduct experiments on the representative computer vision tasks of instance segmentation and class activation mapping, further verifying the superiority of BranchNet over the baseline models. Models and code are available at https://github.com/zyyupup/BranchNet/.",Collaborative work; Computer architecture; Training; Task analysis; Visualization; Convolutional codes; Computer vision; BranchNet; collaborative learning; feature enhancement
"In electronic manufacturing, anomaly detection of surface mount devices (SMDs) through computer vision (CV) is an important task to control the production quality of SMDs. The difficulty of the detection is that some anomalous regions on the surfaces of SMDs are very minor and with variable shapes, which leads to poor detection efficiency. To solve this problem, based on the assumption that normal samples can be reconstructed more accurately than anomalous samples, a self-supervised image anomaly detection framework with a multiscale two-branch feature fusion strategy is proposed. Specifically, it adopts autoencoder (AE) as the basic framework, and to enhance the reconstruction error between input anomalous samples and the reconstructed ones, a self-supervised learning task of reconstructing images is introduced to have the model neglect the encoding of the suspected anomalous regions found by a contextual attention mask (CAM) module. Meanwhile, a multiscale feature fusion strategy is developed to fuse texture and structure features in the decoder to reconstruct samples. Moreover, a multilevel anomalous score criterion is proposed to enlarge the scores for the samples with very minor anomalies. At last, an SMD-capacitor anomaly detection dataset (SMDC-DET) is built to evaluate the proposed method. The experiments show that the proposed method achieves an average area under the curve (AUC) accuracy of 98.82%, much better when compared to the start-of-the-art existing anomaly detection methods.",Image reconstruction; Feature extraction; Anomaly detection; Task analysis; Decoding; Surface reconstruction; Semantics; Anomaly detection; multilevel anomalous score; multiscale features fusion; self-attention mechanism; self-supervised learning
"Mesh reconstruction from a 3D point cloud is an important topic in the fields of computer graphic, computer vision, and multimedia analysis. In this paper, we propose a voxel structure-based mesh reconstruction framework. It provides the intrinsic metric to improve the accuracy of local region detection. Based on the detected local regions, an initial reconstructed mesh can be obtained. With the mesh optimization in our framework, the initial reconstructed mesh is optimized into an isotropic one with the important geometric features such as external and internal edges. The experimental results indicate that our framework shows great advantages over peer ones in terms of mesh quality, geometric feature keeping, and processing speed. The source code of the proposed method is publicly available(1).",Three-dimensional displays; Optimization; Surface reconstruction; Image reconstruction; Noise reduction; Measurement; Reconstruction algorithms; Mesh reconstruction; intrinsic metric; voxel structure; isotropic property
"Semantic segmentation, as a fundamental task in computer vision, is capable of providing perception ability in many robot applications, such as automatic navigation. To enhance the segmentation accuracy, self-attention mechanism is adopted as a key technique for capturing the long-range dependency and enlarging the receptive fields. However, it requires high computation complexity and GPU memory. In this letter, we propose an Embedded Attention Network to relieve the undesired computational cost. Specifically, we introduce an Embedded Attention (EA) block to improve the segmentation performance and efficiency. Firstly, EA block generates a group of compact while coarse feature bases with the capability of reducing large amount of computation cost. Then an embedded attention is employed to collect the global contextual information and update the representation of the coarse bases from a global view. Finally, the updated bases are leveraged to estimate the attention similarity. We take the well-estimated feature bases to perform feature aggregation. Our approach achieves a considerable computation cost reduction, which suggests it is more powerful than other counterparts in most robot platforms. We conduct extensive experiments on two benchmark semantic segmentation datasets, i.e., CityScapes and ADE20 K. The results demonstrate that the proposed Embedded Attention network delivers comparable performance with high efficiency.",Semantics; Task analysis; Sun; Image segmentation; Costs; Convolution; Computational modeling; Computer vision for transportation; object detection; segmentation and categorization; deep learning; self-attention
"Computer applications have considerably shifted from single data processing to machine learning in recent years due to the accessibility and availability of massive volumes of data obtained through the internet and various sources. Machine learning is automating human assistance by training an algorithm on relevant data. Supervised, Unsupervised, and Reinforcement Learning are the three fundamental categories of machine learning techniques. In this paper, we have discussed the different learning styles used in the field of Computer vision, Deep Learning, Neural networks, and machine learning. Some of the most recent applications of machine learning in computer vision include object identification, object classification, and extracting usable information from images, graphic documents, and videos. Some machine learning techniques frequently include zero-shot learning, active learning, contrastive learning, self-supervised learning, life-long learning, semi-supervised learning, ensemble learning, sequential learning, and multi-view learning used in computer vision until now. There is a lack of systematic reviews about all learning styles. This paper presents literature analysis of how different machine learning styles evolved in the field of Artificial Intelligence (AI) for computer vision. This research examines and evaluates machine learning applications in computer vision and future forecasting. This paper will be helpful for researchers working with learning styles as it gives a deep insight into future directions.",Machine learning; Computer vision; Object detection; Artificial intelligence; Machine learning algorithms; Image segmentation; Feature extraction; Machine learning techniques; computer vision; supervised learning; multi-task learning; object detection; artificial intelligence; image categorization; zero-shot learning
"In recent years, a gain in popularity and significance of science understanding has been observed due to the high paced progress in computer vision techniques and technologies. The primary focus of computer vision based scene understanding is to label each and every pixel in an image as the category of the object it belongs to. So it is required to combine segmentation and detection in a single framework. Recently many successful computer vision methods has been developed to aid scene understanding for a variety of real world application. Scene understanding systems typically involves detection and segmentation of different natural and manmade things. A lot of research has been performed in recent years, mostly with a focus on things (a well-defined objects that has shape, orientations and size) with a less focus on stuff classes (amorphous regions that are unclear and lack a shape, size or other characteristics Stuff region describes many aspects of scene, like type, situation, environment of scene etc. and hence can be very helpful in scene understanding. Existing methods for scene understanding still have to cover a challenging path to cope up with the challenges of computational time, accuracy and robustness for varying level of scene complexity. A robust scene understanding method has to effectively deal with imbalanced distribution of classes, overlapping objects, fuzzy object boundaries and poorly localized objects. The proposed method presents Panoptic Segmentation on Cityscapes Dataset. Mobilenet-V2 is used as a backbone for feature extraction that is pre-trained on ImageNet. MobileNet-V2 with state-of-art encoder-decoder architecture of DeepLabV3+ with some customization and optimization is employed Atrous convolution along with Spatial Pyramid Pooling are also utilized in the proposed method to make it more accurate and robust. Very promising and encouraging results have been achieved that indicates the potential of the proposed method for robust scene understanding in a fast and reliable way.",Panoptic segmentation; instance segmentation; semantic segmentation; deep learning; computer vision; scene understanding; autonomous applications; atrous convolution
"Due to the success of deep learning in wide range of computer vision and computer graphics tasks, there is an increasing number of developed methods leveraging deep neural networks to solve human motion prediction. Recent motion prediction methods focus on solving many issues to predict accurate and natural human motion in temporal domain. In this study, we present a comprehensive survey of deep-learning-based human motion prediction methods. First, we define the human motion prediction problem and the scope of this study. We then provide related background knowledge and a comprehensive list of motion prediction methods based on our proposed classification. Next, we provide a complete survey of the characteristics widely used in the literature and explain the evaluation processes. Finally, we presented a quantitative comparison of recent studies and address the remaining unsolved issues while exploring possible research directions for future research.",Deep learning; Predictive models; Three-dimensional displays; Task analysis; Market research; Bones; Training; Future motion; human motion prediction; deep learning
"Object recognition and tracking are two of the most dynamic research sub-areas that belong to the field of Computer Vision. Computer vision is one of the most active research fields that lies at the intersection of deep learning and machine vision. This paper presents an efficient ensemble algorithm for the recognition and tracking of fixed shape moving objects while accommodating the shift and scale invariances that the object may encounter. The first part uses the Maximum Average Correlation Height (MACH) filter for object recognition and determines the bounding box coordinates. In case the correlation based MACH filter fails, the algorithms switches to a much reliable but computationally complex feature based object recognition technique i.e., affine scale invariant feature transform (ASIFT). ASIFT is used to accommodate object shift and scale object variations. ASIFT extracts certain features from the object of interest, providing invariance in up to six affine parameters, namely translation (two parameters), zoom, rotation and two camera axis orientations. However, in this paper, only the shift and scale invariances are used. The second part of the algorithm demonstrates the use of particle filters based Approximate Proximal Gradient (APG) technique to periodically update the coordinates of the object encapsulated in the bounding box. At the end, a comparison of the proposed algorithm with other stateof-the-art tracking algorithms has been presented, which demonstrates the effectiveness of the proposed algorithm with respect to the minimization of tracking errors.",Object racking; MACH filter; ASIFT; particle filter; recognition
"Multi-object tracking (MOT) is an important field in computer vision that provides a critical understanding of video analysis in various applications, such as vehicle tracking in intelligent transportation systems (ITS). Several deep learning-based approaches have been introduced to basic motion and IoU trackers by extracting appearance features to assist in challenging situations such as lossy detection and occlusion. This study proposes a portable appearance extension (PAE) for single-stage object detection to jointly detect and extract appearance embeddings using a shared model. Furthermore, a novel training framework with a single image and without re-identification annotations is presented using an augmentation module, saving a tremendous amount of human labeling effort and increasing the real-world application adoption rate. Using UA-DETRAC dataset, RetinaNet-PAE and SSD-PAE achieve comparable results with current state-of-the-art models, where RetinaNet-PAE prioritizes detection and tracking performance with a 58.0% HOTA score and 4 FPS. In contrast, SSD-PAE prioritizes latency performance with a 47.3% HOTA score and 40 FPS.",Feature extraction; Object detection; Detectors; Training; Computational modeling; Computer architecture; Task analysis; Computer vision; object detection and tracking; appearance embedding; vehicle tracking
"The information perceived via visual observations of real-world phenomena is unstructured and complex. Computer vision (CV) is the field of research that attempts to make use of that information. Recent approaches of CV utilize deep learning (DL) methods as they perform quite well if training and testing domains follow the same underlying data distribution. However, it has been shown that minor variations in the images that occur when these methods are used in the real world can lead to unpredictable and catastrophic errors. Transfer learning is the area of machine learning that tries to prevent these errors. Especially, approaches that augment image data using auxiliary knowledge encoded in language embeddings or knowledge graphs (KGs) have achieved promising results in recent years. This survey focuses on visual transfer learning approaches using KGs, as we believe that KGs are well suited to store and represent any kind of auxiliary knowledge. KGs can represent auxiliary knowledge either in an underlying graph-structured schema or in a vector-based knowledge graph embedding. Intending to enable the reader to solve visual transfer learning problems with the help of specific KG-DL configurations we start with a description of relevant modeling structures of a KG of various expressions, such as directed labeled graphs, hypergraphs, and hyper-relational graphs. We explain the notion of feature extractor, while specifically referring to visual and semantic features. We provide a broad overview of knowledge graph embedding methods and describe several joint training objectives suitable to combine them with high dimensional visual embeddings. The main section introduces four different categories on how a KG can be combined with a DL pipeline: 1) Knowledge Graph as a Reviewer; 2) Knowledge Graph as a Trainee; 3) Knowledge Graph as a Trainer; and 4) Knowledge Graph as a Peer. To help researchers find meaningful evaluation benchmarks, we provide an overview of generic KGs and a set of image processing datasets and benchmarks that include various types of auxiliary knowledge. Last, we summarize related surveys and give an outlook about challenges and open issues for future research.",Knowledge graph; visual transfer learning; knowledge-based machine learning
"Manual inspection of infrastructure damages such as building cracks is difficult due to the objectivity and reliability of assessment and high demands of time and costs. This can be automated using unmanned aerial vehicles (UAVs) for aerial imagery of damages. Numerous computer vision-based approaches have been applied to address the limitations of crack detection but they have their limitations that can be overcome by using various hybrid approaches based on artificial intelligence (AI) and machine learning (ML) techniques. The convolutional neural networks (CNNs), an application of the deep learning (DL) method, display remarkable potential for automatically detecting image features such as damages and are less sensitive to image noise. A modified deep hierarchical CNN architecture has been used in this study for crack detection and damage assessment in civil infrastructures. The proposed architecture is based on 16 convolution layers and a cycle generative adversarial network (CycleGAN). For this study, the crack images were collected using UAVs and open-source images of mid to high rise buildings (five stories and above) constructed during 2000 in Sydney, Australia. Conventionally, a CNN network only utilizes the last layer of convolution. However, our proposed network is based on the utility of multiple layers. Another important component of the proposed CNN architecture is the application of guided filtering (GF) and conditional random fields (CRFs) to refine the predicted outputs to get reliable results. Benchmarking data (600 images) of Sydney-based buildings damages was used to test the proposed architecture. The proposed deep hierarchical CNN architecture produced superior performance when evaluated using five methods: GF method, Baseline (BN) method, Deep-Crack BN, Deep-Crack GF, and SegNet. Overall, the GF method outperformed all other methods as indicated by the global accuracy (0.990), class average accuracy (0.939), mean intersection of the union overall classes (IoU) (0.879), precision (0.838), recall (0.879), and F-score (0.8581) values. Overall, the proposed CNN architecture provides the advantages of reduced noise, highly integrated supervision of features, adequate learning, and aggregation of both multi-scale and multilevel features during the training procedure along with the refinement of the overall output predictions.",building damages; convolutional neural networks (CNNs); computer vision; cracks; generative adversarial network (CycleGAN); infrastructure inspection; infrastructure monitoring; Unmanned Aerial Vehicle (UAV)
"The Norway lobster, Nephrops norvegicus, is one of the main commercial crustacean fisheries in Europe. The abundance of Nephrops norvegicus stocks is assessed based on identifying and counting the burrows where they live from underwater videos collected by camera systems mounted on sledges. The Spanish Oceanographic Institute (IEO) and Marine Institute Ireland (MI Ireland) conducts annual underwater television surveys (UWTV) to estimate the total abundance of Nephrops within the specified area, with a coefficient of variation (CV) or relative standard error of less than 20%. Currently, the identification and counting of the Nephrops burrows are carried out manually by the marine experts. This is quite a time-consuming job. As a solution, we propose an automated system based on deep neural networks that automatically detects and counts the Nephrops burrows in video footage with high precision. The proposed system introduces a deep-learning-based automated way to identify and classify the Nephrops burrows. This research work uses the current state-of-the-art Faster RCNN models Inceptionv2 and MobileNetv2 for object detection and classification. We conduct experiments on two data sets, namely, the Smalls Nephrops survey (FU 22) and Cadiz Nephrops survey (FU 30), collected by Marine Institute Ireland and Spanish Oceanographic Institute, respectively. From the results, we observe that the Inception model achieved a higher precision and recall rate than the MobileNet model. The best mean Average Precision (mAP) recorded by the Inception model is 81.61% compared to MobileNet, which achieves the best mAP of 75.12%.",Faster RCNN; computer vision; nephrops norvegicus; nephrops norvegicus stock assessment; underwater videos classification
"Over the years, the evolution of face recognition (FR) algorithms has been steep and accelerated by a myriad of factors. Motivated by the unexpected elements found in real-world scenarios, researchers have investigated and developed a number of methods for occluded face recognition (OFR). However, due to the SarS-Cov2 pandemic, masked face recognition (MFR) research branched from OFR and became a hot and urgent research challenge. Due to time and data constraints, these models followed different and novel approaches to handle lower face occlusions, i.e., face masks. Hence, this study aims to evaluate the different approaches followed for both MFR and OFR, find linked details about the two conceptually similar research directions and understand future directions for both topics. For this analysis, several occluded and face recognition algorithms from the literature are studied. First, they are evaluated in the task that they were trained on, but also on the other. These methods were picked accordingly to the novelty of their approach, proven state-of-the-art results, and publicly available source code. We present quantitative results on 4 occluded and 5 masked FR datasets, and a qualitative analysis of several MFR and OFR models on the Occ-LFW dataset. The analysis presented, sustain the interoperable deployability of MFR methods on OFR datasets, when the occlusions are of a reasonable size. Thus, solutions proposed for MFR can be effectively deployed for general OFR.",Face recognition; Training data; Computer vision; Computational modeling; Deep learning; Deep learning; biometrics; occluded face recognition; masked face recognition; face biometrics; computer vision
"Image dehazing is a fundamental problem in computer vision and has hitherto engendered prodigious amounts of studies. Recently, with the well-recognized success of deep learning techniques, this field has been dominated by deep dehazing models. However, deep learning is not always a panacea, especially for the practicalities of image dehazing, because high computational complexity, expensive maintenance costs, and high carbon emission are three noticeable problems. Computational efficiency is, therefore, a decisive factor in real-world circumstances. To cope with this growing demand, we propose a linear time algorithm tailored to three primitive parts: unsharp masking (pre-processing), dehazing, and color gamut expansion (post-processing). The first enhances the sharpness according to the local variance of image intensities. The second removes haze based on the improved color attenuation prior, and the third addresses a residual effect of color gamut reduction. Extensive experimental results demonstrated that the proposed method performed comparatively with popular benchmarks, notably deep dehazing models. With such a comparative performance, the proposed method is still fast and efficient, favoring real-world computer vision systems.",Image color analysis; Deep learning; Scattering; Atmospheric modeling; Degradation; Cameras; Image dehazing; unsharp masking; color gamut expansion; linear time complexity
"The monocular depth estimation (MDE) is the task of estimating depth from a single frame. This information is an essential knowledge in many computer vision tasks such as scene understanding and visual odometry, which are key components in autonomous and robotic systems. Approaches based on the state of the art vision transformer architectures are extremely deep and complex not suitable for real-time inference operations on edge and autonomous systems equipped with low resources (i.e. robot indoor navigation and surveillance). This paper presents SPEED, a Separable Pyramidal pooling EncodEr-Decoder architecture designed to achieve real-time frequency performances on multiple hardware platforms. The proposed model is a fast-throughput deep architecture for MDE able to obtain depth estimations with high accuracy from low resolution images using minimum hardware resources (i.e. edge devices). Our encoder-decoder model exploits two depthwise separable pyramidal pooling layers, which allow to increase the inference frequency while reducing the overall computational complexity. The proposed method performs better than other fast-throughput architectures in terms of both accuracy and frame rates, achieving real-time performances over cloud CPU, TPU and the NVIDIA Jetson TX1 on two indoor benchmarks: the NYU Depth v2 and the DIML Kinect v2 datasets.",Estimation; Computer architecture; Real-time systems; Task analysis; Convolution; Decoding; Transformers; Computer vision; monocular depth estimation; fast-throughput; edge devices
"The Region of interest (ROI) analysis is widely used in image analytics, video coding, computer graphics, computer vision, medical imaging, nuclear medicine, computer tomography and many other areas in medical applications. This ROI determination process using subjective method (e.g. using human vision) often differ from the objective ones (e.g. using mathematical modelling). However, there is no existing method in the literature that could provide a single decision when both methods' ROI data is available. To address this limitation, a robust algorithm is developed by combining the human eye tracking (subjective) and the graph-based visual saliency modelling (objective) information to determine a more realistic ROI for a scene. To carry out this process, in one hand, several different independent human visual saliency factors such as pupil size, pupil dilation, central tendency, fixation pattern, and gaze plot for a group of twenty-two participants are collected by applying on a set of publicly available eighteen video sequences. On the other hand, the features of Graph based visual saliency (GBVS) highlights conspicuity in the scene. Gleaned from these two pieces of information, the proposed algorithm determines the final ROI based on some heuristics. Experimental results show that for a wide range of video sequences and compared to the existing deep learning based (MxSalNet) and depth pixel (DP) based ROI, the proposed ROI is more consistent to the benchmark ROI, which was previously decided by a group of video coding experts. As the subjective and objective options frequently create an ambiguity to reach a single decision on ROI, the proposed algorithm could determine an ultimate decision, which is eventually validated by experts' opinion.",Visualization; Tracking; Mathematical models; Gaze tracking; Brain modeling; Estimation; Video sequences; Gaze tracking; Visualization; Eye tracking; expert opinion; GBVS; region-of-interest; visual saliency
"Video visual relation detection, which aims to detect the visual relations between objects in the form of relation triplet <subject, predicate, object> (e.g., person-ride-bike, dog-toward-car, etc.), is a significant and fundamental task in computer vision. However, most of the existing works about visual relation instances are focused on static images. Modeling the non-static relationships in videos has drawn little attention due to lacking large-scale video dataset support. In our work, we propose a video dataset named Video Predicate Detection and Reasoning (VidPDR) for dynamic video visual relation detection, which consists of 1,000 videos with dense manually dynamic labeled annotations on 21 object classes and 37 predicates classes. Moreover, we propose a novel spatio-temporal feature extraction framework with 3D Convolutional Neural Networks (ST3DCNN), which includes three modules 1) object trajectory, 2) short-term relation prediction, and 3) greedy relational association. We conducted appropriate experiments on public datasets and our own dataset (VidPDR). Results demonstrate that our proposed method has a great improvement in comparison to the state-of-the-art baselines.",Visualization; Feature extraction; Trajectory; Three-dimensional displays; Convolutional neural networks; Task analysis; Object detection; Computer vision; 3D convolutional neural network; video visual relation detection
"Computer vision and deep learning approaches have an important role in industrial inspection systems. Computer vision technology is essential for fast, defect-free control of products in the production line. The importance of the computer vision concept is recognized when the problems of the classical methods are taken into consideration. Metallic defect detection is a challenging problem as metal surfaces are easily affected by environmental factors such as lighting and light reflection. Since traditional detection algorithms are inefficient in complex problems, we propose a novel method to detect and classify metal surface defects, such as cracks, scratches, inclusion, etc. The type and location of defects were detected by the Faster Regional Convolutional Neural Network (Faster R-CNN), combined with the Shape From Shading (SFS) method, which can extract surface characteristics. The Northeastern University (NEU) surface defect database was used for defective samples. The proposed algorithm has also been tested on an unlabeled dataset (KolektorSDD2/KSDD2) to show labeling performance. The results on both labeled and unlabeled datasets have demonstrated state-of-the-art performance in automatic defect detection, classification, and labeling. The proposed method has satisfactory results for the detection of defects on the metal surface, and the mean average precision is 0.83. The average precision of crazing, pitted surface, patches, scratches, inclusion, and rolled-in scale are 0.98, 0.81, 0,90, 0.79, 0.88, and 0.62, respectively.",Deep learning; metal surface; shape from shading; faster r-cnn
"The early 21st-century technological advancements tilted the scales towards data-driven learning. Thus, modern machine-learning systems rely heavily on data to learn complex models to efficiently provide relevant predictions. Data-driven learning suffers from overfitting, a situation in which the learning process seems to have converged into a model that, unfortunately, lacks generalization power. One way to withstand overfitting is to expand the training dataset with more diverse samples. Typically, this is implemented (particularly in computer vision research, which is of interest in this study) by multiplying the original sample using several transformations. Although this strategy might seem straightforward, it does not affect any preexisting dataset bias because the initial distribution remains more or less similar. Ideally, new samples of unseen data must be found, but the cost of acquiring them individually is high. This study presents a novel pipeline that combines state-of-the-art modules to automatically create new thematic datasets with low bias. The proposed method was able to acquire and allocate more than 880K previously unseen images to produce a data collection, that InceptionV3 classified it with 72% accuracy and achieved 0.0008 performance variance when testing on similar datasets.",Benchmark testing; Training data; Artificial intelligence; Data models; Machine learning; Ontologies; Computer vision; Search methods; Web sites; AI; dataset bias; domain shift; image datasets; machine learning; web search
"Oral Squamous Cell Carcinoma (OSCC) is a type of Head and Neck Squamous Cell Carcinoma (HNSCC) and it should be diagnosed at early stages to accomplish efficient treatment, increase the survival rate, and reduce death rate. Histopathological imaging is a wide-spread standard used for OSCC detection. However, it is a cumbersome process and demands expert's knowledge. So, there is a need exists for automated detection of OSCC using Artificial Intelligence (AI) and Computer Vision (CV) technologies. In this background, the current research article introduces Improved Slime Mould Algorithm with Artificial Intelligence Driven Oral Cancer Classification (ISMA-AIOCC) model on Histopathological images (HIs). The presented ISMA-AIOCC model is aimed at identification and categorization of oral cancer using HIs. At the initial stage, linear smoothing filter is applied to eradicate the noise from images. Besides, MobileNet model is employed to generate a useful set of feature vectors. Then, Bidirectional Gated Recurrent Unit (BGRU) model is exploited for classification process. At the end, ISMA algorithm is utilized to fine tune the parameters involved in BGRU model. Moreover, ISMA algorithm is created by integrating traditional SMA and Chaotic Oppositional Based Learning (COBL). The proposed ISMA-AIOCC model was validated for performance using benchmark dataset and the results pointed out the supremacy of ISMA-AIOCC model over other recent approaches.",Computer aided diagnosis; deep learning; BGRU; biomedical imaging; oral cancer; histopathological images
"In this article, a lateral feature enhancement (LFE) backbone network is proposed to enrich feature representation effectively for page object detection (POD) across various scales. Our LFE backbone network has three feature enhancement modules. First, feature enhancement of large page object is a bottom-up feature pyramid, enhancing features of large page objects, which convey more important information to readers. Second, the LFE includes a top-down feature pyramid propagating representative semantical features to lower layers and a lateral connection for feature enhancement in each layer. Third, lateral skip connection is designed to retain the original feature details. The stacking strategies of bottom-up, top-down, and lateral connections are beneficial to overall object detection. Visualization of feature indicates that the proposed LFE backbone network enhances global semantic information as well as detailed features of small page objects. Comparative experiments on the two state-of-the-art datasets show that it achieves excellent results with 0.950 mean of AP (mAP) on PubLayNet and 0.892 mAP on POD with more strict metric intersection over union (IoU) = 0.8, respectively. Compared with both computer vision (CV)-based unimodal detectors and multimodal detectors, the proposed LFE network performs excellently. Visual effect experiments compare the performances of CV-based detectors. The results show that our detector outperforms others with strict metric, especially in the detection of small page objects.",Deep convolutional neural network (CNN); document image; feature enhancement; page object detection (POD)
"The near future has been envisioned as a collaboration of humans with mobile robots to help in the day-to-day tasks. In this paper, we present a viable approach for a real-time computer vision based object detection and recognition for efficient indoor navigation of a mobile robot. The mobile robotic systems are utilized mainly for home assistance, emergency services and surveillance, in which critical action needs to be taken within a fraction of second or real-time. The object detection and recognition is enhanced with utilization of the proposed algorithm based on the modification of You Look Only Once (YOLO) algorithm, with lesser computational requirements and relatively smaller weight size of the network structure. The proposed computer-vision based algorithm has been compared with the other conventional object detection/recognition algorithms, in terms of mean Average Precision (mAP) score, mean inference time, weight size and false positive percentage. The presented framework also makes use of the result of efficient object detection/recognition, to aid the mobile robot navigate in an indoor environment with the utilization of the results produced by the proposed algorithm. The presented framework can be further utilized for a wide variety of applications involving indoor navigation robots for different services.",Computer-vision; real-time computing; object detection; robot; robot navigation; localization; environment sensing; neural networks; YOLO
"Correspondence-based rotation search and point cloud registration are two fundamental problems in robotics and computer vision. However, the presence of outliers, sometimes even occupying the great majority of the putative correspondences, can make many existing algorithms either fail or have very high computational cost. In this letter, we present RANSIC (RANdom Sampling with Invariant Compatibility), a fast and highly robust method applicable to both problems based on a new paradigm combining random sampling with invariance and compatibility. RANSIC starts with randomly selecting small subsets from the correspondence set, then seeks potential inliers as the graph vertices from random subsets through the compatibility tests based on invariants established in each problem, and eventually returns the eligible inliers when there exists a K-degree vertex (where K is initially set and updated during the algorithm) and the residual errors satisfy a certain termination condition at the same time. In synthetic and real experiments, we show that RANSIC is fast for use, robust against over 95% outliers, and also able to recall approximately 100% inliers, outperforming other state-of-the-art solvers for both the rotation search and point cloud registration problems.",Computer vision for automation; RGB-D perception; rotation search; point cloud registration; robust estimation
"Fabric defect detection is generally performed based on human visual inspection. This method is not effective and it has various difficulties such as eye delusion and labor cost. To deal with these problems, machine learning and computer vision-based intelligent systems have been developed. In this paper, a novel real-time fabric defect detection system is proposed. The proposed industrial vision system has been operated in real-time on a loom. Firstly, two fabric databases are constructed using real fabric images and new defective patch capture (DPC) algorithm. One of the main objectives in this study is to develop a CNN architecture that focuses only on fabric defect detection. One of the most unique aspects of the study is to detect defective pixel regions of fabric images with Fourier analysis on a patch-based and integrate it with deep learning Thanks to the novel developed fast Fourier transform-based DPC algorithm, defective texture areas become visible and defect-free areas are suppressed, even on complex denim fabric textures. Secondly, an appropriate convolution neural networks (CNN) model is developed. Thus the new dataset dataset is refined using negative mining method and CNN model. However, traditional feature extraction and classification approaches are also used to compare classification performances of deep models and traditional models. Experimental results show that our proposed CNN model integrated with negative mining can classify the defected images with high accuracy. Also, the proposed CNN model has been tested in real-time on a loom, and it achieves 96.5% detection accuracy. The proposed model obtains better accuracy and speed performance in terms of detection accuracy with a much smaller model size.",Computer vision; fabric defect detection; CNN; feature extraction
"In the field of computer vision, fine-grained visual categorization has attracted a lot of attention and made great progress due to convolutional neural networks and a large number of publicly available datasets. With next-generation sensing technology, RGB-D cameras can provide high-quality synchronized RGB and depth images for solving many computer vision problems. Although RGB-D cameras have been used in the context of multi-view object category detection and scene understanding, they have not been widely used in fine-grained classification. In this paper, we introduce a multiview RGB-D dataset RGBD-FG for fine-grained categorization. Currently, the dataset contains 93 051 RGB-D images covering 19 super-categories and 50 sub-categories of common vegetables and fruit, and is organized in a hierarchical manner. We provide extensive experimental results to establish state-of-the-art benchmarks for our dataset, illustrating its diversity and scope for improvement through future work. We also propose a novel modality-specific multimodal network called FS-Multimodal network, which can solve two limitations of multimodal networks trained based on fine-tuning techniques: over-fitting and lack of effective depth-specific features. We hope that our study lays the foundations for fine-grained categorization of RGB-D data.",Dogs; Sensors; Automobiles; Birds; Benchmark testing; Task analysis; Image color analysis; Deep convolutional neural network; fine-grained categorization; modality-specific features; multimodal; RGB-D dataset
"Physicochemical and sensory analyses are commonly used to determine the quality characteristics of food samples in Food Industries. These methods are tedious, laborious, produce chemical residues, and involve the destruction of the samples. For the meat industries, this work proposes a non-invasive and non-destructive computer-aided inspection system, based on computer vision and ensemble machine learning techniques. The paper presents all the possibilities for the development of the system, making an exhaustive comparison of different algorithms used to extract features from the images of the samples, and various machine learning approaches, studying up to 6160 different models, and selecting the top 110 for the ensemble proposal. The system determines all the physicochemical, textural, and sensory quality characteristics of pork and beef loins in four meat states (fresh, thawed, cooked, and cured) with good precision, being a real alternative to the usual methods for the Food Industry.",Feature extraction; Predictive models; Prediction algorithms; Food technology; Magnetic resonance imaging; Instruments; Image color analysis; Computer-aided system; feature extraction; loin; magnetic resonance imaging; quality parameters; regressor
"Human pose estimation is one of the issues that have gained many benefits from using state-of-the-art deep learning-based models. Human pose, hand and mesh estimation is a significant problem that has attracted the attention of the computer vision community for the past few decades. A wide variety of solutions have been proposed to tackle the problem. Deep Learning-based approaches have been extensively studied in recent years and used to address several computer vision problems. However, it is sometimes hard to compare these methods due to their intrinsic difference. This paper extensively summarizes the current deep learning-based 2D and 3D human pose, hand and mesh estimation methods with a single or multi-person, single or double-stage methodology-based taxonomy. The authors aim to make every step in the deep learning-based human pose, hand and mesh estimation techniques interpretable by providing readers with a readily understandable explanation. The presented taxonomy has clearly illustrated current research on deep learning-based 2D and 3D human pose, hand and mesh estimation. Moreover, it also provided dataset and evaluation metrics for both 2D and 3DHPE approaches.",3D pose estimation; Generator; Discriminator; Loss function; Deep neural network; Deep learning; Mesh estimation; Evaluation metric; Dataset
"Nature-inspired computing has been a real source of motivation for the development of many meta-heuristic algorithms. The biological optic system can be patterned as a cascade of sub-filters from the photoreceptors over the ganglion cells in the fovea to some simple cells in the visual cortex. This spark has inspired many researchers to examine the biological retina in order to learn more about information processing capabilities. The photoreceptor cones and rods in the human fovea resemble hexagon more than a rectangular structure. However, the hexagonal meshes provide higher packing density, consistent neighborhood connectivity, and better angular correction compared to the rectilinear square mesh. In this paper, a novel 2-D interpolation hexagonal lattice conversion algorithm has been proposed to develop an efficient hexagonal mesh framework for computer vision applications. The proposed algorithm comprises effective pseudo-hexagonal structures which guarantee to keep align with our human visual system. It provides the hexagonal simulated images to visually verify without using any hexagonal capture or display device. The simulation results manifest that the proposed algorithm achieves a higher Peak Signal-to-Noise Ratio of 98.45 and offers a high-resolution image with a lesser mean square error of 0.59.",Computer vision; Nature Inspired Computing; hexagonal image; color image; Human Visual System
"Smart city-aspiring urban areas should have a number of necessary elements in place to achieve the intended objective. Precise controlling and management of traffic conditions, increased safety and surveillance, and enhanced incident avoidance and management should be top priorities in smart city management. At the same time, Vehicle License Plate Number Recognition (VLPNR) has become a hot research topic, owing to several real-time applications like automated toll fee processing, traffic law enforcement, private space access control, and road traffic surveillance. Automated VLPNR is a computer vision-based technique which is employed in the recognition of automobiles based on vehicle number plates. The current research paper presents an effective Deep Learning (DL)-based VLPNR called DLVLPNR model to identify and recognize the alphanumeric characters present in license plate. The proposed model involves two main stages namely, license plate detection and Tesseract-based character recognition. The detection of alphanumeric characters present in license plate takes place with the help of fast RCNN with Inception V2 model. Then, the characters in the detected number plate are extracted using Tesseract Optical Character Recognition (OCR) model. The performance of DL-VLPNR model was tested in this paper using two benchmark databases, and the experimental outcome established the superior performance of the model compared to other methods.",Deep learning; smart city; tesseract; computer vision; vehicle license plate recognition
"The productivity analysis of cable crane transportation in the construction field is of great significance to improve crane equipment management and reduce operation costs. However, the traditional manual recording method of analyzing cable crane productivity is time-consuming and tedious. The existing vision-based method requires significant amounts of time to collect extensive images at construction sites and does not achieve high-precision detection in complex scenes. Thus, an automated vision-based method for productivity analysis of cable crane transportation is proposed using a new synthetic image approach based on an augmented reality (AR) technique. The unmanned aerial vehicle-based three-dimensional (3D) reconstruction of a crane bucket model is superimposed on a realistic scene using AR to synthesize the images for vision-based model training without manual image acquisition at a construction site. The feature pyramid network and attention module are integrated into Faster region-based convolutional neural network (Faster R-CNN) to enhance the capability of feature extraction for the high-precision detection of a crane bucket and its ID number, which provides the logical basis for calculating productivity. The proposed vision-based productivity analysis method is evaluated on large-scale hydraulic engineering. The results demonstrate that the mean average precision (mAP) of detection performance is 98.01% using the model trained by AR-based synthetic images, which confirms the proposed AR-based synthetic image method could provide a new image generation mode for the construction industry. Additionally, the bias of productivity between the proposed method and ground truth is 0.03%, which confirms the effectiveness and accuracy of the proposed method.",Cable crane transportation; Productivity analysis; Computer vision; Augmented reality (AR); Faster region-based convolutional neural network (Faster R-CNN); Object detection
"Surface defect inspection of railways is important to ensure safe transportation. However, challenging conditions, such as uneven illumination and similar foreground and background, hinder defect inspection. With the development of deep learning and the wide application of the computer vision, defect inspection has made great progress. Accordingly, we propose a depth repeated-enhancement RGB (red-green-blue) network (DRERNet) for rail surface defect inspection. DRERNet fully uses depth and RGB information to better inspect defects on rail surfaces using an encoder-decoder architecture. In the encoder, a novel cross modality enhancement fusion module uses details from RGB maps and location information from depth maps to perform cross-modality fusion. In the decoder, the details and location information in a multimodality complementation module are repeatedly used to progressively refine the DRERNet prediction. We performed extensive experiments, and compared the proposed DRERNet with 10 state-of-the-art methods on the industrial NEU RSDDS-AUG RGB-depth dataset. The comparison results demonstrate that DRERNet consistently performs better than other methods in the all evaluation measures.",Surface morphology; Rails; Surface treatment; Inspection; Decoding; Computer vision; Weight measurement; Rail flaws detection; RGB-D image; salient object detection; cross-modality enhancement fusion; multimodality complementation
"The appearance quality index of tobacco leaves is widely used in the tobacco industry. But in national standards for flue-cured tobacco, the specified indicators only have qualitative descriptions, and a few have a range of quantitative values, lacking quantitative calculation methods, which affects the effective use of these indicators in tobacco automatic grading. In this work, we provided a computer vision-based quantitative research approach for color intensity, length, waste, and body of tobacco appearance quality indicators. We also designed quantitative algorithms for these indices to achieve precise quantitative values to address this issue. Especially we proposed the quantization algorithm of color intensity and waste originally. In order to employ the quantification algorithm for each index, the tobacco leaf image was first segmented to determine the tobacco leaf region in the picture. Second, a mesh segmentation technique for the color intensity is developed. A comparison of the color differences between the several sub-images of the tobacco leaf image is divided. The pixel length was swiftly calculated, the boundary points at both ends of the tobacco leaf were located, the minimum outer rectangle of the tobacco leaf was calculated for the length index, and the actual length was obtained by the checkerboard reference data. Internal waste and marginal waste are the categories under which the waste index is divided. To locate holes and abnormal areas for internal waste, the connected region analysis is employed. The waveform of the edge was created and studied to determine the missing part of the edge. The actual area of the tobacco leaf was calculated by the design algorithm, the body index was expressed as weight per unit area, and the weight was determined by a pressure sensor. Finally, each index's experimental verification is designed. The empirical findings demonstrate that semantic segmentation average accuracy is 8.4% higher than threshold segmentation in the extraction of tobacco leaf regions. The average relative error between the calculated tobacco leaf length and the manual measurement is 2.83%. The average accuracy of tobacco leaf position classification was 88.52% under the six classifiers. The correlation coefficient between tobacco leaf body quantification value and tobacco leaf thickness value is 0.9270.",Computer vision; tobacco leaf grading; appearance quality indicators; quantitative analysis; semantic segmentation
"The main objective of yoga pose grading is to assess the input yoga pose and compare it to a standard pose in order to provide a quantitative evaluation as a grade. In this paper, a computer vision-based yoga pose grading approach is proposed using contrastive skeleton feature representations. First, the proposed approach extracts human body skeleton keypoints from the input yoga pose image and then feeds their coordinates into a pose feature encoder, which is trained using contrastive triplet examples; finally, a comparison of similar encoded pose features is made. Furthermore, to tackle the inherent challenge of composing contrastive examples in pose feature encoding, this paper proposes a new strategy to use both a coarse triplet example-comprised of an anchor, a positive example from the same category, and a negative example from a different category, and a fine triplet example-comprised of an anchor, a positive example, and a negative example from the same category with different pose qualities. Extensive experiments are conducted using two benchmark datasets to demonstrate the superior performance of the proposed approach.",yoga pose grading; skeleton extraction; contrastive learning; yoga pose classification; deep learning
"Low-light image enhancement plays a central role in various downstream computer vision tasks. Vision Transformers (ViTs) have recently been adapted for low-level image processing and have achieved a promising performance. However, ViTs process images in a window- or patch-based manner, compromising their computational efficiency and long-range dependency. Additionally, existing ViTs process RGB images instead of RAW data from sensors, which is sub-optimal when it comes to utilizing the rich information from RAW data. We propose a fully end-to-end Conv-Transformer-based model, RawFormer, to directly utilize RAW data for low-light image enhancement. RawFormer has a structure similar to that of U-Net, but it is integrated with a thoughtfully designed Conv-Transformer Fusing (CTF) block. The CTF block combines local attention and transposed self-attention mechanisms in one module and reduces the computational overhead by adopting a transposed self-attention operation. Experiments demonstrate that RawFormer outperforms state-of-the-art models by a significant margin on low-light RAW image enhancement tasks.",Low-light image enhancement; vision transformer; image processing; RAW camera data processing
"3D object recognition is a challenging task for intelligent and robot systems in industrial and home indoor environments. It is critical for such systems to recognize and segment the 3D object instances that they encounter on a frequent basis. The computer vision, graphics, and machine learning fields have all given it a lot of attention. Traditionally, 3D segmentation was done with hand-crafted features and designed approaches that didn't achieve acceptable performance and couldn't be generalized to large-scale data. Deep learning approaches have lately become the preferred method for 3D segmentation challenges by their great success in 2D computer vision. However, the task of instance segmentation is currently less explored. In this paper, we propose a novel approach for efficient 3D instance segmentation using red green blue and depth (RGB-D) data based on deep learning. The 2D region based convolutional neural networks (Mask R-CNN) deep learning model with point based rending module is adapted to integrate with depth information to recognize and segment 3D instances of objects. In order to generate 3D point cloud coordinates (x, y, z), segmented 2D pixels (u, v) of recognized object regions in the RGB image are merged into (u, v) points of the depth image. Moreover, we conducted an experiment and analysis to compare our proposed method from various points of view and distances. The experimentation shows the proposed 3D object recognition and instance segmentation are sufficiently beneficial to support object handling in robotic and intelligent systems.",Instance segmentation; 3D object segmentation; deep learning; point cloud coordinates
"In this paper, a combined framework is proposed that includes Hyperdimensional (HD) computing, neural networks, and k-means clustering to fulfill a computationally simple incremental learning framework in a facial recognition system. The main advantages of HD computing algorithms are the simple computations needed, the high resistance to noise,and the ability to store excessive amounts of information into a single HD vector. The problem of incremental learning revolves around the ability to regularly update the knowledge within the framework to include new subjects in an online manner. Using an HD computing classifier proved efficient and highly accurate to implement an incremental learning framework as no re-training was required after each online update to the framework wbich is HD computing biggest advantage. Another advantage is that HD computing classifiers can achieve a high degree of generalization. The framework was tested on a total of 11 open source benchmark data sets. A number of experimental tests were preformed to ensure consistent performance of the framework under different conditions against different data sets.",Face recognition; Feature extraction; Training data; Principal component analysis; Support vector machines; Biological neural networks; Classification algorithms; Computer vision; Learning systems; Neural networks; Computer vision; facial recognition; HD computing; incremental learning
"Unmanned aerial vehicles (UAVs) have been widely used in postdisaster search and rescue operations, object tracking, and other tasks. Therefore, the autonomous perception of UAVs based on computer vision has become a research hotspot in recent years. However, UAV images include dense objects, small objects, and arbitrary object directions, which bring about significant challenges to existing object detection methods. To alleviate these issues, we propose a global-local feature enhanced network (GLF-Net). Considering the difficulty of processing UAV images with complex scenes and dense objects, we designed a backbone based on an involution and self-attention that can extract effective features from complex objects. A multiscale feature fusion module is also proposed to address the presence of numerous small objects in UAV images through multiscale object detection and feature fusion. To accurately detect rotated objects, a rotated regional proposal network was designed based on the midpoint offset representation, which can apply a rotated box to determine the real direction and contour of an object. GLF-Net achieves a state-of-the-art detection accuracy [86.52% mean average precision (mAP)] on our created rotated object detection UAV (RO-UAV) dataset, while achieving 96.95% and 97% mAP on the public datasets high resolution ship collections 2016 (HRSC2016) and the University of Chinese Academy of Sciences High Resolution Aerial Object Detection Dataset (UCAS-AOD), respectively. The experimental results demonstrate that our method achieves a high detection accuracy and generalization, which can meet the practical requirements of UAVs under various complex scenarios.",Feature extraction; Object detection; Autonomous aerial vehicles; Support vector machines; Proposals; Detectors; Computer vision; Autonomous perception; computer vision; global-local feature enhanced; rotated object detector; unmanned aerial vehicles (UAVs)
"Grain size plays a fundamental role in the mechanical properties of materials. Recently, automatic measurement of average grain size attacks more and more attention based on computer vision. However, low contrast, twin grains, thin boundary, and low connectivity limit the achievement of automatic and accurate image analysis. Inspired by the calculation procedure of grain size, we propose a center-guided and connectivity-preserving network for grain boundaries segmentation. On one hand, the proposed center feature extraction module and center-guided feature recalibration mechanism (CFRM) make the network pay more attention to the center area. On the other hand, a connectivity-preserving loss function is integrated with the network, which forces the network to converge toward high connectivity. Benefiting from the above aspects, our network can segment the grain boundary with high structural integrity and avoid the complex post-processing process. Experiments on the SRIF-GSM dataset reveal that our method achieves 85.98 mIoU and 95.60 clDice scores, demonstrating significant advantages compared with the state-of-the-art semantic segmentation methods.",Center-guided; computer vision; connectivity preserving; grain size measurement; quantitative metallography; analysis system (QMAS)
"Person detection has attracted great attention in the computer vision area and is an imperative element in human-centric computer vision. Although the predictive performances of person detection networks have been improved dramatically, they are vulnerable to adversarial patch attacks. Changing the pixels in a restricted region can easily fool the person detection network in safety-critical applications such as autonomous driving and security systems. Despite the necessity of countering adversarial patch attacks, very few efforts have been dedicated to defending person detection against adversarial patch attack. In this paper, we propose a novel defense strategy that defends against an adversarial patch attack by optimizing a defensive frame for person detection. The defensive frame alleviates the effect of the adversarial patch while maintaining person detection performance with clean person. The proposed defensive frame in the person detection is generated with a competitive learning algorithm which makes an iterative competition between detection threatening module and detection shielding module in person detection. Comprehensive experimental results demonstrate that the proposed method effectively defends person detection against adversarial patch attacks.",Adversarial patch; defensive pattern; universal defensive frame; competitive learning; person detection
"The area affected by the earthquake is vast and often difficult to entirely cover, and the earthquake itself is a sudden event that causes multiple defects simultaneously, that cannot be effectively traced using traditional, manual methods. This article presents an innovative approach to the problem of detecting damage after sudden events by using an interconnected set of deep machine learning models organized in a single pipeline and allowing for easy modification and swapping models seamlessly. Models in the pipeline were trained with a synthetic dataset and were adapted to be further evaluated and used with unmanned aerial vehicles (UAVs) in real-world conditions. Thanks to the methods presented in the article, it is possible to obtain high accuracy in detecting buildings defects, segmenting constructions into their components and estimating their technical condition based on a single drone flight.",Seismic measurements; Safety; Training data; Earthquakes; Computer vision; Machine learning; Autonomous aerial vehicles; Drones; Synthetic data; Structural health monitoring; machine learning; defect detection; synthetic dataset
"Damage detection is a key procedure in maintenance throughout structures' life cycles and post-disaster loss assessment. Due to the complex types of structural damages and the low efficiency and safety of manual detection, detecting damages with high efficiency and accuracy is the most popular research direction in civil engineering. Computer vision (CV) technology and deep learning (DL) algorithms are considered as promising tools to address the aforementioned challenges. The paper aims to systematically summarized the research and applications of DL-based CV technology in the field of damage detection in recent years. The basic concepts of DL-based CV technology are introduced first. The implementation steps of creating a damage detection dataset and some typical datasets are reviewed. CV-based structural damage detection algorithms are divided into three categories, namely, image classification-based (IC-based) algorithms, object detection-based (OD-based) algorithms, and semantic segmentation-based (SS-based) algorithms. Finally, the problems to be solved and future research directions are discussed. The foundation for promoting the deep integration of DL-based CV technology in structural damage detection and structural seismic damage identification has been laid.",deep learning; damage detection; computer vision; loss assessment
"Human pose estimation is still a challenging task in computer vision, especially in the case of camera view transformation, joints occlusions and overlapping, the task will be of ever-increasing difficulty to achieve success. Most existing methods pass the input through a network, which typically consists of high-to-low resolution sub-networks that are connected in series. Still, during the up-sampling process, the spatial relationships and details might be lost. This paper designs a parallel atrous convolutional network with body structure constraints (PAC-BCNet) to address the problem. Among the mentioned techniques, the parallel atrous convolution (PAC) is constructed to deal with scale changes by connecting multiple different atrous convolution sub-networks in parallel. And it is used to extract features from different scales without reducing the resolution. Besides, the body structure constraints (BC), which enhance the correlation between each keypoint, are constructed to obtain better spatial relationships of the body by designing keypoints constraints sets and improving the loss function. In this work, a comparative experiment of the serial atrous convolution, the parallel atrous convolution, the ablation study with and without body structure constraints are conducted, which reasonably proves the effectiveness of the approach. The model is evaluated on two widely used human pose estimation benchmarks (MPII and LSP). The method achieves better performance on both datasets.",Computer vision; human pose estimation; parallel atrous convolution; body structure constraints
"Person re-identification (re-ID) is an important topic in computer vision. We study the one-example re-ID task, where each identity has only one labeled example along with many unlabeled examples. In practice, for the unlabeled data, it is difficult to differentiate each person because of many conditions, such as low resolutions, occlusions, and lighting. In previous works, fine-grained information has been proven to be useful for supervised re-ID. To solve choosing a reliable, easy sample for self-paced learning, we exploit fine-grained features to metric the distances between labeled data and unlabeled data, the combination strategy of global and overlapping-part distance selecting more positive data for model training. In addition, the attention mechanism has been introduced to suppress the interruption of background. The training data are split into three parts, i.e., labeled data, pseudolabeled data, and instance-labeled data. First, the model is initialized by one-shot data for each identity. Then, pseudolabels are estimated from the unlabeled data and updating the model iteratively. The self-paced progressive sampling method is adopted to increase the number of the selected pseudolabeled candidates step by step. Notably, with pretrained model, on Market-1501, the rank-1 accuracy of our method is 86.0% which exceeds most other methods, experiment on two image-based datasets demonstrate promising results under one example re ID setting. (C) 2022 SPIE and IS&T",computer vision; person re-identification; one-example learning; semisupervised learning
"Diabetes or Diabetes Mellitus (DM) is the upset that happens due to high glucose level within the body. With the passage of time, this polygenic disease creates eye deficiency referred to as Diabetic Retinopathy (DR) which can cause a major loss of vision. The symptoms typically originate within the retinal space square in the form of enlarged veins, liquid dribble, exudates, haemorrhages and small scale aneurysms. In current therapeutic science, pictures are the key device for an exact finding of patients' illness. Meanwhile, an assessment of new medicinal symbolisms stays complex. Recently, Computer Vision (CV) with deep neural networks can train models with high accuracy. The thought behind this paper is to propose a computerized learning model to distinguish the key precursors of Dimensionality Reduction (DR). The proposed deep learning framework utilizes the strength of selected models (VGG and Inception V3) by fusing the extracated features. To select the most discriminant features from a pool of features, an entropy concept is employed before the classification step. The deep learning models are fit for measuring the highlights as veins, liquid dribble, exudates, haemorrhages and miniaturized scale aneurysms into various classes. The model will ascertain the loads, which give the seriousness level of the patient's eye. The model will be useful to distinguish the correct class of seriousness of diabetic retinopathy pictures.",Deep neural network; diabetic retinopathy; retina; features extraction; classification
"In the computer vision field, understanding human dynamics is not only a great challenge but also very meaningful work, which plays an indispensable role in public safety. Despite the complexity of human dynamics, physicists have found that pedestrian motion in a crowd is governed by some internal rules, which can be formulated as a motion model, and an effective model is of great importance for understanding and reconstructing human dynamics in various scenes. In this paper, we revisit the related research in social psychology and propose a two-part motion model based on the shortest path principle. One part of the model seeks the origin and destination of a pedestrian, and the other part generates the movement path of the pedestrian. With the proposed motion model, we simulated the movement behavior of pedestrians and classified them into various patterns. We next reconstructed the crowd motions in a real-world scene. In addition, to evaluate the effectiveness of the model in crowd motion simulations, we created a new indicator to quantitatively measure the correlation between two groups of crowd motion trajectories. The experimental results show that our motion model outperformed the state-of-the-art model in the above applications.",crowd motion; computer vision; motion model of pedestrians; shortest path principle; origin and destination
"To date, unmanned aerial vehicles (UAVs), commonly known as drones, have been widely used in precision agriculture (PA) for crop monitoring and crop spraying, allowing farmers to increase the efficiency of the farming process, meanwhile reducing environmental impact. However, to spray pesticides effectively and safely to the trees in small fields or rugged environments, such as mountain areas, is still an open question. To bridge this gap, in this study, an onboard computer vision (CV) component for UAVs is developed. The system is low-cost, flexible, and energy-effective. It consists of two parts, the hardware part is an Intel Neural Compute Stick 2 (NCS2), and the software part is an object detection algorithm named the Ag-YOLO. The NCS2 is 18 grams in weight, 1.5 watts in energy consumption, and costs about $66. The proposed model Ag-YOLO is inspired by You Only Look Once (YOLO), trained and tested with aerial images of areca plantations, and shows high accuracy (F1 score = 0.9205) and high speed [36.5 frames per second (fps)] on the target hardware. Compared to YOLOv3-Tiny, Ag-YOLO is 2x faster while using 12x fewer parameters. Based on this study, crop monitoring and crop spraying can be synchronized into one process, so that smart and precise spraying can be performed.",object detection; precise spraying; embedded AI; YOLO; NCS2
"Embedded networking has a broad prospect. Because of the Internet and the rapid development of PC skills, computer vision technology has a wide range of applications in many fields, especially the importance of identifying wrong movements in sports training. To study the computer vision technology to identify the wrong movement of athletes in sports training, in this paper, a hidden Markov model based on computer vision technology is constructed to collect video and identify the landing and take-off movements and badminton serving movements of a team of athletes under the condition of sports training, Bayesian classification algorithm to analyze the acquired sports training action data, obtain the error frequency, and the number of errors of the landing jump action, and the three characteristic data of the displacement, velocity, and acceleration of the body's center of gravity of the athlete in the two cases of successful and incorrect badminton serve actions and compared and analyzed the accuracy of the action recognition method used in this article, the action recognition method based on deep learning and the action recognition method based on EMG signal under 30 experiments. The training process of deep learning is specifically split into two stages: 1st, a monolayer neuron is built layer by layer so that the network is trained one layer at a time; when all layers are fully trained, a tuning is performed using a wake-sleep operation. The final result shows that the frequency of the wrong actions of the athletes on the landing jump is concentrated in the knee valgus, the total frequency of error has reached 58%, and the frequency of personal error has reached 45%; the problem of the landing distance of the two feet of the team athletes also appeared more frequently, the total frequency reached 50%, and the personal frequency reached 30%. Therefore, athletes should pay more attention to the problems of knee valgus and the distance between feet when performing landing jumps; the difference in the displacement, speed, and acceleration of the body's center of gravity during the badminton serve will affect the error of the action. And the action recognition method used in this study has certain advantages compared with the other two action recognition methods, and the accuracy of action recognition is higher.",
"Current deep learning applications in structural health monitoring (SHM) are mostly related to surface damage such as cracks and rust. Methods using traditional image processing techniques (such as filtering and edge detection) usually face difficulties in diagnosing internal damage in thicker specimens of heterogeneous materials. In this paper, we propose a damage diagnosis framework using a deep convolutional neural network (CNN) and transfer learning, focusing on internal damage such as voids and cracks. We use thermography to study the heat transfer characteristics and infer the presence of damage in the structure. It is challenging to obtain sufficient data samples for training deep neural networks, especially in the field of SHM. Therefore we use finite element (FE) computer simulations to generate a large volume of training data for the deep neural network, considering multiple damage shapes and locations. These computer-simulated data are used along with pre-trained convolutional cores of a sophisticated computer vision-based deep convolutional network to facilitate effective transfer learning. The CNN automatically generates features for damage diagnosis as opposed to manual feature generation in traditional image processing. Systematic parameter selection study is carried out to investigate accuracy versus computational expense in generating the training data. The methodology is demonstrated with an example of damage diagnosis in concrete, a heterogeneous material, using both computer simulations and laboratory experiments. The combination of FE simulation, transfer learning and experimental data is found to achieve high accuracy in damage localization with affordable effort.",convolutional neural network; transfer learning; deep learning; machine learning; feature generation; structural health monitoring; concrete
"Functional performance of exposed aggregate concrete pavement (EACP), such as tyre-pavement noise, is influenced by the surface texture depth and wavelength. To minimise tyre-pavement noise, the texture wavelength, which is represented by the exposed aggregate number (EAN), should be controlled. Normally, the EAN is conducted by manual counting. It requires much human efforts. Therefore, this study suggests an efficient method to count the EAN on a digital image of EACP surface through the deep learning model faster region-based convolutional neural network (Faster R-CNN). The TensorFlow Object Detection API was used to adjust the parameters in the training model. Results of the suggested model were compared with the manual counting in the mock-up and field test dataset. The result showed that the mean absolute error was 5.34 and 8.19 for the mock-up and field tests, respectively. Therefore, the proposed method can be used to preliminarily estimate the EAN under specified condition.",Exposed aggregate concrete pavement; exposed aggregate counting; object detection and counting; computer vision; TensorFlow object detection API
"The rice seed setting rate (RSSR) is an important component in calculating rice yields and a key phenotype for its genetic analysis. Automatic calculations of RSSR through computer vision technology have great significance for rice yield predictions. The basic premise for calculating RSSR is having an accurate and high throughput identification of rice grains. In this study, we propose a method based on image segmentation and deep learning to automatically identify rice grains and calculate RSSR. By collecting information on the rice panicle, our proposed image automatic segmentation method can detect the full grain and empty grain, after which the RSSR can be calculated by our proposed rice seed setting rate optimization algorithm (RSSROA). Finally, the proposed method was used to predict the RSSR during which process, the average identification accuracy reached 99.43%. This method has therefore been proven as an effective, non-invasive method for high throughput identification and calculation of RSSR. It is also applicable to soybean yields, as well as wheat and other crops with similar characteristics.",rice grain identification; computer vision; deep learning; rice seed setting rate; image segmentation
"Hand gesture recognition is one of the most sought technologies in the field of machine learning and computer vision. There has been an unprecedented demand for applications through which one can detect the hand signs for deaf people and people who use sign language to communicate, thereby detecting hand signs and correspondingly predicting the next word or recommending the word that may be most appropriate, followed by producing the word that the deaf people and people who use sign language to communicate want to say. This article presents an approach to develop such a system by that we can determine the most appropriate character from the sign that is being shown by the user or the person to the system. To enable pattern recognition, various machine learning techniques have been explored and we have used the CNN networks as a reliable solution in our context. The creation of such a system involves several convolution layers through which features have been captured layer by layer. The gathered features from the image are further used for training the model. The trained model efficiently predicts the most appropriate character in response to the sign exposed to the model. Thereafter, the predicted character is used to predict further words from it according to the recommendation system used in this case. The proposed system attains a prediction accuracy of 91.07%.",
"Arthropod abundance, biomass and taxonomic diversity are key metrics often used to assess the efficacy of restoration efforts. Gathering these metrics is a slow and laborious process, quantified by an expert manually sorting and weighing arthropod specimens. We present a tool to accelerate bulk arthropod classification and biomass estimates utilizing machine learning methods for computer vision. Our approach requires pre-sorted arthropod samples to create a training dataset. We construct a dataset considering 18 terrestrial arthropod functional groups collected in southern Ontario, Canada. The dataset contains 517 high-resolution images with approximately 20 individuals per image taken from either a petri dish or a bulk tray. Our tool uses the watershed algorithm to obtain precisely cropped individuals without any object annotations. After manually sorting cropped images of biological 'debris' and petri dish edges, three classifiers, DenseNet121, ResNet101 and MobileNetv2, each with trade-offs of computational efficiency versus accuracy, are trained and compared to predict arthropod functional groups for each cropped individual. To calculate biomass, we compare seven linear and nonlinear models considering the arthropod pixel masks obtained using the watershed algorithm, in combination with images of a single function group with recorded weights, to calculate the per pixel density per functional group. From our experimentation, we recommend using DenseNet121 as it had the highest top-1 functional group classification accuracy, likely a result of being the model with the largest number of parameters, with 86.14% considering the 20 labelled classes (18 arthropods plus debris and petri dish edge) in comparison to ResNet101 (85.10%) and MobileNetv2 (84.94%). For biomass estimation, we recommend using the average per pixel density which had the highest ranked performance considering both total error, 0.043 g (0.855% error), and cumulative class-specific error, 1.62 g (40.67% average error across all classes), in comparison to the total ground truth biomass of 5.10 g. Our estimated Simpson's Index of Diversity was 0.9404 in comparison to the ground truth 0.9408. Our method simultaneously classifies >1,000 arthropods to functional groupings while estimating total and class specific biomass, without any computer vision bounding box or mask annotations, all from a single photo. We release our code and dataset to further research efforts in computer vision for arthropod classification.",abundance; arthropod; biodiversity; biomass; classification; computer vision; deep learning; malaise trap
"As the COVID-19 pandemic continues, the need for a better health care facility is highlighted more than ever. Besides physical health, mental health conditions have become a significant concern. Unfortunately, there are few opportunities for people to receive mental health care. There are inadequate facilities for seeking mental health support even in big cities, let alone remote areas. This paper presents the structure and implementation procedures for a mental health support system combining technology and professionals. The system is a web platform where mental health seekers can register and use functionalities like NLP-based chatbot for personality assessment, chatting with like-minded people, and one-to-one video conferencing with a mental health professional. The video calling feature of the system has emotion detection capabilities using computer vision. The system also includes downloadable prescription facilities and a payment gateway for secure transactions. From a technological aspect, the conversational NLP-based chatbot and computer vision-powered video calling are the system's most important features. The system has a documentation facility to analyze the mental health condition over time. The web platform is built using React.js for the frontend and Express.js for the backend. MongoDB is used as the database of the platform. The NLP chatbot is built on a three-layered deep neural network model that is programmed in the Python language and uses the NLTK, TensorFlow, and Keras sequential API. Video conference is one of the most important features of the platform. To create the video calling feature, Express.js, Socket.io, and Socket.io-client have been used. The emotion detection feature is implemented on video conferences using computer vision, Haar Cascade, and TensorFlow. All the implemented features are tested and work fine. The targeted users for the platform are teenagers, youth, and the middle-aged population. Mental health-seeking is still considered taboo in some societies today. Apart from basic established facilities, this social dilemma of undergoing treatment for mental health is causing severe damage to individuals. A solution to this problem can be a remote platform for mental health support. With this goal in mind, this system is designed to provide mental health support to people remotely from anywhere worldwide.",
"Crowd counting is a computer vision task on which considerable progress has recently been made thanks to convolutional neural networks. However, it remains a challenging task even in scene-specific settings, in real-world application scenarios where no representative images of the target scene are available, not even unlabelled, for training or fine-tuning a crowd counting model. Inspired by previous work in other computer vision tasks, we propose a simple but effective solution for the above application scenario, which consists of automatically building a scene-specific training set of synthetic images. Our solution does not require from end-users any manual annotation effort nor the collection of representative images of the target scene. Extensive experiments on several benchmark data sets show that the proposed solution can improve the effectiveness of existing crowd counting methods. (C) 2021 Elsevier Ltd. All rights reserved.",Crowd counting; Scene-specific settings; Synthetic training images
"During the phase of building survey, spalling and its severity should be detected as earlier as possible to provide timely information on structural heath to building maintenance agency. Correct detection of spall severity can significantly help decision makers develop effective maintenance schedule and prioritize their financial resources better. This study aims at developing a computer vision-based method for automatic classification of concrete spalling severity. Based on input image of concrete surface, the method is capable of distinguishing between a minor spalling in which the depth of the broken-off material is less than the concrete cover layer and a deep spalling in which the reinforcing steel bars have been revealed. To characterize concrete surface condition, image texture descriptors of statistical measurement of color channels, gray-level run length, and center-symmetric local binary pattern are used. Based on these texture-based features, the support vector machine classifier optimized by the jellyfish search metaheuristic is put forward to construct a decision boundary that partitions the input data into two classes of shallow spalling and deep spalling. A dataset consisting of 300 image samples has been collected to train and verify the proposed computer vision method. Experimental results supported by the Wilcoxon signed-rank test point out that the newly developed method is highly suitable for concrete spall severity classification with accuracy rate. 93.33%, F1 score. 0.93, and area under the receiver operating characteristic curve. 0.97.",
"Human pose estimation (HPE) is crucial for computer vision (CV). Moreover, it's a vital step for computers to understand human actions and behaviours. However, the huge number of parameters and calculations in the HPE model have brought big challenges to deploy to resource-constrained mobile devices. Aiming to overcome the challenge, we propose a sparse pruning method (SPM) for the HPE model. First, L1 regularisation is added in the training phase of the original model, and network parameters of the convolution layers (CLs) and batch normalisation layers (BNLs) are sparsely trained to obtain a network structure with sparse weights. We then combine the sparse weights of filters with the scaling parameters of the BNLs to determine their importance. Finally, the structured pruning method is used to prune the sparse filters and corresponding channels. SPM can reduce the number of model parameters and calculations without affecting precision. Promising results indicate that SPM outperforms other advanced pruning methods.",Human pose estimation; computer vision; sparse pruning method; scaling parameters; structured pruning method
"Computer vision is a key technique to make agricultural machinery smart. Deep neural network has achieved great success in computer vision. How to use it at a small size, low cost, low power consumption device with high accuracy and speed on strawberry harvesting machinery has drawn much research attention. Since the infield situation has reduced number of objects and that they are easier to be distinguished from the background compared to other computer vision datasets, the huge neural network structure can be simplified in order to speed up the detection inference without penalizing the detection accuracy. In this research, a new deep neural network called RTSD-Net is proposed based on stat-of-art light-weighted YOLOv4-tiny with reduced layers and modified structure for real-time strawberry detection under infield condition. The original CSPNet was replaced by 2 types of CSPNet designed with reduced parameters and a simplified structure and 4 new network structures are designed by combining these 2 types. The performances of the 4 networks were evaluated. It was observed that the number of parameters of these 4 networks and the detection speed of the model is negatively correlated. Simplified structure and reduced parameters can contribute to faster operational speed. The last one was selected and named as RTSD-Net. Comparing with YOLOv4 tiny, the accuracy of RTSD-Net is only reduced by 0.62% but the speed is increased by 25FP5, which is 25.93% higher than that of YOLOv4-tiny. Embedded system Jetson Nano was selected as the evaluation platform to evaluate the RTSD-Net's performance for edge computing. The original Open Neural Network Exchange (ONNX) model was loaded on Jetson Nano and the speed of RTSD-Net was 13.1FPS, which is 19.0% higher than that of YOLOv4-tiny. After speeded up by TensorRT method, the transformed model reached 25.20fps, which is twice as fast as the ONNX model, and 15% faster than the YOLOv4-tiny model. After speeding up, the efficiency of RTSD-Net is enough for computer vision based strawberry detection and harvesting. In summary, the proposed RTSD-Net has good potential in smart strawberry harvesting machinery and the idea of redesigning neural structure and reducing parameters to speed up the detection rate of deep neural network is expected to have good application in edge computing.",YOLO; Object detection; Deep learning; Edge computing
"The rise of deep learning in today's applications entailed an increasing need in explaining the model's decisions beyond prediction performances in order to foster trust and accountability. Recently, the field of explainable AI (XAI) has developed methods that provide such explanations for already trained neural networks. In computer vision tasks such explanations, termed heatmaps, visualize the contributions of individual pixels to the prediction. So far XAI methods along with their heatmaps were mainly validated qualitatively via human-based assessment, or evaluated through auxiliary proxy tasks such as pixel perturbation, weak object localization or randomization tests. Due to the lack of an objective and commonly accepted quality measure for heatmaps, it was debatable which XAI method performs best and whether explanations can be trusted at all. In the present work, we tackle the problem by proposing a ground truth based evaluation framework for XAI methods based on the CLEVR visual question answering task. Our framework provides a (1) selective, (2) controlled and (3) realistic testbed for the evaluation of neural network explanations. We compare ten different explanation methods, resulting in new insights about the quality and properties of XAI methods, sometimes contradicting with conclusions from previous comparative studies. The CLEVR-XAI dataset and the benchmarking code can be found at https://github.com/ahmedmagdiosman/clevr-xai.",Explainable AI; Evaluation; Benchmark; Convolutional neural network; Visual question answering; Computer vision; Relation network
"Head pose estimation (HPE) notoriously represents a crucial task for many computer vision applications in robotics, biometry and video surveillance. While, in general, HPE can be performed on both still images and frames extracted from live video or captured footage, its functional approach and the related processing pipeline may have a significant impact on suitability to different application contexts. This implies that, for any real-time application in which HPE is required, this information, namely the angular value of yaw, pitch and roll axes, should be provided in real-time as well. Since, so far, the primary aim in HPE research has been on improving estimation accuracy, there are only a few works reporting the computing time of the proposed HPE method and even less explicitly addressing it. The present work stems from a previous Partitioned Iterated Function Systems-based approach providing state-of-the-art accuracy with high computing cost, and improve it by means of two regression models, namely Gradient Boosting Regressor and Extreme Gradient Boosting Regressor, achieving much faster response and an even lower mean absolute error on the yaw and roll axis, as shown by experiments conducted on the BIWI and AFLW2000 datasets.",computer vision; face biometrics; pose estimation
"Semantic segmentation is a challenging task in computer vision which is widely used in autonomous driving and scene understanding. State-of-the-art semantic segmentation networks, like DeepLab and PSPNet, make full use of multiple feature information to improve spatial resolution. However, the feature resolution in the scale-axis is not dense enough for practical applications. To tackle this problem, a multi-stream network is designed with atrous convolutional layers at multiple rates to capture objects and context at multiple scales. Furthermore, intra-connections and inter-connections are designed to fuse multi-scale features densely which produce a feature pyramid with much larger scale diversity and larger receptive field by involving small quantity of computation. The proposed module can be easily used in other methods and it helps to increase the performance. Compared with existing methods, the proposed network, called Multi-stream Densely Connected Network, reaches competitive results on ADE20K dataset, PASCAL VOC 2012 dataset, and Cityscapes dataset.",computer vision; image processing; image segmentation
"Deep learning has been widely applied in many fields. Efficient optimization algorithms contribute a lot to the enhancement of deep learning. However, reverse studies on how deep learning can solve optimization problems, especially mathematical programming, are relatively scarce. In this work, we aim to initiate a discussion on using deep learning to solve parametric mathematical programming, which can be converted to find a mapping from parameter space to solution space. Given that deep convolutional neural networks (DCNNs) have been successfully applied in the field of computer vision, converting a mathematical programming problem into an image representation may provide the solution. This work takes the 0-1 knapsack problem (KP) as an example to build an original image representation method. After modifying the image details, the proposed method is extended to the general 0-1 linear programming (LP) problem, and a deep learning architecture is designed to solve the transferred computer vision problem. The efficiency of the proposed DCNNs method is validated on a large number of problems, and results show that this method can solve the 0-1 LP accurately and efficiently without iteration. The solution speed is 20 times faster than that of traditional optimization solvers.",Deep learning; Mathematical programming; Image representation; Bars; Linear programming; Optimized production technology; Optimization; 0-1 linear programming (LP); deep convolutional neural network (DCNN); deep learning; image representation; mathematical programming; optimization
"The precise inspection of geometric parameters is crucial for quality control in the context of Industry 4.0. The current technique of precise inspection depends on the operation of professional personnel, and the measuring accuracy is restricted by the proficiency of operators. To solve the defects, this paper proposes a precise inspection framework for the geometric parameters of polyvinyl chloride (PVC) pipe section (G-PVC), using low-cost visual sensors and high-precision computer vision algorithms. Firstly, a robust imaging system was built to acquire images of a PVC pipe section under irregular illumination changes. Next, an engineering semantic model was established to calculate G-PVC like inner diameter, outer diameter, wall thickness, and roundness. After that, a region-of-interest (ROI) extraction algorithm was combined with an improved edge operator to obtain the coordinates of measured points on PVC end-face image in a stable and precise manner. Finally, our framework was proved highly precise and robust through experiments.",polyvinyl chloride (PVC) pipe; geometric; parameters; visual inspection; region of&nbsp; interest (ROI); edge operator
"Technological advances enable the design of systems that interact more closely with humans in a multitude of previously unsuspected fields. Martial arts are not outside the application of these techniques. From the point of view of the modeling of human movement in relation to the learning of complex motor skills, martial arts are of interest because they are articulated around a system of movements that are predefined, or at least, bounded, and governed by the laws of Physics. Their execution must be learned after continuous practice over time. Literature suggests that artificial intelligence algorithms, such as those used for computer vision, can model the movements performed. Thus, they can be compared with a good execution as well as analyze their temporal evolution during learning. We are exploring the application of this approach to model psychomotor performance in Karate combats (called kumites), which are characterized by the explosiveness of their movements. In addition, modeling psychomotor performance in a kumite requires the modeling of the joint interaction of two participants, while most current research efforts in human movement computing focus on the modeling of movements performed individually. Thus, in this work, we explore how to apply a pose estimation algorithm to extract the features of some predefined movements of Ippon Kihon kumite (a one-step conventional assault) and compare classification metrics with four data mining algorithms, obtaining high values with them.",human activity recognition (HAR); computer vision; deep learning; human pose estimation (HPE); OpenPose; martial arts; karate
"Wheat head detection is a core computer vision problem related to plant phenotyping that in recent years has seen increased interest as large-scale datasets have been made available for use in research. In deep learning problems with limited training data, synthetic data have been shown to improve performance by increasing the number of training examples available but have had limited effectiveness due to domain shift. To overcome this, many adversarial approaches such as Generative Adversarial Networks (GANs) have been proposed as a solution by better aligning the distribution of synthetic data to that of real images through domain augmentation. In this paper, we examine the impacts of performing wheat head detection on the global wheat head challenge dataset using synthetic data to supplement the original dataset. Through our experimentation, we demonstrate the challenges of performing domain augmentation where the target domain is large and diverse. We then present a novel approach to improving scores through using heatmap regression as a support network, and clustering to combat high variation of the target domain.",plant phenotyping; computer vision; deep learning; domain adaptation
"Computer vision is currently experiencing success in various domains due to the harnessing of deep learning strategies. In the case of precision agriculture, computer vision is being investigated for detecting fruits from orchards. However, such strategies limit too-high complexity computation that is impossible to embed in an automated device. Nevertheless, most investigation of fruit detection is limited to a single fruit, resulting in the necessity of a one-to-many object detection system. This paper introduces a generic detection mechanism named FruitDet, designed to be prominent for detecting fruits. The FruitDet architecture is designed on the YOLO pipeline and achieves better performance in detecting fruits than any other detection model. The backbone of the detection model is implemented using DenseNet architecture. Further, the FruitDet is packed with newer concepts: attentive pooling, bottleneck spatial pyramid pooling, and blackout mechanism. The detection mechanism is benchmarked using five datasets, which combines a total of eight different fruit classes. The FruitDet architecture acquires better performance than any other recognized detection methods in fruit detection.",deep learning; object detection; agriculture; convolutional neural network
"Human action recognition (HAR) has gained significant attention recently as it can be adopted for a smart surveillance system in Multimedia. However, HAR is a challenging task because of the variety of human actions in daily life. Various solutions based on computer vision (CV) have been proposed in the literature which did not prove to be successful due to large video sequences which need to be processed in surveillance systems. The problem exacerbates in the presence of multi-view cameras. Recently, the development of deep learning (DL)-based systems has shown significant success for HAR even for multi-view camera systems. In this research work, a DL-based design is proposed for HAR. The proposed design consists of multiple steps including feature mapping, feature fusion and feature selection. For the initial feature mapping step, two pre-trained models are considered, such as DenseNet201 and InceptionV3. Later, the extracted deep features are fused using the Serial based Extended (SbE) approach. Later on, the best features are selected using Kurtosis-controlled Weighted KNN. The selected features are classified using several supervised learning algorithms. To show the efficacy of the proposed design, we used several datasets, such as KTH, IXMAS, WVU, and Hollywood. Experimental results showed that the proposed design achieved accuracies of 99.3%, 97.4%, 99.8%, and 99.9%, respectively, on these datasets. Furthermore, the feature selection step performed better in terms of computational time compared with the state-of-the-art.",human action recognition; deep learning; features fusion; features selection; recognition
"Rail corrugation appears as oscillatory wear on the rail surface caused by the interaction between the train wheels and the railway. Corrugation shortens railway service life and forces early rail replacement. Consequently, service can be suspended for days during rail replacement, adversely affecting an important means of transportation. We propose an inspection method for rail corrugation using computer vision through an algorithm based on feature descriptors to automatically distinguish corrugated from normal surfaces. We extract seven features and concatenate them to form a feature vector obtained from a railway image. The feature vector is then used to build support vector machine. Data were collected from seven different tracks as video streams acquired at 30 fps. The trained support vector machine was used to predict test frames of rails as being either corrugated or normal. The proposed method achieved a high performance, with 97.11% accuracy, 95.52% precision, and 97.97% recall. Experimental results show that our method is more effective in identifying corrugated images than reference state-of the art works.",rail corrugation detection; cepstrum transformation; frequency-domain analysis; static harmonic feature; support vector machine
"Tailing is defined as an event where a suspicious person follows someone closely. We define the problem of tailing detection from videos as an anomaly detection problem, where the goal is to find abnormalities in the walking pattern of the pedestrians (victim and follower). We, therefore, propose a modified Time-Series Vision Transformer (TSViT), a method for anomaly detection in video, specifically for tailing detection with a small dataset. We introduce an effective way to train TSViT with a small dataset by regularizing the prediction model. To do so, we first encode the spatial information of the pedestrians into 2D patterns and then pass them as tokens to the TSViT. Through a series of experiments, we show that the tailing detection on a small dataset using TSViT outperforms popular CNN-based architectures, as the CNN architectures tend to overfit with a small dataset of time-series images. We also show that when using time-series images, the performance of CNN-based architecture gradually drops, as the network depth is increased, to increase its capacity. On the other hand, a decreasing number of heads in Vision Transformer architecture shows good performance on time-series images, and the performance is further increased as the input resolution of the images is increased. Experimental results demonstrate that the TSViT performs better than the handcrafted rule-based method and CNN-based method for tailing detection. TSViT can be used in many applications for video anomaly detection, even with a small dataset.",tailing detection; Vision Transformer; anomaly detection; deep learning; computer vision
"A video-based method to quantify animal posture movement is a powerful way to analyze animal behavior. Both humans and fish can judge the physiological state through the skeleton framework. However, it is challenging for farmers to judge the breeding state in the complex underwater environment. Therefore, images can be transmitted by the underwater camera and monitored by a computer vision model. However, it lacks datasets in artificial intelligence and is unable to train deep neural networks. The main contributions of this paper include: (1) the world's first fish posture database is established. 10 key points of each fish are manually marked. The fish flock images were taken in the experimental tank and 1000 single fish images were separated from the fish flock. (2) A two-stage attitude estimation model is used to detect fish key points. The evaluation of the algorithm performance indicates the precision of detection reaches 90.61%, F1-score reaches 90%, and Fps also reaches 23.26. We made a preliminary exploration on the pose estimation of fish and provided a feasible idea for fish pose estimation.",aquaculture automation; rotating box; fish detection; fish pose; computer vision
"Cameras have been widely used in traffic operations. While many technologically smart camera solutions in the market can be integrated into Intelligent Transport Systems (ITS) for automated detection, monitoring and data generation, many Network Operations (a.k.a Traffic Control) Centres still use legacy camera systems as manual surveillance devices. In this paper, we demonstrate effective use of these older assets by applying computer vision techniques to extract traffic data from videos captured by legacy cameras. In our proposed vision-based pipeline, we adopt recent state-of-the-art object detectors and transfer-learning to detect vehicles, pedestrians, and cyclists from monocular videos. By weakly calibrating the camera, we demonstrate a novel application of the image-to-world homography which gives our monocular vision system the efficacy of counting vehicles by lane and estimating vehicle length and speed in real-world units. Our pipeline also includes a module which combines a convolutional neural network (CNN) classifier with projective geometry information to classify vehicles. We have tested it on videos captured at several sites with different traffic flow conditions and compared the results with the data collected by piezoelectric sensors. Our experimental results show that the proposed pipeline can process 60 frames per second for pre-recorded videos and yield high-quality metadata for further traffic analysis.",Cameras; Pipelines; Detectors; Videos; Australia; Urban areas; Computer vision; Intelligent transportation systems (ITS); machine learning; object detection; traffic image analysis; object tracking; camera calibration
"Object detection is one of the most critical tasks in the field of Computer vision. This task comprises identifying and localizing an object in the image. Architectural floor plans represent the layout of buildings and apartments. The floor plans consist of walls, windows, stairs, and other furniture objects. While recognizing floor plan objects is straightforward for humans, automatically processing floor plans and recognizing objects is challenging. In this work, we investigate the performance of the recently introduced Cascade Mask R-CNN network to solve object detection in floor plan images. Furthermore, we experimentally establish that deformable convolution works better than conventional convolutions in the proposed framework. Prior datasets for object detection in floor plan images are either publicly unavailable or contain few samples. We introduce SFPI, a novel synthetic floor plan dataset consisting of 10,000 images to address this issue. Our proposed method conveniently exceeds the previous state-of-the-art results on the SESYD dataset with an mAP of 98.1%. Moreover, it sets impressive baseline results on our novel SFPI dataset with an mAP of 99.8%. We believe that introducing the modern dataset enables the researcher to enhance the research in this domain.",object detection; Cascade Mask R-CNN; floor plan images; deep learning; transfer learning; dataset augmentation; computer vision
"Raspberries are fruit of great importance for human beings. Their products are segmented by quality. However, estimating raspberry quality is a manual process carried out at the reception of the fruit processing plant, and is thus exposed to factors that could distort the measurement. The agriculture industry has increased the use of deep learning (DL) in computer vision systems. Non-destructive and computer vision equipment and methods are proposed to solve the problem of estimating the quality of raspberries in a tray. To solve the issue of estimating the quality of raspberries in a picking tray, prototype equipment is developed to determine the quality of raspberry trays using computer vision techniques and convolutional neural networks from images captured in the visible RGB spectrum. The Faster R-CNN object-detection algorithm is used, and different pretrained CNN networks are evaluated as a backbone to develop the software for the developed equipment. To avoid imbalance in the dataset, an individual object-detection model is trained and optimized for each detection class. Finally, both hardware and software are effectively integrated. A conceptual test is performed in a real industrial scenario, thus achieving an automatic evaluation of the quality of the raspberry tray, in this way eliminating the intervention of the human expert and eliminating errors involved in visual analysis. Excellent results were obtained in the conceptual test performed, reaching in some cases precision of 100%, reducing the evaluation time per raspberry tray image to 30 s on average, allowing the evaluation of a larger and representative sample of the raspberry batch arriving at the processing plant.",raspberries; deep learning; disease and defect detection
"Pothole repair is one of the paramount tasks in road maintenance. Effective road surface monitoring is an ongoing challenge to the management agency. The current pothole detection, which is conducted image processing with a manual operation, is labour-intensive and time-consuming. Computer vision offers a mean to automate its visual inspection process using digital imaging, hence, identifying potholes from a series of images. The goal of this study is to apply different YOLO models for pothole detection. Three state-of-the-art object detection frameworks (i.e., YOLOv4, YOLOv4-tiny, and YOLOv5s) are experimented to measure their performance involved in real-time responsiveness and detection accuracy using the image set. The image set is identified by running the deep convolutional neural network (CNN) on several deep learning pothole detectors. After collecting a set of 665 images in 720 x 720 pixels resolution that captures various types of potholes on different road surface conditions, the set is divided into training, testing, and validation subsets. A mean average precision at 50% Intersection-over-Union threshold (mAP_0.5) is used to measure the performance of models. The study result shows that the mAP_0.5 of YOLOv4, YOLOv4-tiny, and YOLOv5s are 77.7%, 78.7%, and 74.8%, respectively. It confirms that the YOLOv4-tiny is the best fit model for pothole detection.",computer vision; real-time; pothole detection; deep learning; YOLO
"Insects are declining in abundance and diversity, but their population trends remain uncertain as insects are difficult to monitor. Manual methods require substantial time investment in trapping and subsequent species identification. Camera trapping can alleviate some of the manual fieldwork, but the large quantities of image data are challenging to analyse. By embedding the image analyses into the recording process using computer vision techniques, it is possible to focus efforts on the most ecologically relevant image data. Here, we present an intelligent camera system, capable of detecting, tracking, and identifying individual insects in situ. We constructed the system from commercial off-the-shelf components and used deep learning open source software to perform species detection and classification. We present the Insect Classification and Tracking algorithm (ICT) that performs real-time classification and tracking at 0.33 frames per second. The system can upload summary data on the identity and movement track of insects to a server via the internet on a daily basis. We tested our system during the summer 2020 and detected 2994 insect tracks across 98 days. We achieved an average precision of 89% for correctly classified insect tracks of eight different species. This result was based on 504 manually verified tracks observed in videos during 10 days with varying insect activities. Using the track data, we could estimate the mean residence time for individual flower visiting insects within the field of view of the camera, and we were able to show a substantial variation in residence time among insect taxa. For honeybees, which were most abundant, residence time also varied through the season in relation to the plant species in bloom. Our proposed automated system showed promising results in non-destructive and real-time monitoring of insects and provides novel information about phenology, abundance, foraging behaviour, and movement ecology of flower visiting insects.",Computer vision; deep learning; insects; pollinators; real-time; tracking
"Human Action Recognition (HAR) has achieved a remarkable milestone in the field of computer vision. Apart from its varied applications in human-computer interactions, surveillance systems and robotics, in recent times, it has extended its applicability in the fields like healthcare, multimedia retrieval, social networking, and education as well. Over the years, various approaches have been proposed by researchers to develop systems for HAR. In this context, this survey mainly deals with the various categories of approaches that have been proposed for HAR in the last ten years. To be specific, HAR techniques range from conventional machine learning methods to recently popular deep learning methods, and this field is growing fast. Hence, there is a need for frequent surveys to keep track of the latest techniques employed, the corresponding results achieved, and the problems that remain to help show the path forward, which this survey aims to accomplish. HAR can be classified into two divisions: unimodal and multimodal depending on the type of input vectors-unimodal implies that the data comes from a single source, while multimodal means the input dataset is from more than one source. This survey covers significant methods developed for the unimodal HAR in the past decade. The unimodal methods have been classified and described using the concepts of machine learning. Further, numerous models suggested for HAR using deep learning have been discussed elaborately. A list of different feature extractors and a detailed account of some majorly used video and still-image datasets have also been described in this survey, along with some useful insights into future work scope.",
"In the last few years, we have witnessed a renewed and fast-growing interest in continual learning with deep neural networks with the shared objective of making current AI systems more adaptive, efficient and autonomous. However, despite the significant and undoubted progress of the field in addressing the issue of catastrophic forgetting, benchmarking different continual learning approaches is a difficult task by itself. In fact, given the proliferation of different settings, training and evaluation protocols, metrics and nomenclature, it is often tricky to properly characterize a continual learning algorithm, relate it to other solutions and gauge its real-world applicability. The first Continual Learning in Computer Vision challenge held at CVPR in 2020 has been one of the first opportunities to evaluate different continual learning algorithms on a common hardware with a large set of shared evaluation metrics and 3 different settings based on the realistic CORe50 video benchmark. In this paper, we report the main results of the competition, which counted more than 79 teams registered and 11 finalists. We also summarize the winning approaches, current challenges and future research directions. (C) 2021 Elsevier B.V. All rights reserved.",Continual learning; Lifelong learning; Incremental learning; Challenge; Computer vision
"Determining the optical flow of a video is a compute-intensive task essential for computer vision. For achieving this processing in real time, the whole algorithm deployment chain must be thought of for efficiency first. The development is usually divided into two parts: first, designing an algorithm that meets precision constraints, then, implementing and optimizing its execution on the targeted platform. We argue that unifying those operations enhances performance on the embedded processor. This paper is based on an industrial use case of computer vision. The objective is to determine dense optical flow in real time on an embedded GPU platform: the Nvidia AGX Xavier. The CLG (combined local-global) optical flow method, initially chosen, is analyzed to understand the convergence speed of its underlying optimization problem. The Jacobi solver is selected for implementation because of its parallel nature. The whole multi-level processing is then ported to the GPU, using several specific optimization strategies. In particular, we analyze the impact of fusing the solver's iterations with the roofline model. As a result, with a 30 W power budget, our implementation runs at 60FPS, on 640 x 512 images, with a four-level processing. Hopefully, this example should provide feedback on the issues that arise when trying to port a method to a parallel platform and serve for further implementations of computer vision algorithms on specialized hardware.",Algorithm design; Optical flow; GPU optimization; Linear solvers; Image processing
"The number of computer vision and image processing tasks has increased during the last years. Although Python is most of the time the first choice in this area, there are situations, where the utilization of another programming language such as Java should be preferred. For this reason, multiple Java based frameworks as e.g. OpenIMAJ, ND4J or multiple OpenCV wrappers are available. Unfortunately, these frameworks are not interoperable at all. In this work, the open-source Imaging Framework is introduced to solve exactly this problem. The project features a concept for combining multiple frameworks and provides an interoperable and extendable foundation to 9 image-related projects with 10 different image representations in Java. (C) 2021 The Authors. Published by Elsevier B.V.",Java; Image processing; Computer vision; Interoperability and extendability
"Neural networks have become state-of-the-art computer vision tools for tasks that learn implicit representations of geometrical scenes. This paper proposes a two-part network architecture that exploits a view-synthesis network to understand a context scene and a graph convolutional network to generate a shape body model of an object within the field of view of a spacecraft's optical navigation sensors. Once the first part of the network's architecture understands the spacecraft's environment, it can generate images from novel observations. The second part uses a multiview set of images to construct a 3D graph-based representation of the object. The proposed network pipeline produces shape models with accuracies that compete with state-of-the-art methods currently used for missions to small bodies. The network pipeline can be trained for multi-environment missions. Moreover, the onboard implementation may be more cost-effective than the current state-of-the-art.",Resident Space Object; Neural Networks; Computer Vision; Sensors
"Monitoring the growth conditions and behavior of fish will enable scientific management, reduce the threat of losses caused by disease and stress. Traditional monitoring methods are time-consuming, laborious, and untimely monitoring readily leads to aquaculture accidents. As a non-invasive, objective, and repeatable tool, machine vision systems have been widely used in various aspects of aquaculture monitoring. Nevertheless, the complex underwater environment makes it difficult to obtain ideal data processing results only using traditional image processing methods. Due to their powerful feature extraction capabilities, deep learning (DL) algorithms have been widely used in underwater image processing. Hence, the combination of DL algorithms and machine vision for the automated monitoring of aquaculture is of great importance. As evidence for the multidisciplinary aspects of DL applications, attention is focused on the latest DL methods applied to five fields of research: classification, detection, counting, behavior recognition, and biomass estimation. Meanwhile, due to the low training efficiency of DL models caused by insufficient dataset, transfer learning and GAN have also put into spotlight of this filed to pursue high performance of DL models. We also present the challenges and benchmarks in terms of the advantages and disadvantages of the selected method in each field. In addition, we review the sources of image acquisition and pre-processing methods in aquaculture. Finally, the challenges and prospects of DL in aquaculture machine vision systems are discussed. The literature review shows that the deep neural networks such as AlexNet, LSTM, VGG, and GoogLeNet, have been used for aquaculture machine vision systems.",Deep learning; Machine vision; Aquaculture; Image acquisition; Image preprocessing
"In recent years, a steady swell of biological image data has driven rapid progress in healthcare applications of computer vision and machine learning. To make sense of this data, scientists often rely on detailed annotations from domain experts for training artificial intelligence (AI) algorithms. The time-consuming and costly process of collecting annotations presents a sizable bottleneck for AI research and development. HALS (Human-Augmenting Labeling System) is a collaborative human-AI labeling workflow that uses an iterative review-and-revise model to improve the efficiency of this critical process in computational pathology.",
"Image captioning means generate descriptive sentences from a query image automatically. It has recently received widespread attention from the computer vision and natural language processing communities as an emerging visual task. Currently, both components have evolved considerably by exploiting object regions, attributes, attention mechanism methods, entity recognition with novelties, and training strategies. However, despite the impressive results, the research has not yet come to a conclusive answer. This survey aims to provide a comprehensive overview of image captioning methods, from technical architectures to benchmark datasets, evaluation metrics, and comparison of state-of-the-art methods. In particular, image captioning methods are divided into different categories based on the technique adopted. Representative methods in each class are summarized, and their advantages and limitations are discussed. Moreover, many related state-of-the-art studies were quantitatively compared to determine the recent trends and future directions in image captioning. The ultimate goal of this work is to serve as a tool for understanding the existing literature and highlighting future directions in the area of image captioning for Computer Vision and Natural Language Processing communities may benefit from.",
"Histopathology is diagnosis based on visual examination of tissue sections under a microscope. With the growing number of digitally scanned tissue slide images, computer-based segmentation and classification of these images is a high-demand area of research. Convolutional neural networks (CNNs) constitute the most popular classification architecture for a variety of image classification problems. However, applying CNNs to histology slides is not a trivial task and has several challenges, ranging from variations in the colors of slides to excessive high resolution and lack of proper labeling. In this advanced review, we introduce the application of CNN-based architectures to digital histological image analysis, discuss some problems associated with such analysis, and look at possible solutions. This article is categorized under: Application Areas > Health Care Fundamental Concepts of Data and Knowledge > Big Data Mining Technologies > Machine Learning",CNN; computer vision; deep learning; histopathology; manifold learning
"In this issue of Blood, Matek et al(1) developed a computer vision model to differentiate between bone marrow cell morphologies on a large, expert annotated dataset.",
"Distracted or drowsy driving is unsafe driving behavior responsible for thousands of crashes every year. Studying driver behavior has challenges associated with observing drivers in their natural environment. The naturalistic driving study (NDS) has become the most sought-after approach, since it eliminates the bias of a controlled setup, allowing researchers to understand drivers' behavior in real-world scenarios. Video recordings collected in NDS research are incredibly insightful in identifying driver errors. Computer vision techniques have been used to autonomously analyze video data and classify drivers' behavior. While computer vision scientists focus on image analytics, NDS researchers are interested in the factors impacting driver behavior. This survey paper makes a concerted effort to serve both communities by comprehensively reviewing studies, describing their data collection, computer vision techniques implemented, and performance in classifying driver behavior. The scope is limited to studies employing at least one camera observing the driver inside a vehicle. Based on their objective, papers have been classified as detecting low-level (e.g. head orientation) or high-level (e.g. distraction detection) driver information. Papers have been further classified based on the datasets they employ. In addition to twelve public datasets, many private datasets have also been identified, and their data collection design is discussed to highlight any impact on model performance. Across each task, algorithms employed and their performance are discussed to establish a baseline. A comparison of different frameworks for NDS video data analytics throws light on the existing gaps in the state-of-the-art that can be addressed by future computer vision research.",Vehicles; Task analysis; Computer vision; Feature extraction; Cameras; Accidents; Data mining; Driver behavior analysis; gaze; face detection; head orientation; drowsiness; driver distraction; lane change; survey
"Motor vehicle crashes are great threats to our life, which may result in numerous fatalities, as well as tremendous economic and societal costs. Driver inattention, either distraction or fatigue, is the major cause among human factors responsible for the crashes. Distracted driving has been getting increasingly severe, and has caused many more crashes than drowsy driving, while the latter has been more extensively studied. Therefore, we are motivated to present a comprehensive survey on vision-based approaches for driver distraction analysis. In the paper, we firstly provide an overview of driver distraction, then introduce the available datasets and explore the various cues for driver behavior distraction analysis. After that, two forms of driver behavior distraction (visual distraction and manual distraction) are analyzed separately. Lastly, we conclude the evolvement and future directions of driver distraction analysis for safe driving. To the best of our knowledge, we are the first to propose driver behavior distraction and analyze it in a hierarchical way.",Driver distraction; Vision-based methods; Driver inattention monitoring systems
"Deep neural networks have achieved great success in both computer vision and natural language processing tasks. How to improve the gradient flows is crucial in training very deep neural networks. To address this challenge, a gradient enhancement approach is proposed through constructing the short circuit neural connections. The proposed short circuit is a unidirectional neural connection that back propagates the sensitivities rather than gradients in neural networks from the deep layers to the shallow layers. Moreover, the short circuit is further formulated as a gradient truncation operation in its connecting layers, which can be plugged into the backbone models without introducing extra training parameters. Extensive experiments demonstrate that the deep neural networks, with the help of short circuit connection, gain a large margin of improvement over the baselines on both computer vision and natural language processing tasks. The work provides the promising solution to the low-resource scenarios, such as, intelligence transport systems of computer vision, question answering of natural language processing.",
"Timely and accurate recognition of construction waste (CW) composition can provide yardstick information for its subsequent management (e.g., segregation, determining proper disposal destination). Increasingly, smart technologies such as computer vision (CV), robotics, and artificial intelligence (AI) are deployed to automate waste composition recognition. Existing studies focus on individual waste objects in well-controlled environments, but do not consider the complexity of the real-life scenarios. This research takes the challenges of the mixture and clutter nature of CW as a departure point and attempts to automate CW composition recognition by using CV technologies. Firstly, meticulous data collection, cleansing, and annotation efforts are made to create a high-quality CW dataset comprising 5,366 images. Then, a state-of-the-art CV semantic segmentation technique, DeepLabv3+, is introduced to develop a CW segmentation model. Finally, several training hyperparameters are tested via orthogonal experiments to calibrate the model performance. The proposed approach achieved a mean Intersection over Union (mIoU) of 0.56 in segmenting nine types of materials/objects with a time performance of 0.51 s per image. The approach was found to be robust to variation of illumination and vehicle types. The study contributes to the important problem of material composition recognition, formalizing a deep learning-based semantic segmentation approach for CW composition recognition in complex environments. It paves the way for better CW management, particularly in engaging robotics, in the future. The trained models are hosted on GitHub, based on which researchers can further finetune for their specific applications.",Construction and demolition waste; Waste composition; Construction waste management; Artificial intelligence; Computer vision; Semantic segmentation
"The human vision system can efficiently recognize multiscale objects in cluttered backgrounds. The scheme can be achieved with a visual attention mechanism by concentrating visual resources to the saliency area while ignoring other task-irrelevant areas. However, in the computer vision community, when recognizing multiscale faces in cluttered backgrounds, object detection modules are necessary to locate face regions and reduce the influence of complex backgrounds on the recognition model, which inevitably increases the computational complexity. Motivated by the human vision system, this study proposes the attention developmental network to recognize multiscale faces without using face detectors. A top down attention mechanism is used to teach the network to focus on the face areas and ignore the backgrounds. An attention-based synapse maintenance mechanism is also introduced to further suppress the background pixels and improve the accuracy of face recognition. Comparative experiments show that our method can attain at least 13% of accuracy improvement over bionic neural networks and ResNet-based recognition networks on the same model scale with less training epochs. (c) 2021 Elsevier B.V. All rights reserved.",Face recognition; Developmental network; Attention mechanism; Synapse maintenance mechanism
"Bridges are prone to severe deterioration agents which promote their degradation over the course of their lifetime. Furthermore, maintenance budgets are being trimmed. This state of circumstances entails the development of a computer vision-based method for the condition assessment of bridge elements in an attempt to circumvent the drawbacks of visual inspection-based models. Scaling is progressive local flaking or loss in the surface portion of concrete that affects the functional and structural integrity of reinforced concrete bridges. As such, this research study proposes a self-adaptive three-tier method for the automated detection and assessment of scaling severity levels in reinforced concrete bridges. The first tier relies on the integration of cross entropy function and grey wolf optimization (GWO) algorithm for the segmentation of scaling pixels. The second tier is designated for the autonomous interpretation of scaling area. In this model, a hybrid feature extraction algorithm is proposed based on the fusion of singular value decomposition and discrete wavelet transform for the efficient and robust extraction of the most dominant features in scaling images. Then an integration of Elman neural network and GWO algorithm is proposed for the sake of improving the prediction accuracies of scaling area though optimization of both structure and parameters of Elman neural network. The third tier aims at establishing a unified scaling severity index to assess the extent of severities of scaling according to its area and depth. The developed method is validated through multi-layered comparative analysis that involved performance evaluation comparisons, statistical comparisons and box plots. Results demonstrated that the developed scaling detection model significantly outperformed a set of widely-utilized classical segmentation models achieving mean squared error, mean absolute error, peak signal to noise ratio and cross entropy of 0.175, 0.407, 55.754 and 26011.019, respectively. With regards to the developed scaling evaluation model, it accomplished remarkable better and more robust performance that other meta-heuristic-based Elman neural network models and conventional prediction models. In this context, it obtained mean absolute percentage error, root-mean squared error and mean absolute error 1.513%, 29.836 and 12.066, respectively, as per split validation. It is anticipated that the developed integrated computer vision-based method could serve as the basis of automated, reliable and cost-effective inspection platform of reinforced concrete bridges which can assist departments of transportation in taking effective preventive maintenance and rehabilitation actions.",Bridges; computer vision; scaling; cross entropy; grey wolf optimization; discrete wavelet transform; Elman neural network
"Submarines are considered extremely strategic for any naval army due to their stealth capability. Periscopes are crucial sensors for these vessels, and emerging to the surface or periscope depth is required to identify visual contacts through this device. This maneuver has many procedures and usually has to be fast and agile to avoid exposure. This paper presents and implements a novel architecture for real submarine periscopes developed for future Brazilian naval fleet operations. Our system consists of a probe that is connected to the craft and carries a 360 camera. We project and take the images inside the vessel using traditional VR/XR devices. We also propose and implement an efficient computer vision-based MR technique to estimate and display detected vessels effectively and precisely. The vessel detection model is trained using synthetic images. So, we built and made available a dataset composed of 99,000 images. Finally, we also estimate distances of the classified elements, showing all the information in an AR-based interface. Although the probe is wired-connected, it allows for the vessel to stand in deep positions, reducing its exposure and introducing a new way for submarine maneuvers and operations. We validate our proposal through a user experience experiment using 19 experts in periscope operations.",computer vision; deep learning; mixed reality; object detection; periscope; synthetic data; submarine; transfer learning
"Technological breakthroughs in recent years have led to a revolution in fields such as Machine Vision and Search and Rescue Robotics (SAR), thanks to the application and development of new and improved neural networks to vision models together with modern optical sensors that incorporate thermal cameras, capable of capturing data in post-disaster environments (PDE) with rustic conditions (low luminosity, suspended particles, obstructive materials). Due to the high risk posed by PDE because of the potential collapse of structures, electrical hazards, gas leakage, etc., primary intervention tasks such as victim identification are carried out by robotic teams, provided with specific sensors such as thermal, RGB cameras, and laser. The application of Convolutional Neural Networks (CNN) to computer vision is a breakthrough for detection algorithms. Conventional methods for victim identification in these environments use RGB image processing or trained dogs, but detection with RGB images is inefficient in the absence of light or presence of debris; on the other hand, developments with thermal images are limited to the field of surveillance. This paper's main contribution focuses on implementing a novel automatic method based on thermal image processing and CNN for victim identification in PDE, using a Robotic System that uses a quadruped robot for data capture and transmission to the central station. The robot's automatic data processing and control have been carried out through Robot Operating System (ROS). Several tests have been carried out in different environments to validate the proposed method, recreating PDE with varying conditions of light, from which the datasets have been generated for the training of three neural network models (Fast R-CNN, SSD, and YOLO). The method's efficiency has been tested against another method based on CNN and RGB images for the same task showing greater effectiveness in PDE main results show that the proposed method has an efficiency greater than 90%.",robotic systems; thermal images; convolutional neural networks; computer vision; search and rescue robots; ROS; Unitree A1
"Autonomous Vehicles (AVs) have the potential to solve many traffic problems, such as accidents, congestion and pollution. However, there are still challenges to overcome, for instance, AVs need to accurately perceive their environment to safely navigate in busy urban scenarios. The aim of this paper is to review recent articles on computer vision techniques that can be used to build an AV perception system. AV perception systems need to accurately detect non-static objects and predict their behaviour, as well as to detect static objects and recognise the information they are providing. This paper, in particular, focuses on the computer vision techniques used to detect pedestrians and vehicles. There have been many papers and reviews on pedestrians and vehicles detection so far. However, most of the past papers only reviewed pedestrian or vehicle detection separately. This review aims to present an overview of the AV systems in general, and then review and investigate several detection computer vision techniques for pedestrians and vehicles. The review concludes that both traditional and Deep Learning (DL) techniques have been used for pedestrian and vehicle detection; however, DL techniques have shown the best results. Although good detection results have been achieved for pedestrians and vehicles, the current algorithms still struggle to detect small, occluded, and truncated objects. In addition, there is limited research on how to improve detection performance in difficult light and weather conditions. Most of the algorithms have been tested on well-recognised datasets such as Caltech and KITTI; however, these datasets have their own limitations. Therefore, this paper recommends that future works should be implemented on more new challenging datasets, such as PIE and BDD100K.",autonomous vehicle; vehicle detection; pedestrian detection; generic object detection; deep learning; traditional technique
"With the growth of computer vision-based applications, an explosive amount of images have been uploaded to cloud servers that host such online computer vision algorithms, usually in the form of deep learning models. JPEG has been used as the de facto compression and encapsulation method for images. However, standard JPEG configuration does not always perform well for compressing images that are to be processed by a deep learning model-for example, the standard quality level of JPEG leads to 50% of size overhead (compared with the best quality level selection) on ImageNet under the same inference accuracy in popular computer vision models (e.g., InceptionNet and ResNet). Knowing this, designing a better JPEG configuration for online computer vision-based services is still extremely challenging. First, cloud-based computer vision models are usually a black box to end-users; thus, it is challenging to design JPEG configuration without knowing their model structures. Second, the optimal JPEG configuration is not fixed; instead, it is determined by confounding factors, including the characteristics of the input images and the model, the expected accuracy and image size, and so forth. In this article, we propose a reinforcement learning (RL)-based adaptive JPEG configuration framework, AdaCompress. In particular, we design an edge (i.e., user-side) RL agent that learns the optimal compression quality level to achieve an expected inference accuracy and upload image size, only from the online inference results, without knowing details of the model structures. Furthermore, we design an explore-exploit mechanism to let the framework fast switch an agent when it detects a performance degradation, mainly due to the input change (e.g., images captured across daytime and night). Our evaluation experiments using real-world online computer vision-based APIs from Amazon Rekognition, Face++, and Baidu Vision show that our approach outperforms existing baselines by reducing the size of images by one-half to one-third while the overall classification accuracy only decreases slightly. Meanwhile, AdaCompress adaptively re-trains or re-loads the RL agent promptly to maintain the performance.",Edge computing; reinforcement learning; adaptive compression; machine learning service
"Recently, artificial intelligence has been successfully used in fields, such as computer vision, voice, and big data analysis. However, various problems, such as security, privacy, and ethics, also occur owing to the development of artificial intelligence. One such problem are deepfakes. Deepfake is a compound word for deep learning and fake. It refers to a fake video created using artificial intelligence technology or the production process itself. Deepfakes can be exploited for political abuse, pornography, and fake information. This paper proposes a method to determine integrity by analyzing the computer vision features of digital content. The proposed method extracts the rate of change in the computer vision features of adjacent frames and then checks whether the video is manipulated. The test demonstrated the highest detection rate of 97% compared to the existing method or machine learning method. It also maintained the highest detection rate of 96%, even for the test that manipulates the matrix of the image to avoid the convolutional neural network detection method.",deepfake; computer vision; the rate of change
"Multi-view data are popular in many machine learning and computer vision applications. For example, in computer vision fields, one object can be described with images, text or videos. Recently, multi-view subspace clustering approaches, which can make use of the complementary information among different views to improve the performance of clustering, have attracted much attention. In this paper, we propose a novel multi-view subspace clustering method with Kronecker-basis-representation-based tensor sparsity measure (MSC-KBR) to address multi-view subspace clustering problem. In the MSC-KBR model, we first construct a tensor based on the subspace representation matrices of different views, and, then the high-order correlations underlying different views can be explored. We also adopt a novel Kronecker-basis-representation-based tensor sparsity measure (KBR) to the constructed tensor to reduce the redundancy of the learned subspace representations and improve the accuracy of clustering. Different from the traditional unfolding-based tensor norm, KBR can encode both sparsity insights delivered by Tucker and CANDECOMP/PARAFAC decompositions for a general tensor. By using the augmented Lagrangian method, an efficient algorithm is presented to solve the optimization problem of the MSC-KBR model. The experimental results on some datasets show that the proposed MSC-KBR model outperforms many state-of-the-art multi-view clustering approaches.",Multi-view features; Subspace clustering; Tucker decomposition; CANDECOMP; PARAFAC (CP) decomposition
"To a great extent, immersion of a virtual environment (VE) depends on the naturalness of the interface it provides for interaction. As people commonly exploit gestures during communication, therefore interaction based on hand-postures enhances the degree of realism of a VE. However, the choice of selecting hand postures for interaction varies from person to person. Generalizing the use of a specific posture with a particular interaction requires considerable computation which in turns depletes intuition of a 3D interface. By investigating machine learning in the domain of virtual reality (VR), this paper presents an open posture-based approach for 3D interaction. The technique is user-independent and relies neither on the size and color of hand nor on the distance between camera and posing-position. The system works in two phases-in the first phase, hand-postures are learnt, whereas in the second phase the known postures are used to perform interaction. With an ordinary camera, a scanned image is partitioned into equal size non-overlapping tiles. Four light-weight features, based on binary histogram and invariant moments, are calculated for each part and portion of a posture-image. The support vector machine classifier is trained by posture-specific knowledge carried accumulatively in each tile. By posing any known posture, the system extracts the tiles information to detect a particular hand-posture. At the successful recognition, appropriate interaction is activated in the designed VE. The proposed system is implemented in a case-study application; vision-based open posture interaction using the libraries of OpenCV and OpenGL. The system is assessed in three separate evaluation sessions. Results of the evaluations testify efficacy of the approach in various VR applications.",Virtual environments; Hand tracking; Gesture-based interactions; Computer vision
"The speed and accuracy of phenotype detection from medical images are some of the most important qualities needed for any informed and timely response such as early detection of cancer or detection of desirable phenotypes for animal breeding. To improve both these qualities, the world is leveraging artificial intelligence and machine learning against this challenge. Most recently, deep learning has successfully been applied to the medical field to improve detection accuracies and speed for conditions including cancer and COVID-19. In this study, we applied deep neural networks, in the form of a generative adversarial network (GAN), to perform image-to-image processing steps needed for ovine phenotype analysis from CT scans of sheep. Key phenotypes such as gigot geometry and tissue distribution were determined using a computer vision (CV) pipeline. The results of the image processing using a trained GAN are strikingly similar (a similarity index of 98%) when used on unseen test images. The combined GAN-CV pipeline was able to process and determine the phenotypes at a speed of 0.11 s per medical image compared to approximately 30 min for manual processing. We hope this pipeline represents the first step towards automated phenotype extraction for ovine genetic breeding programmes.",generative adversarial network; machine learning; automated medical image processing; deep neural network; animal science; CT scans; computer vision
"The raindrops on the glass will affect driving safety, such as rear-view camera, outside mirror and windshield, etc. This article proposed a robust raindrop detection using deep learning on embedded platform with AI accelerator for real-time implementation. A training model is established through a convolution neural network (CNN)-like architecture to classify the images by the vehicle camera into three classes: no rain, heavy rain, and light rain. The classification results are used to control the speed of the motor to implement an automatic wiper control system. The training model, ResNet, is used to classify the image with good tradeoff between the computational cost and accuracy. For real-time application, the camera module on the Google Coral Dev board on embedded system platform is used to test the video stream and to estimate the performance of this system. Results show that the recognition accuracy reaches 95%, and the processing speed can achieve 20 frames per second (fps) on the embedded system.",Rain; Computer architecture; Deep learning; Computational modeling; Training data; Neural networks; Cameras; Raindrop detection; artificial intelligence; deep learning; convolution neural network; ADAS; computer vision; vehicle safety; ResNet; embedded system; AI accelerator
"Many large-scale and complex structural components are applied in the aeronautics and automobile industries. However, the repeated alternating or cyclic loads in service tend to cause unexpected fatigue fractures. Therefore, developing real-time and visible monitoring methods for fatigue crack initiation and propagation is critically important for structural safety. This paper proposes a machine learning-based fatigue crack growth detection method that combines computer vision and machine learning. In our model, computer vision is used for data creation, and the machine learning model is used for crack detection. Then computer vision is used for marking and analyzing the crack growth path and length. We apply seven models for the crack classification and find that the decision tree is the best model in this research. The experimental results prove the effectiveness of our method, and the crack length measurement accuracy achieved is 0.6 mm. Furthermore, the slight machine learning models help us realize real-time and visible fatigue crack detection.",Fatigue crack; Growth prediction; Mechanoresponsive luminogen; Structural health monitoring; Computer vision; Machine learning
"Featured Application: Results of the work are applied to the design of the fully automatic computer vision system for online asbestos fiber content (productivity) estimation in veins of rock chunks in an open pit.<br>The paper discusses the results of the research and development of an innovative deep learning-based computer vision system for the fully automatic asbestos content (productivity) estimation in rock chunk (stone) veins in an open pit and within the time comparable with the work of specialists (about 10 min per one open pit processing place). The discussed system is based on the applying of instance and semantic segmentation of artificial neural networks. The Mask R-CNN-based network architecture is applied to the asbestos-containing rock chunks searching images of an open pit. The U-Net-based network architecture is applied to the segmentation of asbestos veins in the images of selected rock chunks. The designed system allows an automatic search and takes images of the asbestos rocks in an open pit in the near-infrared range (NIR) and processes the obtained images. The result of the system work is the average asbestos content (productivity) estimation for each controlled open pit. It is validated to estimate asbestos content as the graduated average ratio of the vein area value to the selected rock chunk area value, both determined by the trained neural network. For both neural network training tasks the training, validation, and test datasets are collected. The designed system demonstrates an error of about 0.4% under different weather conditions in an open pit when the asbestos content is about 1.5-4%. The obtained accuracy is sufficient to use the system as a geological service tool instead of currently applied visual-based estimations.",deep learning; computer vision systems; asbestos content control; object detection; semantic segmentation
"Indian herbal plants are used in agriculture and in the food, cosmetics, and pharmaceutical industries. Laboratory-based tests are routinely used to identify and classify similar herb species by analyzing their internal cell structures. In this paper, we have applied computer vision techniques to do the same. The original leaf image was preprocessed using the ChanVese active contour segmentation algorithm to efface the background from the image by setting the contraction bias as (v)-1 and smoothing factor (mu) as 0.5, and bringing the initial contour close to the image boundary. Thereafter the segmented grayscale image was fed to a leaky capacitance fired neuron model (LCFN), which differentiates between similar herbs by combining different groups of pixels in the leaf image. The LFCN's decay constant (f), decay constant (g) and threshold (h) parameters were empirically assigned as 0.7, 0.6 and h=18 to generate the 1D feature vector. The LCFN time sequence identified the internal leaf structure at different iterations. Our proposed framework was tested against newly collected herbal species of natural images, geometrically variant images in terms of size, orientation and position. The 1D sequence and shape features of aloe, betel, Indian borage, bittergourd, grape, insulin herb, guava, mango, nilavembu, nithiyakalyani, sweet basil and pomegranate were fed into the 5-fold Bayesian regularization neural network (BRNN), K-nearest neighbors (KNN), support vector machine (SVM), and ensemble classifier to obtain the highest classification accuracy of 91.19%.",Chan-Vese segmentation; Leaky Capacitance and Fired Neuron (LCFN); time sequence; Bayesian Regularization Neural Network (BRNN); computer vision
"BACKGROUND Pests cause significant damage to agricultural crops and reduce crop yields. Use of manual methods of pest forecasting for integrated pest management is labor-intensive and time-consuming. Here, we present an automatic system for monitoring pests in large fields, with the aim of replacing manual forecasting. The system comprises an automatic detection and counting system and a human-computer data statistical fitting system. Image data sets of the target pests from large fields are first input into the system. The number of pests in the image is then counted both manually and using the automatic system. Finally, a mapping relationship between counts obtained using the automated system and by agricultural experts is established using the statistical fitting system. RESULTS Trends in the pest-count curves produced using the manual and automated counting methods were very similar. To sample the number of pests for manual statistics, plants were shaken to transfer the pests from the plant to a plate. Hence, pests hiding within plant crevices were also sampled and included in the count, whereas the automatic method counted only the pests visible in the images. Therefore, the computer index threshold was much lower than the manual index threshold. However, the proposed system correctly reflected trends in pest numbers obtained using computer vision. CONCLUSION The experimental results demonstrate that our automatic pest-monitoring system can generate pest grades and can replace manual forecasting methods in large fields.",pest counting; pest control; computer vision; deep learning; integrated pest management
"Material characterization has been proved to be the most intuitive approach to understand the chemical composition, structure, and microstructure of materials, which is the basis of material design. One of the most important steps in material design is to extract the characteristics from an image, and find their associations with the material structure and properties. Therefore, in recent years, with the rapid development of machine vision algorithms, characterization images have attracted attention in the field of material characterization. Researchers use computer vision algorithms, such as image denoising and enhancement, to preprocess the representation image, image segmentation and classification to detect and separate each microstructure from the characterization image, and quantitatively analyze the properties of materials. Herein, the application of computer vision algorithms in material image representation is summarized and discussed. The latest and valuable views for experts and scholars in both computer vision and material grounds are presented. Thus, this review provides guidance for material exploration and promotes the developments of artificial intelligence in the field of materials.",artificial intelligence; computer vision; guide imaging; machine learning; material characterization; microscopy
"The rapid development of computer vision has led to an increasing amount of 3D data, such as multiple views and point clouds, which are widely used in 3D object recognition and retrieval. Intuitively, the quality of 3D data is the most crucial factor that directly affects the performance of 3D applications. However, how to evaluate the 3D data quality, especially the multi-view data quality, is still an open question. To tackle this issue, we propose an entropy-based multi-view information quantification model (MV-Info model) to quantitatively evaluate the multi-view data information. Our proposed MV-Info model consists of hierarchical data module, feature generation module, and quantitative calculation module. Besides, it considers the information entropy theory for more reasonable quantification results. In our method, how much information we can observe from a group of views can be quantified, which can be used to support 3D recognition and retrieval. We also designed a series of experiments to evaluate the effectiveness of the proposed model. The experimental results demonstrate the rationality and validity of the proposed model.",information quantification; multi-view data; 3D object; computer vision
"Visual Question Answering (VQA) is a multi-disciplinary research problem that has captured the attention of both computer vision as well as natural language processing researchers. In Visual Question Answering, a system is given an image; a question in a natural language related to that image as an input, and the VQA system is required to give an answer in natural language as an output. A VQA algorithm may require common sense reasoning over the information contained in the image and world knowledge to produce the right answer. In this paper, we have discussed some of the core concepts used in VQA systems and present a comprehensive survey of efforts in the past to address this problem. Apart from traditional VQA models, we have also discussed visual question answering models that require reading texts present in images and evaluated on recently developed datasets like TextVQA, ST-VQA, and OCR-VQA. Apart from standard datasets discussed in previous surveys, we have also discussed some new datasets developed in 2019 and 2020 such as GQA, OK-VQA, TextVQA, ST-VQA, and OCR-VQA. The new evaluation metrics such as BLEU, MPT, METEOR, Average Normalized Levenshtein Similarity (ANLS), Validity, Plausibility, Distribution, Consistency, Grounding, F1-Score are explained together with the evaluation metrics discussed by previous surveys. We conclude our survey with a discussion on open issues in each phase of the VQA task and present some promising future directions. (c) 2021 Elsevier B.V. All rights reserved.",Computer vision; Natural language processing; Deep neural networks; World knowledge; Attention
"The mouse is one of the wonderful inventions of Human-Computer Interaction (HCI) technology. Currently, wireless mouse or a Bluetooth mouse still uses devices and is not free of devices completely since it uses a battery for power and a dongle to connect it to the PC. In the proposed AI virtual mouse system, this limitation can be overcome by employing webcam or a built-in camera for capturing of hand gestures and hand tip detection using computer vision. The algorithm used in the system makes use of the machine learning algorithm. Based on the hand gestures, the computer can be controlled virtually and can perform left click, right click, scrolling functions, and computer cursor function without the use of the physical mouse. The algorithm is based on deep learning for detecting the hands. Hence, the proposed system will avoid COVID-19 spread by eliminating the human intervention and dependency of devices to control the computer.",
"Transmission electron microscopy (TEM) has a multitude of uses in biomedical imaging due to its ability to discern ultrastructure morphology at the nanometer scale. Through its ability to directly visualize virus particles, TEM has for several decades been an invaluable tool in the virologist's toolbox. As applied to HIV-1 research, TEM is critical to evaluate activities of inhibitors that block the maturation and morphogenesis steps of the virus lifecycle. However, both the preparation and analysis of TEM micrographs requires time consuming manual labor. Through the dedicated use of computer vision frameworks and machine learning techniques, we have developed a convolutional neural network backbone of a two-stage Region Based Convolutional Neural Network (RCNN) capable of identifying, segmenting and classifying HIV-1 virions at different stages of maturation and morphogenesis. Our results outperformed common RCNN backbones, achieving 80.0% mean Average Precision on a diverse set of micrographs comprising different experimental samples and magnifications. We expect that this tool will be of interest to a broad range of researchers. (C) 2021 The Author(s). Published by Elsevier B.V. on behalf of Research Network of Computational and Structural Biotechnology.",Quantitative biology; Artificial intelligence; Deep learning; Electron microscopy; HIV-1; Virology; Computer vision
"The advancement of technology and the accomplishments of Industry 4.0 have expanded the possibilities for research in the vision system of the industrial inspection sector, which are intelligent, widely connected, fully adaptable, autonomous, and fully accessible based on the IoT environment. This advancement precisely bridges the gap between machine vision and object dimension measurements. The main goal is to develop the system of the 3SMVI in the milling machine based on the camera system and lighting system to ensure the quality of the product. This study proposes the development of an automated smart system-based interpreter STEP-NC information for a machine vision inspection (3SMVI) for detecting and measuring the surface feature of Example 1 Part 21 of ISO 14649 standard. This presents a 3SMVI development architecture that will guide the advancement with 3SMVI of the current Intelitek proLIGHT CNC milling machine. A standardised architectural 3SMVI platform is created. On that basis, machine tools, physical processes, real database range, operating units, and intelligent computer technologies are interconnected through a broad range of networks, including Wi-Fi and wireless connections. Machine vision inspection system (MVIS) is suggested as the digital model of the cyber domain physical tool. The OpenCV library developed a system platform that becomes cloud connectivity between the Raspberry Pi 4 board and the USB microprocessor on camera range. As a result, the machine vision inspection system is operated based on an algorithm designed for automatic measuring on a standard model. The primary outcome is developing a system prototype capable of inspecting the surface feature of a workpiece in real-time operation. Nonetheless, connectivity between machines can be carried out through Cyber Digital Twins (CDT). The obtained MVIS and prototype structure is appropriate for online surface measurement, and the system implementation has been designed and realised for automated measurement.",Inspection; STEP-NC; Machine vision; Cyber Digital Twins; Measurement; IoT; OpenCV
"Image segmentation is an important issue in many industrial processes, with high potential to enhance the manufacturing process derived from raw material imaging. For example, metal phases contained in microstructures yield information on the physical properties of the steel. Existing prior literature has been devoted to develop specific computer vision techniques able to tackle a single problem involving a particular type of metallographic image. However, the field lacks a comprehensive tutorial on the different types of techniques, methodologies, their generalizations and the algorithms that can be applied in each scenario. This paper aims to fill this gap. First, the typologies of computer vision techniques to perform the segmentation of metallographic images are reviewed and categorized in a taxonomy. Second, the potential utilization of pixel similarity is discussed by introducing novel deep learning-based ensemble techniques that exploit this information. Third, a thorough comparison of the reviewed techniques is carried out in two openly available real-world datasets, one of them being a newly published dataset directly provided by ArcelorMittal, which opens up the discussion on the strengths and weaknesses of each technique and the appropriate application framework for each one. Finally, the open challenges in the topic are discussed, aiming to provide guidance in future research to cover the existing gaps.",Image segmentation; Metallography; Machine learning; Deep learning; Microscopy images; Computer vision
"Representation learning for video is increasingly gaining attention in the field of computer vision. For instance, video prediction models enable activity and scene forecasting or vision-based planning and control. In this article, we investigate the combination of differentiable physics and spatial transformers in a deep action conditional video representation network. By this combination our model learns a physically interpretable latent representation and can identify physical parameters. We propose supervised and self-supervised learning methods for our architecture. In experiments, we consider simulated scenarios with pushing, sliding and colliding objects, for which we also analyze the observability of the physical properties. We demonstrate that our network can learn to encode images and identify physical properties like mass and friction from videos and action sequences. We evaluate the accuracy of our training methods, and demonstrate the ability of our method to predict future video frames from input images and actions.",Physical scene understanding; Video representation learning; Differentiable physics
"Human vision is able to compensate imperfections in sensory inputs from the real world by reasoning based on prior knowledge about the world. Deep learning has had a significant impact on computer vision due to its inherent ability in handling imprecision, but the absence of a reasoning framework based on domain knowledge limits its ability to interpret complex scenarios. We propose semi-lexical languages as a formal basis for reasoning with imperfect tokens provided by the real world. The power of deep learning is used to map the imperfect tokens into the alphabet of the language, and symbolic reasoning is used to determine the membership of input in the language. Semi-lexical languages have bindings that prevent the variations in which a semi-lexical token is interpreted in different parts of the input, thereby leaning on deduction to enhance the quality of recognition of individual tokens. We present case studies that demonstrate the advantage of using such a framework over pure deep learning and pure symbolic methods. (c) 2021 Elsevier B.V. All rights reserved.",Neuro-symbolic deduction; Semantic interpretation; Explainable inference
"To improve the accuracy of computer vision-based bridge crack-width identification, two factors were investigated in this study. First, a fully convolutional neural network combined with a U-Net architecture was used to extract crack pixels and a crack midline (i.e., crack skeleton). A database including 100 images with 572 x 572 pixels labelled for cracks was developed. The results revealed a mean of 84.4% of the average mean intersection over union of the U-Net. In addition, a crack-width direction identification method based on the slope of the crack skeleton was proposed, and the accurate extraction of the nonuniform width parameter along the crack was realised. Second, to obtain a mapping from the width pixels to width physical dimensions, a pixel calibration experiment was conducted. Finally, a nonlinear regression model of the distance, focal length, and actual pixel size was developed to overcome the light distortion of the camera. Ultimately, the combination of the two factors completed a high-precision extraction of the entire process of crack-width identification and achieved a 0.01-mm precision under a 96% guarantee rate.",Crack identification; crack width; fully convolutional neural network; U-Net; bridge inspection; computer vision-based
"The ongoing need to sustainably manage fishery resources can benefit from fishery-independent monitoring of fish stocks. Camera systems, particularly baited remote underwater video system (BRUVS), are a widely used and repeatable method for monitoring relative abundance, required for building stock assessment models. The potential for BRUVS-based monitoring is restricted, however, by the substantial costs of manual data extraction from videos. Computer vision, in particular deep learning (DL) models, are increasingly being used to automatically detect and count fish at low abundances in videos. One of the advantages of BRUVS is that bait attractants help to reliably detect species in relatively short deployments (e.g., 1 h). The high abundances of fish attracted to BRUVS, however, make computer vision more difficult, because fish often obscure other fish. We build upon existing DL methods for identifying and counting a target fisheries species across a wide range of fish abundances. Using BRUVS imagery targeting a recovering fishery species, Australasian snapper (Chrysophrys auratus), we tested combinations of three further mathematical steps likely to generate accurate, efficient automation: (1) varying confidence thresholds (CTs), (2) on/off use of sequential non-maximum suppression (Seq-NMS), and (3) statistical correction equations. Output from the DL model was more accurate at low abundances of snapper than at higher abundances (>15 fish per frame) where the model over-predicted counts by as much as 50%. The procedure providing the most accurate counts across all fish abundances, with counts either correct or within 1-2 of manual counts (R-2 = 88%), used Seq-NMS, a 45% CT, and a cubic polynomial corrective equation. The optimised modelling provides an automated procedure offering an effective and efficient method for accurately identifying and counting snapper in the BRUV footage on which it was tested. Additional evaluation will be required to test and refine the procedure so that automated counts of snapper are accurate in the survey region over time, and to determine the applicability to other regions within the distributional range of this species. For monitoring stocks of fishery species more generally, the specific equations will differ but the procedure demonstrated here could help to increase the usefulness of BRUVS.",automated fish identification; automated marine monitoring; computer vision; deep learning; object detection; stock assessment; relative abundance
"Human-computer interaction (HCI) and computer vision (CV) provide interesting communication features between machines and humans in different real-time applications. Visualization helps to improve the accuracy of detecting target communication objects for better interaction. This paper introduces an integrated detection-based interaction scheme (IDIS) for improving the accuracy and reliability of HCI systems. The input is fetched from the ranged object, and the interaction session is initiated after the classification and detection of the object. The process of robust, longterm interaction with the detected object is achieved through pre-classification. In this detection process, deep learning is used to identify the object and perform its intended interaction requirements. The recurrent process is used to identify the variations in interaction patterns and time. The allocation of interaction sessions is streamlined to improve the interaction span and detection accuracy. The proposed scheme's performance is verified using dataset sources for the following metrics: accuracy, delay, error, and interaction span.",Computer vision; Deep learning; HCI; Object detection; Pre-classification
"Humans can readily detect when an image does not belong in a set by comparing semantic information between images to derive meaning and relationships from the colors, shapes, sizes, locations, and textures within them. Current self-supervised anomaly detection algorithms in computer vision do not possess this ability. Most algorithms learn to detect anomalies through a training objective that only optimizes for machine level features and do not evaluate semantic features. To fill this gap, we propose a novel self-supervised algorithm that detects anomalies by learning and modeling semantic information within a set of images. This is accomplished by first training our algorithm to be sensitive or invariant to targeted semantic information, and then modeling the semantic relationships learned so that we can detect anomalies by measuring how far images deviate from the model. Experimenting with our algorithm, we show that different datasets can have different semantic sensitivities, those sensitivities can fluctuate between and within sets depending on different aspects of the images, and directly targeting those sensitivities can improve anomaly detection performance. Using our algorithm, we were able to achieve a AUROC of 0.7146 on the CIFAR-10 dataset, which is an improvement of 0.0741 over another current leading self-supervised anomaly detection algorithm.",Anomaly detection; Self-supervised learning; Representation learning; Remote sensing; Computer vision; Land use classification; Pretext learning; Feature extraction; Dimensionality reduction; Multivariate statistics
"Out-of-home audience measurement aims to count and characterize the people exposed to advertising content in the physical world. While audience measurement solutions based on computer vision are of increasing interest, no commonly accepted benchmark exists to evaluate and compare their performance. In this paper, we propose the first benchmark for digital out-of-home audience measurement that evaluates the vision-based tasks of audience localization and counting, and audience demographics. The benchmark is composed of a novel, dataset captured at multiple locations and a set of performance measures. Using the benchmark, we present an in-depth comparison of eight open-source algorithms on four hardware platforms with GPU and CPU-optimized inferences and of two commercial off-the-shelf solutions for localization, count, age, and gender estimation. This benchmark and related open-source codes are available at .",Anonymous video analytics; Benchmark; Audience measurement; Performance evaluation
"A large crack detection dataset of 2446 manually labeled images is established to cover a wide range of noise and to evaluate the performance of end-to-end deep convolutional networks in detecting cracking. Five state-of-the-art end-to-end deep computer vision architectures for semantic segmentation are trained and evaluated, including Fully Convolutional Network (FCN), Global Convolutional Network (GCN), Pyramid Scene Parsing Network (PSPNet), UPerNet, and DeepLabv3+. For the backbones, the VGG, ResNet, and DenseNet are adopted. Based on the comparison of test set metrics, DeepLabv3+ with the ResNet101 backbone achieved the highest IoU of 0.6298, the highest recall of 0.6834, and the highest F1 score of 0.7732. The influence of database choice and image noise on crack detection performance is reported. Based on the comparison of predicted images, UperNet with ResNet101 backbone shows the highest performance for images with shadings, while DeepLabv3+ with ResNet101 backbone shows the best performance for images with blemishes. The research outcome can provide reference for the application of fast and accurate detection of cracks in civil engineering.",computer vision; convolutional neural network; crack; deep learning; encoder-decoder; semantic segmentation
"Underwater image enhancement (UIE), as an image processing technique, plays a vital role in computer vision. However, existing approaches treat the restoration process as a whole; thus, they cannot adequately handle the color distortion and low contrast in the enhanced images. In this paper, we propose a global-local-guided model for realizing UIE tasks in a coarse-to-fine manner to alleviate these issues. The proposed model is divided into two paths. The global path targets to estimate basic structure and color information, while the local path targets to remove the undesirable artifacts, e.g., noises over-exposure regions, and blurred edges. By integrating two neural networks into our model, we could recover the underwater images with clear textural details and vivid color. Besides, a learning-based weight map is introduced to make the global-local path on friendly terms, which can balance the pixel intensity distribution from both sides and remove redundant information to a certain degree. Qualitative and quantitative experimental results on various benchmarks demonstrate that our method can effectively tackle color distortion and blurred edges compared with several state-of-the-art methods by a large margin. Finally, we also conduct experiments to demonstrate that our method can be applied in various computer vision tasks, e.g., object detection, matching and edge detection.",Underwater image enhancement; Image processing; Computer vision; Deep learning
"Artificial neural networks are efficient learning algorithms that are considered to be universal approximators for solving numerous real-world problems in areas such as computer vision, language processing, or reinforcement learning. To approximate any given function, neural networks train a large number of parameters-up to millions, or even billions in some cases. The large number of parameters and hidden layers in neural networks make them hard to interpret, which is why they are often referred to as black boxes. In the quest to make artificial neural networks interpretable in the field of computer vision, feature visualization stands out as one of the most developed and promising research directions. While feature visualizations are a valuable tool to gain insights about the underlying function learned by the network, they are still considered to be simple visual aids requiring human interpretation. In this paper, we propose that feature visualizations-class visualizations in particular-are analogous to mental imagery in humans, resembling the experience of seeing or perceiving the actual training data. Therefore, we propose that class visualizations contain embedded knowledge that can be exploited in a more automated manner. We present a series of experiments that shed light on the nature of class visualizations and demonstrate that class visualizations can be considered a conceptual compression of the data used to train the underlying model. Finally, we show that class visualizations can be regarded as convolutional filters and experimentally show their potential for extreme model compression purposes.",artificial neural networks; dataset compression; feature visualizations; knowledge distillation; mental imagery; model compression; visual imagery
"Computer vision is becoming an increasingly trendy word in the area of image processing. With the emergence of computer vision applications, there is a significant demand to recognize objects automatically. Deep CNN (convolution neural network) has benefited the computer vision community by producing excellent results in video processing, object recognition, picture classification and segmentation, natural language processing, speech recognition, and many other fields. Furthermore, the introduction of large amounts of data and readily available hardware has opened new avenues for CNN study. Several inspirational concepts for the progress of CNN have been investigated, including alternative activation functions, regularization, parameter optimization, and architectural advances. Furthermore, achieving innovations in architecture results in a tremendous enhancement in the capacity of the deep CNN. Significant emphasis has been given to leveraging channel and spatial information, with a depth of architecture and information processing via multi-path. This survey paper focuses mainly on the primary taxonomy and newly released deep CNN architectures, and it divides numerous recent developments in CNN architectures into eight groups. Spatial exploitation, multi-path, depth, breadth, dimension, channel boosting, feature-map exploitation, and attention-based CNN are the eight categories. The main contribution of this manuscript is in comparing various architectural evolutions in CNN by its architectural change, strengths, and weaknesses. Besides, it also includes an explanation of the CNN's components, the strengths and weaknesses of various CNN variants, research gap or open challenges, CNN applications, and the future research direction.",CNN; feature-map exploitation; attention-based CNN; deep CNN; object recognition; computer vision
"Chronic Low Back Pain (LBP) is a symptom that may be caused by several diseases, and it is currently the leading cause of disability worldwide. The increased amount of digital images in orthopaedics has led to the development of methods related to artificial intelligence, and to computer vision in particular, which aim to improve diagnosis and treatment of LBP. In this manuscript, we have systematically reviewed the available literature on the use of computer vision in the diagnosis and treatment of LBP. A systematic research of PubMed electronic database was performed. The search strategy was set as the combinations of the following keywords: Artificial Intelligence , Feature Extraction , Segmentation , Computer Vision , Machine Learning , Deep Learning , Neural Network , Low Back Pain , Lumbar . Results: The search returned a total of 558 articles. After careful evaluation of the abstracts, 358 were excluded, whereas 124 papers were excluded after full-text examination, taking the number of eligible articles to 76. The main applications of computer vision in LBP include feature extraction and segmentation, which are usually followed by further tasks. Most recent methods use deep learning models rather than digital image processing techniques. The best performing methods for segmentation of vertebrae, intervertebral discs, spinal canal and lumbar muscles achieve Sorensen-Dice scores greater than 90%, whereas studies focusing on localization and identification of structures collectively showed an accuracy greater than 80%. Future advances in artificial intelligence are expected to increase systems' autonomy and reliability, thus providing even more effective tools for the diagnosis and treatment of LBP.</p>",low back pain; orthopaedics; artificial intelligence; computer vision; digital image processing; deep learning; decision support systems; computer aided diagnosis
"Motorcycles are Vulnerable Road Users (VRU) and as such, in addition to bicycles and pedestrians, they are the traffic actors most affected by accidents in urban areas. Automatic video processing for urban surveillance cameras has the potential to effectively detect and track these road users. The present review focuses on algorithms used for detection and tracking of motorcycles, using the surveillance infrastructure provided by CCTV cameras. Given the importance of results achieved by Deep Learning theory in the field of computer vision, the use of such techniques for detection and tracking of motorcycles is also reviewed. The paper ends by describing the performance measures generally used, publicly available datasets (introducing the Urban Motorbike Dataset (UMD) with quantitative evaluation results for different detectors), discussing the challenges ahead and presenting a set of conclusions with proposed future work in this evolving area.",Motorcycles; Feature extraction; Head; Safety; Roads; Cameras; Shape; Vulnerable road users (VRU); motorcycle detection; vehicle detection; tracking; convolutional neural networks (CNNs); deep learning; computer vision
"Lung cancer is the leading cause of cancer death and morbidity worldwide. Many studies have shown machine learning models to be effective in detecting lung nodules from chest X-ray images. However, these techniques have yet to be embraced by the medical community due to several practical, ethical, and regulatory constraints stemming from the black-box  nature of deep learning models. Additionally, most lung nodules visible on chest X-rays are benign; therefore, the narrow task of computer vision-based lung nodule detection cannot be equated to automated lung cancer detection. Addressing both concerns, this study introduces a novel hybrid deep learning and decision tree-based computer vision model, which presents lung cancer malignancy predictions as interpretable decision trees. The deep learning component of this process is trained using a large publicly available dataset on pathological biomarkers associated with lung cancer. These models are then used to inference biomarker scores for chest X-ray images from two independent data sets, for which malignancy metadata is available. Next, multi-variate predictive models were mined by fitting shallow decision trees to the malignancy stratified datasets and interrogating a range of metrics to determine the best model. The best decision tree model achieved sensitivity and specificity of 86.7% and 80.0%, respectively, with a positive predictive value of 92.9%. Decision trees mined using this method may be considered as a starting point for refinement into clinically useful multi-variate lung cancer malignancy models for implementation as a workflow augmentation tool to improve the efficiency of human radiologists.",lung cancer; chest X-ray; malignancy predictive models; artificial intelligence; machine learning; computer vision; model mining
"The advancement and popularity of computer games make game scene analysis one of the most interesting research topics in the computer vision society. Among the various computer vision techniques, we employ object detection algorithms for the analysis, since they can both recognize and localize objects in a scene. However, applying the existing object detection algorithms for analyzing game scenes does not guarantee a desired performance, since the algorithms are trained using datasets collected from the real world. In order to achieve a desired performance for analyzing game scenes, we built a dataset by collecting game scenes and retrained the object detection algorithms pre-trained with the datasets from the real world. We selected five object detection algorithms, namely YOLOv3, Faster R-CNN, SSD, FPN and EfficientDet, and eight games from various game genres including first-person shooting, role-playing, sports, and driving. PascalVOC and MS COCO were employed for the pre-training of the object detection algorithms. We proved the improvement in the performance that comes from our strategy in two aspects: recognition and localization. The improvement in recognition performance was measured using mean average precision (mAP) and the improvement in localization using intersection over union (IoU).",object detection; game scene; deep learning; YOLO; SSD; R-CNN; EfficientDet
"Visual Question Answering (VQA) is a multimodal research related to Computer Vision (CV) and Natural Language Processing (NLP). How to better ob-tain useful information from images and questions and give an accurate answer to the question is the core of the VQA task. This paper presents a VQA model based on multimodal encoders and decoders with gate attention (MEDGA). Each encoder and decoder block in the MEDGA applies not only self-attention and cross -modal attention but also gate attention, so that the new model can better focus on inter-modal and intra-modal interactions simultaneously within visual and language modality. Besides, MEDGA further filters out noise information irrelevant to the re-sults via gate attention and finally outputs attention results that are closely related to visual features and language features, which makes the answer prediction result more accurate. Experimental evaluations on the VQA 2.0 dataset and the ablation experiments under different conditions prove the effectiveness of MEDGA. In ad-dition, the MEDGA accuracy on the test-std dataset has reached 70.11%, which exceeds many existing methods.",Deep Learning; Artificial Intelligence; Visual Question Answering; Gate Attention; Multimodal Learning
"We introduce a technique to synthetically increase the framerate of semi-repetitive videos (i.e., videos of motion that repeats but not in an identical fashion) to aid in visualization. By reordering and combining frames from all repetitions, we produce a single non-repetitive sequence with much higher temporal resolution. Then, we use a novel frame warping technique based on a dense corrective flow to counteract differences between repetitions. The resulting video maintains smoothness of motion and additionally allows for seamless, infinite looping. We demonstrate the effectiveness of the proposed solution both quantitatively, by measuring the improvement over existing methods, and qualitatively, by performing a user evaluation and providing several examples in the article and accompanying video.",Interpolation; Adaptive optics; Optical imaging; Pipelines; Cameras; Video sequences; Indexes; Computer graphics; picture; image generation; image-based rendering; image processing and computer vision; image processing software; time-varying imagery
"Artificial neural networks have become the go-to solution for computer vision tasks, including problems of the security domain. One such example comes in the form of reidentification, where deep learning can be part of the surveillance pipeline. The use case necessitates considering an adversarial setting-and neural networks have been shown to be vulnerable to a range of attacks. In this paper, the preprocessing defences against adversarial attacks are evaluated, including block-matching convolutional neural network for image denoising used as an adversarial defence. The benefit of using preprocessing defences comes from the fact that it does not require the effort of retraining the classifier, which, in computer vision problems, is a computationally heavy task. The defences are tested in a real-life-like scenario of using a pre-trained, widely available neural network architecture adapted to a specific task with the use of transfer learning. Multiple preprocessing pipelines are tested and the results are promising.",deep learning; computer vision; adversarial attacks; adversarial defences
"Person Re-Identification is an essential task in computer vision, particularly in surveillance applications. The aim is to identify a person based on an input image from surveillance photographs in various scenarios. Most Person re-ID techniques utilize Convolutional Neural Networks (CNNs); however, Vision Transformers are replacing pure CNNs for various computer vision tasks such as object recognition, classification, etc. The vision transformers contain information about local regions of the image. The current techniques take this advantage to improve the accuracy of the tasks underhand. We propose to use the vision transformers in conjunction with vanilla CNN models to investigate the true strength of transformers in person re-identification. We employ three backbones with different combinations of vision transformers on two benchmark datasets. The overall performance of the backbones increased, showing the importance of vision transformers. We provide ablation studies and show the importance of various components of the vision transformers in re-identification tasks.",vision transformers; deep learning; re-ID; image retrieval; multi-camera surveillance system; pedestrian identification; person identification
"Image captioning is a challenging task of computer vision and natural language processing. The big challenge lies in obtaining semantic information from images and translating that into the human language using machines. The interaction of computer vision and natural language processing further increases the complexity of image captioning. Notably, research has been carried out in image captioning to narrow down the semantic gap using deep learning techniques effectively. Deep learning techniques are proficient in dealing with the complexities of image captioning. A detailed study is carried out to identify the various state-of-the-art techniques for image captioning. The working algorithm of technique, positive highlights, and weakness of every technique is discussed in this paper. We also discussed the quantitative evaluation measures used for deep learning techniques and available datasets.",Image Captioning; CNN; RNN; Attention Mechanism; GAN
"Deep reinforcement learning augments the reinforcement learning framework and utilizes the powerful representation of deep neural networks. Recent works have demonstrated the remarkable successes of deep reinforcement learning in various domains including finance, medicine, healthcare, video games, robotics, and computer vision. In this work, we provide a detailed review of recent and state-of-the-art research advances of deep reinforcement learning in computer vision. We start with comprehending the theories of deep learning, reinforcement learning, and deep reinforcement learning. We then propose a categorization of deep reinforcement learning methodologies and discuss their advantages and limitations. In particular, we divide deep reinforcement learning into seven main categories according to their applications in computer vision, i.e. (i) landmark localization (ii) object detection; (iii) object tracking; (iv) registration on both 2D image and 3D image volumetric data (v) image segmentation; (vi) videos analysis; and (vii) other applications. Each of these categories is further analyzed with reinforcement learning techniques, network design, and performance. Moreover, we provide a comprehensive analysis of the existing publicly available datasets and examine source code availability. Finally, we present some open issues and discuss future research directions on deep reinforcement learning in computer vision.",Deep learning; Reinforcement learning; Deep reinforcement learning; Computer vision; Autonomous landmark detection; Object detection; Object tracking; Image registration; Image segmentation; Video analysis
"The applications of computer vision (CV) are continuously increasing along with the enormous demand for real-time data processing. This visual data processing is done with various compute-intensive image/video processing algorithms that may belong to traditional approaches or deep learning approaches. This article aims to provide a survey of state-of-the-art hardware platforms and software frameworks for parallel implementation of traditional CV applications. The article discusses various options for hardware platforms for centralized-computing architecture and edge-computing architecture, and various software frameworks that can be used to leverage the hardware. This discussion is based on a systematic survey of studies/works that show the use of various hardware platforms and software frameworks in order to achieve real-time processing for CV algorithms. Based on the survey, some possible future directions are also discussed.",discrete GPU; edge computing; integrated GPU; review; traditional computer vision
"Facial target detection is an important task in computer vision. Because heterogeneous face detection shows broad prospects, it has attracted extensive attention from the academic community. In recent years, with the rise of deep learning and its applications in computer vision, face detection technology has made great strides. This paper uses multi-task cascaded convolutional neural network (MTCNN) for heterogeneous face feature detection. This algorithm makes full use of the advantages of image pyramid, boundary regression, fully convolutional attention networks and non-maximum suppression. The main idea of this paper is to use candidate frame plus classifier for fast and efficient face detection. Specifically, the candidate window is generated by the proposal network (P-Net), and the high-precision candidate window is filtered and selected by the reduced network (R-Net), and the final bounding box and facial key points are generated by the output network (O-Net). In order to prove the effectiveness of this method in visible light, near-infrared and sketch face recognition scenes, it was verified in the datasets of CUFS, CUFSF and CASIA NIR-VIS 2.0. Experiments show that this method is effective for face images in heterogeneous face and is better than the latest algorithms.",
"Visual Question Answering (VQA) is an extremely stimulating and challenging research area where Computer Vision (CV) and Natural Language Processig (NLP) have recently met. In image captioning and video summarization, the semantic information is completely contained in still images or video dynamics, and it has only to be mined and expressed in a human-consistent way. Differently from this, in VQA semantic information in the same media must be compared with the semantics implied by a question expressed in natural language, doubling the artificial intelligence-related effort. Some recent surveys about VQA approaches have focused on methods underlying either the image-related processing or the verbal related one, or on the way to consistently fuse the conveyed information. Possible applications are only suggested, and, in fact, most cited works rely on general-purpose datasets that are used to assess the building blocks of a VQA system. This paper rather considers the proposals that focus on real-world applications, possibly using as benchmarks suitable data bound to the application domain. The paper also reports about some recent challenges in VQA research. (c) 2021 Published by Elsevier B.V.",Visual question answering; Real-world VQA; VQA for medical applicatons; VQA for assistive applications; VQA for context awareness; VQA in cultural heritage and education
"At present times, COVID-19 has become a global illness and infected people has increased exponentially and it is difficult to control due to the non-availability of large quantity of testing kits. Artificial intelligence (AI) techniques including machine learning (ML), deep learning (DL), and computer vision (CV) approaches find useful for the recognition, analysis, and prediction of COVID-19. Several ML and DL techniques are trained to resolve the supervised learning issue. At the same time, the potential measure of the unsupervised learning technique is quite high. Therefore, unsupervised learning techniques can be designed in the existing DL models for proficient COVID-19 prediction. In this view, this paper introduces a novel unsupervised DL based variational autoencoder (UDL-VAE) model for COVID-19 detection and classification. The UDL-VAE model involved adaptive Wiener filtering (AWF) based preprocessing technique to enhance the image quality. Besides, Inception v4 with Adagrad technique is employed as a feature extractor and unsupervised VAE model is applied for the classification process. In order to verify the superior diagnostic performance of the UDL-VAE model, a set of experimentation was carried out to highlight the effective outcome of the UDL-VAE model. The obtained experimental values showcased the effectual results of the UDL-VAE model with the higher accuracy of 0.987 and 0.992 on the binary and multiple classes respectively. (c) 2021 Elsevier B.V. All rights reserved.",COVID-19; Deep learning; Unsupervised learning; Variational autoencoder; Image classification
"Computational medicine is an emerging discipline that uses computer models and complex software to simulate the development and treatment of diseases. Advances in computer hardware and software technology, especially the development of algorithms and graphics processing units (GPUs), have led to the broader application of computers in the medical field. Computer vision based on mathematical biological modelling will revolutionize clinical research and diagnosis, and promote the innovative development of Chinese medicine, some biological models have begun to play a practical role in various types of research. This paper introduces the concepts and characteristics of computational medicine and then reviews the developmental history of the field, including Digital Human in Chinese medicine. Additionally, this study introduces research progress in computational medicine around the world, lists some specific clinical applications of computational medicine, discusses the key problems and limitations of the research and the development and application of computational medicine, and ultimately looks forward to the developmental prospects, especially in the field of computational Chinese medicine.",computational medicine; model; computational Chinese medicine; artificial intelligence; big data
"Semantic segmentation is one of the most fundamental problems in computer vision with significant impact on a wide variety of applications. Adversarial learning is shown to be an effective approach for improving semantic segmentation quality by enforcing higher-level pixel correlations and structural information. However, state-of-the-art semantic segmentation models cannot be easily plugged into an adversarial setting because they are not designed to accommodate convergence and stability issues in adversarial networks. We bridge this gap by building a conditional adversarial network with a state-of-the-art segmentation model (DeepLabv3+) at its core. To battle the stability issues, we introduce a novel lookahead adversarial learning (LoAd) approach with an embedded label map aggregation module. We focus on semantic segmentation models that run fast at inference for near real-time field applications. Through extensive experimentation, we demonstrate that the proposed solution can alleviate divergence issues in an adversarial semantic segmentation setting and results in considerable performance improvements (+5% in some classes) on the baseline for three standard datasets.",Semantic segmentation; Conditional adversarial training; Computer vision; Deep learning
"Computer vision (CV) technologies are assisting the health care industry in many respects, i.e., disease diagnosis. However, as a pivotal procedure before and after surgery, the inventory work of surgical instruments has not been researched with the CV-powered technologies. To reduce the risk and hazard of surgical tools' loss, we propose a study of systematic surgical instrument classification and introduce a novel attention-based deep neural network called SKA-ResNet which is mainly composed of: (a) A feature extractor with selective kernel attention module to automatically adjust the receptive fields of neurons and enhance the learnt expression and (b) A multi-scale regularizer with KL-divergence as the constraint to exploit the relationships between feature maps. Our method is easily trained end-to-end in only one stage with few additional calculation burdens. Moreover, to facilitate our study, we create a new surgical instrument dataset called SID19 (with 19 kinds of surgical tools consisting of 3800 images) for the first time. Experimental results show the superiority of SKA-ResNet for the classification of surgical tools on SID19 when compared with state-of-the-art models. The classification accuracy of our method reaches up to 97.703%, which is well supportive for the inventory and recognition study of surgical tools. Also, our method can achieve state-of-the-art performance on four challenging fine-grained visual classification datasets.",Health care; Deep learning; Fine-grained classification; Attention mechanism
"Human action analysis has been an active research area in computer vision, and has many useful applications such as human computer interaction. Most of the state-of-the-art approaches of human action analysis are data driven and focus on general action recognition. In this paper, we aim to analyze fitness actions with skeleton sequences and propose an efficient and robust fitness action analysis framework. Firstly, fitness actions from 15 subjects are captured and built to a fitness action dataset (Fitness-28). Secondly, skeleton information is extracted and made alignment with a simplified human skeleton model. Thirdly, the aligned skeleton information is transformed to an uniform human center coordinate system with the proposed spatial-temporal skeleton encoding method. Finally, the action classifier and local-global geometrical registration strategy are constructed to analyze the fitness actions. Experimental results demonstrate that our method can effectively assess fitness action, and have a good performance on artificial intelligence fitness system.",Computer vision; Action assessment; Image processing; Action recognition; Intelligent sports; Performance analysis
"With the continuous innovation of network technology, various kinds of convenient network technologies have grown, and human dependence on network technology has gradually increased, which has resulted in the importance of network information security issues. With the continuous development of my country's industrialization, the application of sensors is becoming more and more extensive, for example, the security vulnerabilities and defects in the operating system itself. Traditional sensors can perceive a certain thing or signal, convert it into an electrical signal and record it, and then use a conversion circuit to output the electrical signal into a value or other display form that is conducive to observation. Nowadays, sensors have been further developed. Based on the original perception function, combined with computer technology, it integrates data storage, data processing, data communication, and other functions, so that it has analysis functions and can better display information. The technical level has reached a new level. Early intelligent recognition mainly used the uniqueness of finger and palm lines to scan and contrast, but due to some weather reasons or skin texture constraints caused by skin texture, these methods showed certain limitations. This paper proposes a new computer vision-based algorithm from face detection technology and face recognition technology. In the face detection technology, it is mainly introduced from the OpenCV method. Face recognition technology is improved in practical applications through the Seetaface method and YouTu method. At the same time, using the contrast experiment, the detection and recognition rates under the three different requirements of side face detection, occlusion detection, and facial exaggerated expression are compared, and the accuracy of each method is improved. The results show that each case is compared in each case. The advantages and disadvantages of the algorithm effectively verify the effectiveness of the method.",
"Steel pipes are widely used in high-risk and high-pressure scenarios such as oil, chemical, natural gas, shale gas, etc. If there is some defect in steel pipes, it will lead to serious adverse consequences. Applying object detection in the field of deep learning to pipe weld defect detection and identification can effectively improve inspection efficiency and promote the development of industrial automation. Most predecessors used traditional computer vision methods applied to detect defects of steel pipe weld seams. However, traditional computer vision methods rely on prior knowledge and can only detect defects with a single feature, so it is difficult to complete the task of multi-defect classification, while deep learning is end-to-end. In this paper, the state-of-the-art single-stage object detection algorithm YOLOv5 is proposed to be applied to the field of steel pipe weld defect detection and compared with the two-stage representative object detection algorithm Faster R-CNN. The experimental results show that applying YOLOv5 to steel pipe weld defect detection can greatly improve the accuracy, complete the multi-classification task, and meet the criteria of real-time detection.",deep learning; object detection; YOLOv5; X-ray non-destructive testing; weld defect
"Visual damage inspection of steel frames by eyes alone is time-consuming and cumbersome; therefore, it produces inconsistent results. Existing computer vision-based methods for inspecting civil structures using deep learning algorithms have not reached full maturity in exactly locating the damage. This paper presents a deep convolutional neural network-based damage locating (DCNN-DL) method that classifies the steel frame images provided as inputs as damaged and undamaged. DenseNet, a DCNN architecture, was trained to classify the damage. The DenseNet output was upscaled and superimposed on the original image to locate the damaged part of the steel frame. The DCNN-DL method was validated using 144 training and 114 validation sets of steel frame images. DenseNet, with an accuracy of 99.3%, outperformed MobileNet and ResNet with accuracies of 96.2% and 95.4%, respectively. This case study confirms that the DCNN-DL method effectively facilitates the real-time inspection and location of steel frame damage.",Steel frame damage; Deep learning; Computer vision; Deep convolutional neural network; Steel structure monitoring
"Both Visual Question Answering (VQA) and image captioning are the problems which involve Computer Vision (CV) and Natural Language Processing (NLP) domains. In general, computer vision models are effectively utilized to represent visual contents. While NLP algorithms are used to represent the sentences. In recent years, VQA and image captioning tasks are tackled independently although they require similar type of algorithms. In this paper, a joint relationship between these two tasks is established and exploited. We present an image captioning based VQA model that uses the knowledge learnt from the image captioning task and transfers that knowledge to VQA task. We integrate the image captioning module into the VQA model by fusing the features obtained from captioning model and the attention-based visual feature. The experimental results demonstrate the improvement in the answer generation accuracy by a margin 3.45 % on VQA 1.0, 3.33% on VQA 2.0 and 1.73% on VQA-CP v2 datasets over the state-of-the-art VQA models.",Visual question answering (VQA); Image captioning; Computer vision (CV); Natural language processing (NLP)
"In order to improve the video image processing technology, this paper presents a moving object detection and tracking algorithm based on computer vision technology. Firstly, the detection performance of the interframe difference method and the background difference model method is compared comprehensively from both theoretical and experimental aspects, and then the Robert edge detection operator is selected to carry out edge detection of the vehicle. The research results show that the algorithm proposed in this paper has the longest running time per frame when tracking a moving target, which is about 2.3 times that of the single frame running time of the CamShift algorithm. The algorithm has high running efficiency and can meet the requirements of real-time tracking of a foreground target. The algorithm has the highest tracking accuracy, the time consumption is reduced, and the error of the tracking frame deviating from the real position of the target is the least.",
"During the phase of periodic asphalt pavement survey, patched and unpatched potholes need to be accurately detected. This study proposes and verifies a computer vision-based approach for automatically distinguishing patched and unpatched potholes. Using two-dimensional images, patched and unpatched potholes may have similar shapes. Therefore, this study relies on image texture descriptors to delineate these two objects of interest. The texture descriptors of statistical measurement of color channels, the gray-level cooccurrence matrix, and the local ternary pattern are used to extract texture information from image samples of asphalt pavement roads. To construct a classification model based on the extracted texture-based dataset, this study proposes and validates an integration of the Support Vector Machine Classification (SVC) and the Forensic-Based Investigation (FBI) metaheuristic. The SVC is used to generalize a classification boundary that separates the input data into two class labels of patched and unpatched potholes. To optimize the SVC performance, the FBI algorithm is utilized to fine-tune the SVC hyperparameters. To establish the hybrid FBI-SVC framework, an image dataset consisting of 600 samples has been collected. The experiment supported by the Wilcoxon signed-rank test demonstrates that the proposed computer vision is highly suitable for the task of interest with a classification accuracy rate = 94.833%.",
"In the past years, deep neural networks (DNN) have become popular in many disciplines such as computer vision (CV), natural language processing (NLP), etc. The evolution of hardware has helped researchers to develop many powerful Deep Learning (DL) models to face numerous challenging problems. One of the most important challenges in the CV area is Medical Image Analysis in which DL models process medical images-such as magnetic resonance imaging (MRI), X-ray, computed tomography (CT), etc.-using convolutional neural networks (CNN) for diagnosis or detection of several diseases. The proper function of these models can significantly upgrade the health systems. However, recent studies have shown that CNN models are vulnerable under adversarial attacks with imperceptible perturbations. In this paper, we summarize existing methods for adversarial attacks, detections and defenses on medical imaging. Finally, we show that many attacks, which are undetectable by the human eye, can degrade the performance of the models, significantly. Nevertheless, some effective defense and attack detection methods keep the models safe to an extent. We end with a discussion on the current state-of-the-art and future challenges.",deep learning; adversarial attack; medical image analysis; computer vision; convolutional neural networks
"Recently, deep neural networks have achieved state-of-the-art performance in multiple computer vision tasks, and become core parts of computer vision applications. In most of their implementations, a standard input preprocessing component called image scaling is embedded, in order to resize the original data to match the input size of pre-trained neural networks. This article demonstrates content disguising attacks by exploiting the image scaling procedure, which cause machine's extracted content to be dramatically dissimilar with that before scaled. Different fromprevious adversarial attacks, our attacks happen in the data preprocessing stage, and hence they are not subject to specific machine learning models. To achieve a better deceiving and disguising effect, we propose and implement three feasible attack approaches with L-0-, L-2- and L-infinity-norm distance metrics. Wehave conducted a comprehensive evaluation on various image classification applications, including three local demos and two remote proprietary services. Wealso investigate the attack effects on a YOLO-v3 object detection demo. Our experimental results demonstrate successful content disguising against all of them, which validate our approaches are practical.",Content disguising; image scaling; adversarial examples; deep learning; computer vision
"Deep learning is a class of machine learning methods that has been successful in computer vision. Unlike traditional machine learning methods that require hand-engineered feature extraction from input images, deep learning methods learn the image features by which to classify data. Convolutional neural networks (CNNs), the core of deep learning methods for imaging, are multilayered artificial neural networks with weighted connections between neurons that are iteratively adjusted through repeated exposure to training data. These networks have numerous applications in radiology, particularly in image classification, object detection, semantic segmentation, and instance segmentation. The authors provide an update on a recent primer on deep learning for radiologists, and they review terminology, data requirements, and recent trends in the design of CNNs; illustrate building blocks and architectures adapted to computer vision tasks, including generative architectures; and discuss training and validation, performance metrics, visualization, and future directions. Familiarity with the key concepts described will help radiologists understand advances of deep learning in medical imaging and facilitate clinical adoption of these techniques. (C) RSNA, 2021",
"The Portuguese population is aging at an increasing rate, which introduces new problems, particularly in rural areas, where the population is small and widely spread throughout the territory. These people, mostly elderly, have low income and are often isolated and socially excluded. This work researches and proposes an affordable Ambient Assisted Living (AAL)-based solution to monitor the activities of elderly individuals, inside their homes, in a pervasive and non-intrusive way, while preserving their privacy. The solution uses a set of low-cost IoT sensor devices, computer vision algorithms and reasoning rules, to acquire data and recognize the activities performed by a subject inside a home. A conceptual architecture and a functional prototype were developed, the prototype being successfully tested in an environment similar to a real case scenario. The system and the underlying concept can be used as a building block for remote and distributed elderly care services, in which the elderly live autonomously in their homes, but have the attention of a caregiver when needed.",computer vision; image analysis; internet of things; monitoring of elderly; low cost
"Due to the great achievements in artificial intelligence, it is predicted that autonomous vehicles with little or even no human involvement will come to market in the near future. Autonomous vehicles are equipped with multiple types of sensors. An autonomous vehicle relies on its sensors to perceive its environment, and this sensory information plays a key role in the vehicle's driving decisions. Hence, ensuring the trustworthiness of the sensor data is crucial for drivers' safety. In this article, we discuss the impact of perception error attacks (PEAs) on autonomous vehicles, and propose a countermeasure called LIFE (LIDAR and Image data Fusion for detecting perception Errors). LIFE detects PEAs by analyzing the consistency between camera image data and LIDAR data using novel machine learning and computer vision algorithms. The performance of LIFE has been evaluated extensively using the KITTI dataset.",Laser radar; Sensors; Cameras; Autonomous vehicles; Sensor fusion; Feature extraction; Sensor phenomena and characterization; Autonomous vehicle; machine learning; computer vision; stereo camera; LIDAR; perception error attack
"Featured Application The fundamental research is relevant to several applications involving dynamic conditions in terms of of incoming image's lightning and overall visibility of content present in it. Some of the applications include surveillance, student authentication in online learning environments, autonomous robotics. Image classification of a visual scene based on visibility is significant due to the rise in readily available automated solutions. Currently, there are only two known spectrums of image visibility i.e., dark, and bright. However, normal environments include semi-dark scenarios. Hence, visual extremes that will lead to the accurate extraction of image features should be duly discarded. Fundamentally speaking there are two broad methods to perform visual scene-based image classification, i.e., machine learning (ML) methods and computer vision methods. In ML, the issues of insufficient data, sophisticated hardware and inadequate image classifier training time remain significant problems to be handled. These techniques fail to classify the visual scene-based images with high accuracy. The other alternative is computer vision (CV) methods, which also have major issues. CV methods do provide some basic procedures which may assist in such classification but, to the best of our knowledge, no CV algorithm exists to perform such classification, i.e., these do not account for semi-dark images in the first place. Moreover, these methods do not provide a well-defined protocol to calculate images' content visibility and thereby classify images. One of the key algorithms for calculation of images' content visibility is backed by the HSL (hue, saturation, lightness) color model. The HSL color model allows the visibility calculation of a scene by calculating the lightness/luminance of a single pixel. Recognizing the high potential of the HSL color model, we propose a novel framework relying on the simple approach of the statistical manipulation of an entire image's pixel intensities, represented by HSL color model. The proposed algorithm, namely, Relative Perceived Luminance Classification (RPLC) uses the HSL (hue, saturation, lightness) color model to correctly identify the luminosity values of the entire image. Our findings prove that the proposed method yields high classification accuracy (over 78%) with a small error rate. We show that the computational complexity of RPLC is much less than that of the state-of-the-art ML algorithms.",Relative Perceived Luminance Classification (RPLC); color model; luminosity; bright; dark; semi-dark
"Neural networks have enabled state-of-the-art approaches to achieve incredible results on computer vision tasks such as object detection. However, previous works have tried to improve the performance in various object detection necks but have failed to extract features efficiently. To solve the insufficient features of objects, this work introduces some of the most advanced and representative network models based on the Faster R-CNN architecture, such as Libra R-CNN, Grid R-CNN, guided anchoring, and GRoIE. We observed the performance of Neighbour Feature Pyramid Network (NFPN) fusion, ResNet Region of Interest Feature Extraction (ResRoIE) and the Recursive Feature Pyramid (RFP) architecture at different scales of precision when these components were used in place of the corresponding original members in various networks obtained on the MS COCO dataset. Compared to the experimental results after replacing the neck and RoIE parts of these models with our Reinforced Neighbour Feature Fusion (RNFF) model, the average precision (AP) is increased by 3.2 percentage points concerning the performance of the baseline network.",computer vision; object detection; feature extraction; region of interest; feature pyramid network
"Computer vision and deep neural networks have been significantly promoting the development of visual perception in these years. Particularly, for autonomous vehicles, real-time image/video data is captured by onboard cameras and analyzed by computer vision techniques in many real applications. In the captured camera data, some contents can be used as auxiliary information to infer individuals' locations and trajectories, which leads to severe privacy leakage but has been rarely studied. Thus, the goal of this article is to protect individuals' location privacy by hiding side-channel information in the captured data while preserving the data utility for downstream applications. To this end, the technology of generative adversarial networks (GAN) is utilized to design two novel models, named ADGAN-I and ADGAN-II, both of which can take the original camera data as inputs and generate privacy-preserving outputs according to predefined sensitive object class. Thus, the processed camera data can defend location inference attack from adversaries in offline applications. Moreover, in ADGAN-I and ADGAN-II, the tradeoff between location privacy and data utility can be effectively balanced. Finally, the results of extensive real-data experiments validate the superiority of our proposed models over the state of the arts in utility preservation and privacy protection for autonomous vehicles' images and videos.",Data privacy; Cameras; Autonomous vehicles; Privacy; Faces; Data models; Visualization; Autonomous vehicles; computer vision; generative adversarial networks; location privacy
"Neural architecture search (NAS) has achieved unprecedented performance in various computer vision tasks. However, most existing NAS methods are defected in search efficiency and model generalizability. In this paper, we propose a novel NAS framework, termed MIGO-NAS, with the aim to guarantee the efficiency and generalizability in arbitrary search spaces. On the one hand, we formulate the search space as a multivariate probabilistic distribution, which is then optimized by a novel multivariate information-geometric optimization (MIGO). By approximating the distribution with a sampling, training, and testing pipeline, MIGO guarantees the memory efficiency, training efficiency, and search flexibility. Besides, MIGO is the first time to decrease the estimation error of natural gradient in multivariate distribution. On the other hand, for a set of specific constraints, the neural architectures are generated by a novel dynamic programming network generation (DPNG), which significantly reduces the training cost under various hardware environments. Experiments validate the advantages of our approach over existing methods by establishing a superior accuracy and efficiency i.e., 2.39 test error on CIFAR-10 benchmark and 21.7 on ImageNet benchmark, with only 1.5 GPU hours and 96 GPU hours for searching, respectively. Besides, the searched architectures can be well generalize to computer vision tasks including object detection and semantic segmentation, i.e., 25xFLOPs compression, with 6.4 mAP gain over Pascal VOC dataset, and 29:9xFLOPs compression, with only 1.41 percent performance drop over Cityscapes dataset. The code is publicly available.",Neural architecture search; multivariate information-geometric optimization; dynamic programming
"Hand gesture recognition is a popular topic in computer vision and makes human-computer interaction more flexible and convenient. The representation of hand gestures is critical for recognition. In this paper, we propose a new method to measure the similarity between hand gestures and exploit it for hand gesture recognition. The depth maps of hand gestures captured via the Kinect sensors are used in our method, where the 3D hand shapes can be segmented from the cluttered backgrounds. To extract the pattern of salient 3D shape features, we propose a new descriptor-3D Shape Context, for 3D hand gesture representation. The 3D Shape Context information of each 3D point is obtained in multiple scales because both local shape context and global shape distribution are necessary for recognition. The description of all the 3D points constructs the hand gesture representation, and hand gesture recognition is explored via dynamic time warping algorithm. Extensive experiments are conducted on multiple benchmark datasets. The experimental results verify that the proposed method is robust to noise, articulated variations, and rigid transformations. Our method outperforms state-of-the-art methods in the comparisons of accuracy and efficiency.",3D shape context; depth map; hand shape segmentation; hand gesture recognition; human-computer interaction
"In pattern recognition, object recognition is an important research domain due to major applications such as autonomous driving, robotics, and visual surveillance. Many computer vision techniques are introduced in the literature. Several challenges exist, such as similar shapes of different objects and imbalanced datasets. They also face irrelevant feature extraction, which degrades the recognition accuracy and increases the computational time. In this article, we proposed a fully automated computer vision pipeline for object recognition. In the proposed method, initially perform the data augmentation to balance the object classes. In the later step, a convolutional neural network (DenseNet201) was considered and modified according to the selected dataset (Caltech101). The modified model is trained by transfer learning and extracts features. The extracted features include a few redundant information removed using an improved whale optimization algorithm (WOA). Final features are classified using several supervised learning algorithms for final recognition. The experimental process was carried out using the augmented Caltech101 dataset and accomplished an accuracy of 93%. Comparison with the benchmark methods illustrated that the implanted accuracy is considerably improved.",Deep Learning; Features Optimization; Features Classification; Object recognition; Transfer Learning
"This paper explores the cloud- versus server-based deployment scenarios of an enhanced computer vision platform for potential deployment on low-resolution 511 traffic video streams. An existing computer vision algorithm based on a spatial-temporal map and designed for high-angle traffic video like that of NGSIM (Next Generation SIMulation) is enhanced for roadside CCTV traffic camera angles. Because of the lower visual angle, determining the directions, splitting vehicles from occlusions, and identifying lane changes become difficult. A motion-flow-based direction determination method, a bisection occlusion detection and splitting algorithm, and a lane-change tracking method are proposed. The model evaluation is conducted by using videos from multiple cameras from the New Jersey Department of Transportation's 511 traffic video surveillance system. The results show promising performance in both accuracy and computational efficiency for potential large-scale cloud deployment. The cost analysis reveals that at the current pricing model of cloud computing, the cloud-based deployment is more convenient and cost-effective for an on-demand network assessment. In contrast, the dedicated-server-based deployment is more economical for long-term traffic detection deployment.",
"Medicinal plants are used to cure different common and chronic diseases in different Asian countries including India. The easy availability and planting possibility make them popular resources for alternative medicinal practices like Ayurveda. These medicinal plants possess proven healing potential without causing any side effects. The biochemical constituents of the medicinal leaves are the fundamental reasons of their healing power which considerably vary with maturity. The existing practices of maturity detection are largely based on chemical analysis of leaves through different instruments which are expensive, invasive in nature and time consuming. This paper reports a computer vision-based system to classify the medicinal leaves along with the corresponding maturity level. The process reported is advantageous in terms of comparatively easier, faster, less expensive and non-invasive operations. The presented system captures the leaf images in controlled illumination and processed leaf images are fed to the convolutional neural network (CNN) architecture-based system for classification of both the type of leaves and maturity stage. The paper also presents application of binary particle swarm optimization ((BPSO) for arriving the values of the CNN hyperparameters. The potential of the system has been assessed with three popular species of medicinal plants, namely Neem, Tulsi and Kalmegh. The classification results were verified with repeated test runs and different standard metrics including tenfold cross-validation method. The paper shows that the presented CNN-driven computer vision framework can provide about 99% classification accuracy for simultaneous prediction of leaf specie and maturity stage. Such significant performance of the presented method can be considered as a potential addition to the existing chemical and instrumental methods.",Medicinal plants; Computer vision; Color features; Binary particle swarm optimization; Deep neural network; Convolutional neural network
"Inferring human pose from a monocular RGB image remains an interesting field of research in computer vision. It serves as a fundamental key for many real-world applications, including human-computer interaction, anima-tion, and detecting abnormal or illegal human behavior. Despite the considerable progress made in this area dur-ing the last decade, the proposed methods face serious problems due to the huge variations in human appearance, occlusions, noisy backgrounds, viewpoints, and other factors that can change the context of the cap-tured information. In this paper, we introduce a survey of state-of-the-art methods to highlight various research that have been proposed to tackle the 2D and 3D pose estimation tasks. Based on the number of persons in the image, two main pipelines are identified: single-person and multi-person methods. Each of these categories is di-vided into two groups according to the proposed architectures. Also, we provide a brief description of current datasets and the different metrics applied to evaluate the methods performances. Finally, we include a discussion about the advantages and disadvantages of the mentioned strategies. (c) 2021 Elsevier B.V. All rights reserved.",2D and 3D human pose estimation; Deep learning; CNN; Computer vision; Single-person and multi-person pose estimation
"Availability is one of the three main goals of information security. This paper contributes to systems' availability by introducing an optimization model for the adaptation (controlling the capturing, coding, and sending features of the video communication system) of live broadcasting of video to limited and varied network bandwidth and/or limited power sources such as wireless and mobile network cases. We first, analyzed the bitrate-accuracy and bitrate-power characteristics of various video transmission techniques for adapting video communication in Artificial Intelligence-based Systems. To optimize resources for live video streaming, we analyze various video parameter settings for adapting the stream to available resources. We consider the object detection accuracy, the bandwidth, and power consumption requirement. The results showed that setting SNR and spatial video encoding features (with upscaling the frames at the destination) are the best techniques that maximizing the object detection accuracy while minimizing the bandwidth and the consumed energy requirements. In addition, we analyze the effectiveness of combining SNR and spatial video encoding features with upscaling and find that we can increase the performance of the streaming system by combining these two techniques. We presented a multi-objective function for determining the parameter or parameters' pairing that provides the optimal object detection's accuracy, power consumption, and bit rate. Results are reported based on more than 15,000 experiments utilizing standard datasets for short video segments and a collected dataset of 300 videos from YouTube. We evaluated results based on the detection index, false-positive index, power consumption, and bandwidth requirements metrics. For a single adaptive parameter, the analysis of the experiment's outcome demonstrate that the multi-objective function achieves object detection accuracy as high as the best while drastically reducing bandwidth requirements and energy consumption. For multiple adaptive parameters, the analysis of the experiment's outcome demonstrate the significant benefits of effective pairings (pairs) of adaptive parameters. For example, by combining the signal-to-noise ratio (SNR) with the spatial feature in H.264, a certain optimal parameter setting can be reached where the power consumption can be reduced to 20%, and the bandwidth requirements to 2% from the original, while keeping the Object Detection Accuracy (ODA) within 10% less of the highest ODA.",Video communications; Optimization; Computer vision systems; Power consumption; Rate-energy-accuracy trade-offs; Video stream adaptation; Artificial intelligence-based systems; Availability of video communication; Multi-objective function; AI-based computer vision systems
"3D Image representation is an important topic in computer vision and pattern recognition. Recently, 3D image analysis by fractional-order orthogonal moments has provided a new research direction, which has prompted researchers to think about efficient and fast classification. In this paper, the authors derived novel sets of fractional-order Legendre moment invariants (FrLMIs), for 3D object description and recognition. Therefore, an analysis of the performance of reconstruction and classification based on fractional-order moment invariants and Deep Neural Networks (DNNs) by changing the number of descriptors was presented. Accordingly, the performance of these proposed fractional-order moments and moment invariants are evaluated through several appropriate experiments, including 3D image reconstruction, region of interest feature extraction, invariance with respect to the geometric transformations and noisy, and 3D Object classification using different fractional parameters. The superiority of the proposed method is verified by comparing the classification percentages obtained by varying the amount of data used during the training process in comparison with the existing methods. The work presented will help to create new neural network architectures that take advantage of the descriptive capacity of 3D fractional-order moments. Finally, these fractional-order moments are very fast and computationally inexpensive which could be useful in many computer vision applications. Based on these characteristics, the proposed FrOLMs and FrLMIs outperformed all existing orthogonal moments.",Fractional-order orthogonal polynomials; Fractional-order moment invariants; 3D image analysis; Global and local features extraction; 3D objects recognition; Deep neural networks
"Multiple-object tracking is a fundamental computer vision task which is gaining increasing attention due to its academic and commercial potential. Multiple-object detection, recognition and tracking are quite desired in many domains and applications. However, accurate object tracking is very challenging, and things are even more challenging when multiple objects are involved. The main challenges that multiple-object tracking is facing include the similarity and the high density of detected objects, while also occlusions and viewpoint changes can occur as the objects move. In this article, we introduce a real-time multiple-object tracking framework that is based on a modified version of the Deep SORT algorithm. The modification concerns the process of the initialization of the objects, and its rationale is to consider an object as tracked if it is detected in a set of previous frames. The modified Deep SORT is coupled with YOLO detection methods, and a concrete and multi-dimensional analysis of the performance of the framework is performed in the context of real-time multiple tracking of vehicles and pedestrians in various traffic videos from datasets and various real-world footage. The results are quite interesting and highlight that our framework has very good performance and that the improvements on Deep SORT algorithm are functional. Lastly, we show improved detection and execution performance by custom training YOLO on the UA-DETRAC dataset and provide a new vehicle dataset consisting of 7 scenes, 11.025 frames and 25.193 bounding boxes.",Computer vision; Multiple-object tracking; Deep learning; Deep SORT; YOLO
"Convolutional neural networks (CNNs) have proven to be very successful in learning task specific computer vision features. To integrate features from different layers in standard CNNs, we present a fusing framework of shortcut convolutional neural networks (S-CNNs). This framework can fuse arbitrary scale features by adding weighted shortcut connections to the standard CNNs. Besides the framework, we propose a shortcut indicator (SI) of binary string to stand for a specific S-CNN shortcut style. Additionally, we design a learning algorithm for the proposed S-CNNs. Comprehensive experiments are conducted to compare its performances with standard CNNs on multiple benchmark datasets for different visual tasks. Empirical results show that if we choose an appropriate fusing style of shortcut connections with learnable weights, S-CNNs can perform better than standard CNNs regarding accuracy and stability in different activation functions and pooling schemes initializations, and occlusions. Moreover, S-CNNs are competitive with ResNets and can outperform GoogLeNet, DenseNets, Multi-scale CNN, and DeepID. (c) 2021 Elsevier Inc. All rights reserved.",Convolutional neural networks; Computer vision; Shortcut connections
"Egocentric videos can bring a lot of information about how humans perceive the world and interact with the environment, which can be beneficial for the analysis of human behaviour. The research in egocentric video analysis is developing rapidly thanks to the increasing availability of wearable devices and the opportunities offered by new large-scale egocentric datasets. As computer vision techniques continue to develop at an increasing pace, the tasks related to the prediction of future are starting to evolve from the need of understanding the present. Predicting future human activities, trajectories and interactions with objects is crucial in applications such as human-robot interaction, assistive wearable technologies for both industrial and daily living scenarios, entertainment and virtual or augmented reality. This survey summarizes the evolution of studies in the context of future prediction from egocentric vision making an overview of applications, devices, existing problems, commonly used datasets, models and input modalities. Our analysis highlights that methods for future prediction from egocentric vision can have a significant impact in a range of applications and that further research efforts should be devoted to the standardization of tasks and the proposal of datasets considering real-world scenarios such as the ones with an industrial vocation.",First person vision; Egocentric vision; Future prediction; Anticipation
"In order to solve the problem of crop disease detection in large-scale planting, a new crop disease detection algorithm based on multi-feature decision fusion is proposed. This paper proposes a multi-feature decision fusion disease discrimination algorithm (PD R-CNN) based on machine vision on crop surfaces. The algorithm is based on the machine vision processing model of R-CNN and integrates a disease discrimination algorithm on the basis of R-CNN. After training on crop image data sets, PD R-CNN can reach the goal of identifying crop surface lesions. This paper uses machine vision image acquisition, image processing and analysis technology to collect and analyze the growth of cucumber seedlings. The research results show that compared with manual judgment, PD R-CNN reduces the workload and can effectively distinguish crop diseases. Through experiments, during the occurrence of pests and diseases, PD R-CNN has a monitoring accuracy of 88.0% for mosaic disease, 92.0% for root rot, 88.0% for powdery mildew, and 86.0% for aphids, indicating that there are errors in actual monitoring, but the accuracy exceeds 85.0% can be put into use.",Image recognition model; Machine vision; Multi-sign decision-making; Crop disease identification algorithm
"Pedestrian Attribute Recognition (PAR) is an important task in computer vision community and plays an important role in practical video surveillance. The goal of this paper is to review existing works using traditional methods or based on deep learning networks. Firstly, we introduce the background of pedestrian attribute recognition, including the fundamental concepts and formulation of pedestrian attributes and corresponding challenges. Secondly, we analyze popular solutions for this task from eight perspectives. Thirdly, we discuss the specific attribute recognition, then, give a comparison between deep learning and traditional algorithm based PAR methods. After that, we show the connections between PAR and other computer vision tasks. Fourthly, we introduce the benchmark datasets, evaluation metrics in this community, and give a brief performance comparison. Finally, we summarize this paper and give several possible research directions for PAR. The project page of this paper can be found at: https://sites.google.com/view/ahu-pedestrianattributes/ . (c) 2021 Elsevier Ltd. All rights reserved.",Pedestrian attribute recognition; Multi-label learning; Multi-task learning; Deep learning; CNN-RNN
"Omnidirectional cameras are capable of providing 360. field-of-view in a single shot. This comprehensive view makes them preferable for many computer vision applications. An omnidirectional view is generally represented as a panoramic image with equirectangular projection, which suffers from distortions. Thus, standard camera approaches should be mathematically modified to be used effectively with panoramic images. In this work, we built a semantic segmentation CNN model that handles distortions in panoramic images using equirectangular convolutions. The proposed model, we call it UNet-equiconv, outperforms an equivalent CNN model with standard convolutions. To the best of our knowledge, ours is the first work on the semantic segmentation of real outdoor panoramic images. Experiment results reveal that using a distortion-aware CNN with equirectangular convolution increases the semantic segmentation performance (4% increase in mIoU). We also released a pixel-level annotated outdoor panoramic image dataset which can be used for various computer vision applications such as autonomous driving and visual localization. Source code of the project and the dataset were made available at the project page (https:// github.com/ semihorhan/semseg-outdoor- pano).",Semantic segmentation; Panoramic images; Omnidirectional vision; Convolutional neural networks
"Artificial intelligence (AI) offers the potential for the development of e-textiles that give wearers a smart and intuitive experience. An emerging challenge in intelligent materials design is hand gesture recognition textiles. Most current research focuses on number gesture recognition via smart gloves, so there is a gap in research that studies contact-less number gesture recognition textiles via computer vision. Meanwhile, there is lack of exploration on the integration of illuminating function and number gesture recognition textiles to improve interactivity by real-time visualizing detection results. In this research, a novel interactive illuminating textile with a touch-less number gesture recognition function has been designed and fabricated by using an open-source AI model. It is used in sync with a polymeric optical fiber textile with illuminative features. The textile is color-changing, controlled by the system's mid-air interactive number gesture recognition capability and has a woven stripe pattern and a double-layer weave structure with open pockets to facilitate integration of the system's components. Also described here is a novel design process that permits textile design and intelligent technology to integrate seamlessly and in synchronization, so that design in effect mediates continuously between the physical textile and the intangible technology. Moreover, this design method serves as a reference for the integration of open-source intelligent hardware and software into e-textiles for enhancement of the intuitive function and value via economy of labor.",Intelligent textile design; hand gesture recognition; polymeric optical fiber; illumination
"Deep learning is nowadays at the forefront of artificial intelligence. More precisely, the use of convolutional neural networks has drastically improved the learning capabilities of computer vision applications, being able to directly consider raw data without any prior feature extraction. Advanced methods in the machine learning field, such as adaptive momentum algorithms or dropout regularization, have dramatically improved the convolutional neural networks predicting ability, outperforming that of conventional fully connected neural networks. This work summarizes, in an intended didactic way, the main aspects of these cutting-edge techniques from a medical imaging perspective.",Deep learning; Image processing; Medical imaging; Educational
"With the rapid development of science and technology, the manufacturing industry has to cope with increasingly stricter requirements in terms of the quality of processed products. To improve production flexibility and automation, computer vision is widely used in machining due to its safety, reliability, continuity, high accuracy, and real-time performance. In this study, a comprehensive review of positioning methods for workpieces in machining is presented from the perspective of computer vision technology. First, the key technologies in image acquisition are described in detail, and a analysis of different lighting modes is conducted. Second, image preprocessing is described by summarizing enhancement and image segmentation methods. Third, from the perspectives of accuracy and speed, feature extraction methods are compared and evaluated. Next, the existing applications of visual positioning technology in machining are discussed. Finally, the existing problems are summarized, and future research directions technology suggested.",Visual positioning; Positioning processing; Optical system; Image preprocessing; Feature extraction
"Fog in hyperspectral images severely limits the visibility of imaging scene and reduces the image contrast, which has a negative effect on the following image interpretation. Defogging methods aim at restoring a high-quality image from the degraded image. Currently, most dehazing methods mainly depend on the atmospheric scattering model in computer vision and multispectral image communities. However, when these approaches are directly used to remove the fog from HSIs, they cannot produce satisfactory defogging performance. To alleviate this issue, we develop a novel fog model to achieve fog removal from hyperspectral images. First, a fog density map is calculated by differentiating the averaged bands falling into visible and infrared spectral ranges. Then, haze abundance in different spectral bands is estimated based on the pixel reflectance between two selected pixels with different haze levels. Finally, the high-quality hyperspectral image is restored by solving the defogging model. Experiments performed on a new benchmark created by ourselves demonstrate that the proposed method obtains favorable dehazing performance in contrast to other approaches in computer vision and remote sensing fields.",Atmospheric modeling; Hyperspectral imaging; Scattering; Image restoration; Sensors; Histograms; Image color analysis; Fog intensity map; haze abundance; hyperspectral image; image defogging
"Diabetic retinopathy, initially symptomless medical condition of diabetes, is one of the significant reasons of vision impairment all over the world. The early detection and diagnosis can reduce the incidence of severe vision loss and improve the efficiency of treatment. Fundus imaging, a non-invasive diagnosis method, is initial and widely used mode for analysing diabetic retinopathy. However, the precision of fundus imaging-based diagnosis of retinal disease is vastly dependent on experience and knowledge of ophthalmologists. Computer-aided diagnostic systems designed for retinal fundus images aid quick diagnosis, offer an external viewpoint during decision making, and serves as an important way of assessing treatment response to retinal diseases. In this paper, firstly the issues faced by ophthalmologists in characterization of various landmark structures and retinal lesions related to diabetic retinopathy by ophthalmologists are stated. Secondly, a comprehensive review of the state-of-the-art methods on landmark structures detection and segmentation, retinal lesions segmentation and diabetic retinopathy screening methods with retinal fundus images is presented. A concise tabular summary of each section comparing various methods, related retinal image databases, performance parameters, advantages, and disadvantages of the published methods is also exhibited. Finally, the main findings with focus to current challenges and ways for further improvement with respect to research gaps are discussed and concluded.",
"Three-dimensional (3D) face reconstruction technology is a key issue in the field of computer vision and computer graphics. The image-based 3D face reconstruction method is the mainstream technology of 3D face reconstruction. This paper uses the 3D face reconstruction algorithm to analyze the 3D reconstruction of a static image face and the structure and realization of the computer standardized management system. In the process of high-resolution two-channel matching, combined with multilevel reconstruction technology, a better initial disparity map is obtained by comparison. Then, the improved disparity map optimization algorithm is used to optimize the initial value, so that the result of the cloud point is smoother on the model surface while maintaining good detail accuracy. The AdaBoost face detection algorithm based on embedded images is used to obtain the image representing the region, and grayscale conversion and normalization processing are performed. Finally, the Poisson surface reconstruction algorithm is used to reconstruct a 3D high-precision face model. The experimental results in this paper show that the AdaBoost face detection algorithm can effectively separate and express face features, reduce the feature dimensions to compactly represent the data and correctly classify expressions. The 3D face reconstruction technology based on static images can increase the recognition rate of a face by 24% and can also increase the texture mapping effect of the local area of the face by 13%.",Static images; Face 3D reconstruction; Computer standardization; Depth information recovery method; Elastic model method
"Point clouds is one of popular 3D representations in computer vision and computer graphics. However, due to the sparseness and non-uniformity, raw point cloud from scanning devices cannot applied to down-stream geometry analyzing tasks directly. In this paper, we propose an end-to-end point cloud up-sampling network to reconstruct the dense yet uniform-distributed point clouds. Firstly, we utilize the spatial relationship of local regions and capture point-wise features progressively. We then propose a novel network to aggregate those features from different levels. Finally, we design an up-sampling module which consists of multi-branch convolution units to generate the dense point clouds. We conduct sufficient experiments on currently available public benchmarks. Experimental results show that proposed method has achieved 0.103 and 0.010 performance on Hausdorff distance and Chamfer Distance on VisionAir dataset, in comparison with the baseline towards uniformity, proximity-to-surface and mesh reconstruction.",Point cloud up-sampling; Deep learning; Feature extraction
"Automatically detecting surface defects from images is an essential capability in manufacturing applications. Traditional image processing techniques are useful in solving a specific class of problems. However, these techniques do not handle noise, variations in lighting conditions, and backgrounds with complex textures. In recent times, deep learning has been widely explored for use in automation of defect detection. This survey article presents three different ways of classifying various efforts in literature for surface defect detection using deep learning techniques. These three ways are based on defect detection context, learning techniques, and defect localization and classification method respectively. This article also identifies future research directions based on the trends in the deep learning area.",deep learning; inspection; defect detection; image processing; machine learning; computer-aided manufacturing; automation; artificial intelligence
"Social image colocalization locates the objects belonging to the same category from a set of images. Its goal is to explore the consistent relationship among social entities in the social system, which is conducive to promoting the development of social scene understanding. In a social relationship, consistency is easy to be ignored but is useful for many fields, e.g., natural language processing and computer vision. The social image colocalization aims to utilize the consistent relationship to discover the objects belonging to the same category and locate them by rectangle bounding boxes. The consistency is principally reflected in the category of objects, which can be achieved by a similar appearance or a uniform structure. Currently, state-of-the-art colocalization methods mainly focus on three solving strategies: candidate region proposal selection, saliency maps-based methods, and deep descriptor transformation. In this article, we sort out several kinds of the present mainstream methods and provide a comprehensive review of their fundamentals and performance. We expect that the overview of colocalization would be helpful for the researchers who are new to this area and provide them enlightenments.",Proposals; Task analysis; Noise measurement; Computational modeling; Computer vision; Optimization; Linear programming; Cofusion; cosaliency; social computation; social image colocalization; social relationship discovery
"This paper presents a novel architecture for detecting mathematical formulas in document images, which is an important step for reliable information extraction in several domains. Recently, Cascade Mask R-CNN networks have been introduced to solve object detection in computer vision. In this paper, we suggest a couple of modifications to the existing Cascade Mask R-CNN architecture: First, the proposed network uses deformable convolutions instead of conventional convolutions in the backbone network to spot areas of interest better. Second, it uses a dual backbone of ResNeXt-101, having composite connections at the parallel stages. Finally, our proposed network is end-to-end trainable. We evaluate the proposed approach on the ICDAR-2017 POD and Marmot datasets. The proposed approach demonstrates state-of-the-art performance on ICDAR-2017 POD at a higher IoU threshold with an f1-score of 0.917, reducing the relative error by 7.8%. Moreover, we accomplished correct detection accuracy of 81.3% on embedded formulas on the Marmot dataset, which results in a relative error reduction of 30%.",formula detection; Cascade Mask R-CNN; mathematical expression detection; document image analysis; deep neural networks; computer vision
"The Chenopodiaceae species are ecologically and financially important, and play a significant role in biodiversity around the world. Biodiversity protection is critical for the survival and sustainability of each ecosystem and since plant species recognition in their natural habitats is the first process in plant diversity protection, an automatic species classification in the wild would greatly help the species analysis and consequently biodiversity protection on earth. Computer vision approaches can be used for automatic species analysis. Modern computer vision approaches are based on deep learning techniques. A standard dataset is essential in order to perform a deep learning algorithm. Hence, the main goal of this research is to provide a standard dataset of Chenopodiaceae images. This dataset is called ACHENY and contains 27030 images of 30 Chenopodiaceae species in their natural habitats. The other goal of this study is to investigate the applicability of ACHENY dataset by using deep learning models. Therefore, two novel deep learning models based on ACHENY dataset are introduced: First, a lightweight deep model which is trained from scratch and is designed innovatively to be agile and fast. Second, a model based on the EfficientNet-B1 architecture, which is pre-trained on ImageNet and is fine-tuned on ACHENY. The experimental results show that the two proposed models can do Chenopodiaceae fine-grained species recognition with promising accuracy. To evaluate our models, their performance was compared with the well-known VGG-16 model after fine-tuning it on ACHENY. Both VGG-16 and our first model achieved about 80% accuracy while the size of VGG-16 is about 16x larger than the first model. Our second model has an accuracy of about 90% and outperforms the other models where its number of parameters is 5x than the first model but it is still about one-third of the VGG-16 parameters.",Biodiversity protection; Chenopodiaceae; deep learning; convolutional neural networks; image classification; plant classification; standard dataset
"Manual trimming of sheepskin is intensive labor, and the working environment is full of rotten smells. The tannery is facing increasingly severe recruitment difficulties. This paper uses computer vision technology to study automatic recognition of sheepskin contours, which is the basis for the subsequent automatic trimming of sheepskin. After observing and analyzing the raw sheepskin images collected by an industrial array camera, a method of sheepskin contour extraction based on computer vision measurement technology is proposed in this paper. This method uses the fast Otsu threshold algorithm based on the pixel set to perform binary image segmentation. Combined with morphological processing for edge defect filling and topology analysis of boundary contour tracking algorithm to extract maximum contour information, it has a pixellevel three-dimensional de-noising preprocessing function and can accurately extract the sheepskin contour in the raw sheepskin image. The experimental results show that using the fast Otsu threshold algorithm proposed in this paper for binary segmentation to extract sheepskin contours, the detection rate is nearly 160% faster than the traditional Otsu algorithm, the edge protection is better, the error segmentation is reduced by nearly 3% and it has good anti-noise performance. It can meet the industrial production requirements of subsequent automatic cutting of sheepskin.",
"Deep neural networks (DNNs), especially those used in computer vision, are highly vulnerable to adversarial attacks, such as adversarial perturbations and adversarial patches. Adversarial patches, often considered more appropriate for a real-world attack, are attached to the target object or its surroundings to deceive the target system. However, most previous research employed adversarial patches that are conspicuous to human vision, making them easy to identify and counter. Previously, the spatially localized perturbation GAN (SLP-GAN) was proposed, in which the perturbation was only added to the most representative area of the input images, creating a spatially localized adversarial camouflage patch that excels in terms of visual fidelity and is, therefore, difficult to detect by human vision. In this study, the use of the method called eSLP-GAN was extended to deceive classifiers and object detection systems. Specifically, the loss function was modified for greater compatibility with an object-detection model attack and to increase robustness in the real world. Furthermore, the applicability of the proposed method was tested on the CARLA simulator for a more authentic real-world attack scenario.",adversarial patch; generative adversarial networks; camouflage
"Recent progress of deep image classification models provides great potential for improving related computer vision tasks. However, the transition to semantic segmentation is hampered by strict memory limitations of contemporary GPUs. The extent of feature map caching required by convolutional backprop poses significant challenges even for moderately sized Pascal images, while requiring careful architectural considerations when input resolution is in the megapixel range. To address these concerns, we propose a novel ladder-style DenseNet-based architecture which features high modelling power, efficient upsampling, and inherent spatial efficiency which we unlock with checkpointing. The resulting models deliver high performance and allow training at megapixel resolution on commodity hardware. The presented experimental results outperform the state-of-the-art in terms of prediction accuracy and execution speed on Cityscapes, VOC 2012, CamVid and ROB 2018 datasets. Source code at https://github.com/ivankreso/LDN.",Semantics; Feature extraction; Image segmentation; Computational modeling; Spatial resolution; Checkpointing; Computer vision; supervised learning; image segmentation; road transportation
"Scene understanding of satellite and aerial images is a pivotal task in various remote sensing (RS) practices, such as land cover and urban development monitoring. In recent years, neural networks have become a de-facto standard in many of these applications. However, semantic segmentation still remains a challenging task. With respect to other computer vision (CV) areas, in RS large labeled datasets are not very often available, due to their large cost and to the required manpower. On the other hand, self-supervised learning (SSL) is earning more and more interest in CV, reaching state-of-the-art in several tasks. In spite of this, most SSL models, pretrained on huge datasets like ImageNet, do not perform particularly well on RS data. For this reason, we propose a combination of a SSL algorithm (particularly, Online Bag of Words) and a semantic segmentation algorithm, shaped for aerial images (namely, Multistage Attention ResU-Net), to show new encouraging results (i.e., 81.76% mIoU with ResNet-18 backbone) on the ISPRS Vaihingen dataset.",semantic segmentation; self-supervised learning; linear attention; Vaihingen dataset
"Computer vision and artificial intelligence applications in medicine are becoming increasingly important day by day, especially in the field of image technology. In this paper we cover different artificial intelligence advances that tackle some of the most important worldwide medical problems such as cardiology, cancer, dermatology, neurodegenerative disorders, respiratory problems, and gastroenterology. We show how both areas have resulted in a large variety of methods that range from enhancement, detection, segmentation and characterizations of anatomical structures and lesions to complete systems that automatically identify and classify several diseases in order to aid clinical diagnosis and treatment. Different imaging modalities such as computer tomography, magnetic resonance, radiography, ultrasound, dermoscopy and microscopy offer multiple opportunities to build automatic systems that help medical diagnosis, taking advantage of their own physical nature. However, these imaging modalities also impose important limitations to the design of automatic image analysis systems for diagnosis aid due to their inherent characteristics such as signal to noise ratio, contrast and resolutions in time, space and wavelength. Finally, we discuss future trends and challenges that computer vision and artificial intelligence must face in the coming years in order to build systems that are able to solve more complex problems that assist medical diagnosis.",Artificial intelligence ( AI); computer vision (CV); medical image analysis; cardiology; oncology; microscopy; neurodegenerative disorders; respiratory diseases; gastroenterology
"In increasing manufacturing productivity with automated surface inspection in smart factories, the demand for machine vision is rising. Recently, convolutional neural networks (CNNs) have demonstrated outstanding performance and solved many problems in the field of computer vision. With that, many machine vision systems adopt CNNs to surface defect inspection. In this study, we developed an effective data augmentation method for grayscale images in CNN-based machine vision with mono cameras. Our method can apply to grayscale industrial images, and we demonstrated outstanding performance in the image classification and the object detection tasks. The main contributions of this study are as follows: (1) We propose a data augmentation method that can be performed when training CNNs with industrial images taken with mono cameras. (2) We demonstrate that image classification or object detection performance is better when training with the industrial image data augmented by the proposed method. Through the proposed method, many machine-vision-related problems using mono cameras can be effectively solved by using CNNs.",machine vision; data augmentation; deep learning; convolutional neural networks; transfer learning
"Synthetic aperture radar (SAR) ship detection is an important part of remote sensing applications. With the development of computer vision, SAR ship detection methods based on convolutional neural network (CNN) can directly perform end-to-end detection of near-shore ship targets. However, CNN-based methods are prone to generate false targets on land areas, especially when using a rotatable bounding box (RBox) for detection. Therefore, how to reduce the false alarm rate becomes a key direction in research for SAR ship detection. In this letter, the problem of negative sample intraclass imbalance in the training stage of CNN-based detection methods is pointed out for the first time, which is considered to be an important reason for the excessive false alarm rate in the land area. Then, a method is proposed to reduce the false targets generated in the land area by CNN-based detection methods. First, an RBox-based model is proposed as the basic architecture for detection. Then, a new loss function is adopted to guide the model to balance the loss contribution of different negative samples during the training stage. The experimental results prove that the proposed method can effectively reduce the false alarm rate of the model and boost the performance of CNN-based detection methods.",Marine vehicles; Training; Synthetic aperture radar; Feature extraction; Image segmentation; Computational modeling; Remote sensing; Computer vision; convolutional neural network (CNN); false alarm suppression; ship detection; synthetic aperture radar (SAR)
"Automatic facial wrinkles detection and inpainting algorithms have gained attention of researchers in cosmetics, forensics and computer vision. The majority of current inpainting algorithms was implemented on the whole face, despite the fact that only imperfections need inpainting. In general, the definition of an imperfection is sign of ageing, spot, scar and freckles. This paper focuses on wrinkles as it is an obvious sign of ageing. We survey the computer vision techniques in facial wrinkles localisation from detection to inpainting. We present a comprehensive literature review on benchmark datasets, automated wrinkles detection algorithms and facial inpainting algorithms. Due to limited study on wrinkle inpainting, we inpaint the wrinkle regions using three state-of-the-art inpainting algorithms, namely flood-fill, Coherence Sensitivity Hashing and exemplar-based method. To assess the realism of inpainting results, we present the original and inpainted images to 40 participants, where they provide rating on the realism scale and age group of each image. The result shows that flood-fill method preserved the realism but there was no significant difference in age prediction. Finally, we conclude the paper by proposing some future directions to advance this field.",Faces; Face recognition; Skin; Detection algorithms; Aging; Forehead; Estimation; Facial wrinkles; wrinkles detection; wrinkles inpainting; realism assessment
"This paper introduces computer vision systems (CVSs), which provides a new method to measure gem colour, and compares CVS and colourimeter (CM) measurements of jadeite-jade colour in the CIELAB space. The feasibility of using CVS for jadeite-jade colour measurement was verified by an expert group test and a reasonable regression model in an experiment involving 111 samples covering almost all jadeite-jade colours. In the expert group test, more than 93.33% of CVS images are considered to have high similarities with real objects. Comparing L*, a*, b*, C*, h, and increment E* (greater than 10) from CVS and CM tests indicate that significant visual differences exist between the measured colours. For a*, b*, and h, the R-2 of the regression model for CVS and CM was 90.2% or more. CVS readings can be used to predict the colour value measured by CM, which means that CVS technology can become a practical tool to detect the colour of jadeite-jade.",colour; computer vision system; colourimeter; jadeite-jade
"This paper presents a new model for multi-object tracking (MOT) with a transformer. MOT is a spatiotemporal correlation task among interest objects and one of the crucial technologies of multi-unmanned aerial vehicles (Multi-UAV). The transformer is a self-attentional codec architecture that has been successfully used in natural language processing and is emerging in computer vision. This study proposes the Vision Transformer Tracker (ViTT), which uses a transformer encoder as the backbone and takes images directly as input. Compared with convolution networks, it can model global context at every encoder layer from the beginning, which addresses the challenges of occlusion and complex scenarios. The model simultaneously outputs object locations and corresponding appearance embeddings in a shared network through multi-task learning. Our work demonstrates the superiority and effectiveness of transformer-based networks in complex computer vision tasks and paves the way for applying the pure transformer in MOT. We evaluated the proposed model on the MOT16 dataset, achieving 65.7% MOTA, and obtained a competitive result compared with other typical multi-object trackers.",MOT; transformer; attention; backbone
"Cataract is the cloudiness present in the eye lens due to denaturation of active protein cells. Cataract affects the quality-of-life and thereby troubling the daily routine activities. Early diagnosis and treatment may reduce the vision loss and delays the cataract progression. To diagnose large-screen population, the computer-aided cataract diagnosis (CACD) system using fundus retinal (FR) images is required. In this paper, a CACD system using FR images is proposed to achieve better diagnostic accuracy. It is perceived that the performance of existing CACD systems is poor against noisy input FR images. However, the distortion such as noise is unavoidable in input images due to complex processes involved in the image acquisition. Hence, it is required to consider the effect of noise in the design of CACD systems. So the proposed CACD system includes this issue in the design and provides the robust performance. In the presented CACD system, the features are extracted using combined feature extraction (CFE) technique using two independently fine-tuned deep convolutional neural networks. The noise level estimation (NLE)-based classification is adopted in the classification stage. In NLE-based classification, a set of multi-class support vector machine (SVM) classifiers, which are trained independently at noise levels from 0 to 25 are considered. Finally, the features extracted using CFE are then mapped to a specific multi-class SVM classifier based on noise level present in an input FR image. From the experimental results, it is observed that the proposed system exhibits superior performance than existing CACD systems under noisy conditions.",Computer-aided diagnosis; Cataract; Deep learning; Additive white Gaussian noise; Robustness; Noise level estimation
"In recent years, deep learning-based models have achieved significant advances in manifold computer vision problems, but tedious parameter tuning has complicated their application to computer-aided diagnostic (CAD) systems. As such, this study introduces a novel pruning strategy to improve the accuracy of five lightweight deep convolutional neural network (DCNN) architectures applied to the classification of skin disease. Unlike conventional pruning methods (such as optimal brain surgeon), the proposed technique does not change the model size yet improves performance after fine tuning. This training approach, intended to improve accuracy without increasing model complexity, is experimentally verified using 1167 pathological images. The clinical data included 11 different skin disease types collected over the past ten years, with varying image quantities in each category. A novel hierarchical pruning method, based on standard deviation, is then developed and used to prune parameters in each convolution layer according to the different weight distributions. This training strategy achieves an 83.5% Top-1 accuracy using a pruned MnasNet (12.5 MB), which is 1.8% higher than that of unpruned InceptionV3 (256 MB). Comparative experiments using other networks (MobileNetV2, SqueezeNet, ShuffleNetV2, Xception, ResNet50, DenseNet121) and dataset (HAM10000) also demonstrate consistent improvements when adopting the proposed model training technique. This distinctive robustness across various network types and simple deployment demonstrates the potential of this methodology for generalization to other computer vision tasks. (C) 2021 Elsevier B.V. All rights reserved.",Skin cancer detection; Deep learning; Lightweight CNNs; Model training strategy; Weight pruning
"In intelligent video surveillance under complex scenes, it is vital to identify the current actions of multi-target human bodies accurately and in real time. In this paper, a real-time multi-person action recognition method with monocular vision is proposed based on sequence models. Firstly, the key points of multi-target human body skeleton in the video are extracted by using the OpenPose algorithm. Then, the human action features are constructed, including limb direction vector and the skeleton height-width ratio. The multi-target human bodies tracking is then achieved by using the tracking algorithm. Next, the tracking results are matched with the action features, and the action recognition model is constructed, which includes the spatial branch based on Deep neural networks and the temporal branch based on Bi-directional RNN and Bi-directional long short-term memory networks. After pre-training, the model can be used to recognize the human body action from action features, and a recognition stabilizer is designed to minimize false alarms. Finally, extensive evaluations on the JHMDB dataset validate the effectiveness and the superiority of the proposed approach.",Action recognition; Human body skeleton; Feature construction; Sequence models; Computer vision
"Objectives The purpose of this study was to build a deep learning model to derive labels from neuroradiology reports and assign these to the corresponding examinations, overcoming a bottleneck to computer vision model development. Methods Reference-standard labels were generated by a team of neuroradiologists for model training and evaluation. Three thousand examinations were labelled for the presence or absence of any abnormality by manually scrutinising the corresponding radiology reports ('reference-standard report labels'); a subset of these examinations (n = 250) were assigned 'reference-standard image labels' by interrogating the actual images. Separately, 2000 reports were labelled for the presence or absence of 7 specialised categories of abnormality (acute stroke, mass, atrophy, vascular abnormality, small vessel disease, white matter inflammation, encephalomalacia), with a subset of these examinations (n = 700) also assigned reference-standard image labels. A deep learning model was trained using labelled reports and validated in two ways: comparing predicted labels to (i) reference-standard report labels and (ii) reference-standard image labels. The area under the receiver operating characteristic curve (AUC-ROC) was used to quantify model performance. Accuracy, sensitivity, specificity, and F1 score were also calculated. Results Accurate classification (AUC-ROC > 0.95) was achieved for all categories when tested against reference-standard report labels. A drop in performance (Delta AUC-ROC > 0.02) was seen for three categories (atrophy, encephalomalacia, vascular) when tested against reference-standard image labels, highlighting discrepancies in the original reports. Once trained, the model assigned labels to 121,556 examinations in under 30 min. Conclusions Our model accurately classifies head MRI examinations, enabling automated dataset labelling for downstream computer vision applications.",Deep learning; Natural language processing; Magnetic resonance imaging; Data curation; Radiology
"One of the most fundamental challenges in computer vision is pedestrian detection since it involves both the classification and localization of pedestrians at a location. To achieve real-time pedestrian detection without having any loss in detection accuracy, an Optimized MobileNet + SSD network is proposed. There are four important components in pedestrian detection: feature extraction, deformation, occlusion handling and classification. The existing methods design these components either independently or in a sequential format, and the interaction among these components has not been explored yet. The proposed network lets the components work in coordination in such a manner that their strengths are improved and the number of parameters is decreased compared to recent detection architectures. We propose a concatenation feature fusion module for adding contextual information in the Optimized MobileNet + SSD network to improve the detection accuracy of pedestrians. The proposed model achieved 80.4% average precision with a detection speed of 34.01 frames per second (fps) when tested on the Jetson Nano board, which is much faster compared to standard video speed (30 fps). Experimental results have shown that the proposed network has a better detection effect during low light conditions and for darker pictures. Therefore, the proposed network is well suited for low-end edge devices.",Pedestrian detection; Computer vision (CV); Optimized MobileNet plus SSD Network; Caltech pedestrian dataset; Jetson Nano board
"Multimedia Internet-of-Things (IoT) systems have been widely utilized in various computer vision tasks and significantly integrated computer vision and networking capabilities. In these systems, convolutional neural networks (CNNs) perform a preliminary analysis of the collected video or image information in the edge devices. However, the high computational cost and huge storage consumption of the complex CNNs prevent their deployment on mobile-edge devices that have limited computational resource and memory. In this article, we aim to simultaneously accelerate and compress CNNs via a multilevel filter pruning (MFP) algorithm, to alleviate the dependence on the hardware of IoT edge nodes. First, a global pruning sensitivity order is defined, which could guide us to perform preliminary pruning from the perspective of convolutional layers' sensitivity. Then, the functional index of each filter is judged by the image entropy of its output feature map, which contributes to further pruning from the perspective of filter function importance. Finally, the moderate fine tuning is adopted to recover the network capability. The experimental results show that the proposed MFP algorithm could reduce 54.5% floating-point operations and 31.9% graphics memory for VGG-16 on CIFAR-10, and achieve 5.45x floating-point acceleration and 19.70x storage reduction for VGG-16 on ImageNet. In the reconstruction phase, the algorithm could recover the network capability much faster than the existing pruning algorithms.",Image entropy; multilevel pruning; multimedia Internet of Things (IoT); sensitivity analysis
"The study of artificial learning processes in the area of computer vision context has mainly focused on achieving a fixed output target rather than on identifying the underlying processes as a means to develop solutions capable of performing as good as or better than the human brain. This work reviews the well-known segmentation efforts in computer vision. However, our primary focus is on the quantitative evaluation of the amount of contextual information provided to the neural network. In particular, the information used to mimic the tacit information that a human is capable of using, like a sense of unambiguous order and the capability of improving its estimation by complementing already learned information. Our results show that, after a set of pre and postprocessing methods applied to both the training data and the neural network architecture, the predictions made were drastically closer to the expected output in comparison to the cases where no contextual additions were provided. Our results provide evidence that learning systems strongly rely on contextual information for the identification task process.",Deep learning; U-net; Semantic segmentation; Metadata preprocessing; Fully convolutional network; Indoor scenes
"Analyzing the walking behavior of the public is vital for revealing the need for infrastructure design in a local neighborhood, supporting human-centric urban area development. Traditional walking behavior analysis practices relying on manual on-street surveys to collect pedestrian flow data are labor-intensive and tedious. On the contrary, automated video analytics using surveillance cameras based on computer vision and deep learning techniques appears more effective in generating pedestrian flow statistics. Nevertheless, most existing methods of pedestrian tracking and attribute recognition suffer from several challenging conditions, such as inter-person occlusion and appearance variations, which leads to ambiguous identities and hence inaccurate pedestrian flow statistics. Therefore, this paper proposes a more robust methodology of pedestrian tracking and attribute recognition, facilitating the analysis of pedestrian walking behavior. Specific limitations of a current state-of-the-art method are inferred, based on which several improvement strategies are proposed: 1) incorporating high-level pedestrian attributes to enhance pedestrian tracking, 2) a similarity measure integrating multiple cues for identity matching, and 3) a probation mechanism for more robust identity matching. From our evaluation using two public benchmark datasets, the developed strategies notably enhance the robustness of pedestrian tracking against the challenging conditions mentioned above. Subsequently, the outputs of trajectories and attributes are aggregated into fine-grained pedestrian flow statistics among different pedestrian groups. Overall, our developed framework can support a more comprehensive and reliable decision-making for human-centric planning and design in different urban areas. The framework is also applicable to exploiting pedestrian movement patterns in different scenes for analyses such as urban walkability evaluation. Moreover, the developed mechanisms are generalizable to future researches as a baseline, which provides generic insights of how to fundamentally enhance pedestrian tracking.",Computer vision; Deep learning; Pedestrian attribute recognition; Pedestrian trajectory tracking; Walking behavior analysis
"Artificial intelligence (AI) in radiology has gained wide interest due to the development of neural network architectures with high performance in computer vision related tasks. As AI based software programs become more integrated into the clinical workflow, radiologists can benefit from better understanding the principles of artificial intelligence. This series aims to explain basic concepts of AI and its applications in medical imaging. In this article, we will review the background of neural network architecture and its application in imaging analysis.",CNN architecture; Artificial intelligence; Deep learning
"The main challenges for the automatic detection of the coronavirus disease (COVID-19) from computed tomography (CT) scans of an individual are: a lack of large datasets, ambiguity in the characteristics of COVID-19 and the detection techniques having low sensitivity (or recall). Hence, developing diagnostic techniques with high recall and automatic feature extraction using the available data are crucial for controlling the spread of COVID-19. This paper proposes a novel stacked ensemble capable of detecting COVID-19 from a patient's chest CT scans with high recall and accuracy. A systematic approach for designing a stacked ensemble from pre-trained computer vision models using transfer learning (TL) is presented. A novel diversity measure that results in the stacked ensemble with high recall and accuracy is proposed. The stacked ensemble proposed in this paper considers four pre-trained computer vision models: the visual geometry group (VGG)-19, residual network (ResNet)-101, densely connected convolutional network (DenseNet)-169 and wide residual network (WideResNet)-50-2. The proposed model was trained and evaluated with three different chest CT scans. As recall is more important than precision, the trade-offs between recall and precision were explored in relevance to COVID-19. The optimal recommended threshold values were found for each dataset.",COVID-19; False negatives; Transfer learning; Recall
"Motion Capture datasets are captured as high-frequency discrete samples using photogrammetric computer vision or various industrial geodetic measurement methods over the relevant model. Because Motion Capture data are inherently large-datasets, expressing Motion Capture data without employing a motion data abstraction approach such as keypose is challenging. Keypose synthesis is a serious problem in many applications. Unfortunately, the local statistical features of Motion Capture data over time periods vary often, making it difficult to determine a key point summing the motion activation involved. Conventional clustering methods, such as Fuzzy C-Means (FCM), that are sensitive to initial conditions, can be fitted to a local solution rather than producing a highly reliable keypose. Data clustering can be performed using evolutionary search methods without being limited to relatively local solutions. In this paper, the Motion Capture data were clustered using evolutionary search algorithms, and related keyposes were synthesized using the minimum-distance to cluster-centers principle. In Experiments section of this paper, motion data containing sportive movements were clustered by using evolutionary algorithms (i.e., Particle Swarm Optimization, Artificial Bee Colony, and Differential Search Algorithm) and classical clustering algorithms (i.e., Self-Organizing Neural Network, FCM) to obtain related keyposes. The computed statistics exposed that evolutionary methods were more successful in obtaining keypose than classical methods.",computer vision; evolutionary search algorithms; keypose; motion capture; motion segmentation; photogrammetry
"Due to the rapid development of science and technology, object detection has become a promising research direction in computer vision. In recent years, most object detection frameworks proposed in the existing research are 2D. However, 2D object detection cannot take three-dimensional space into account, resulting in its inability to be used to solve problems in real world. Hence, we conduct this 3D object detection survey in the hope that 3D object detection methods can be better applied to the contexts of intelligent video surveillance, robot navigation and autonomous driving technology. There exist various 3D object detection methods while in this paper we only focus on the popular deep learning based methods. We divide these approaches into four categories according to the input data category. Besides, we discuss the innovations of these frames and compare their experimental results in terms of accuracy. Finally, we indicate the technical difficulties associated with current 3D object detection and discuss future research directions.",Computer vision; 3D object detection; Deep learning; Autonomous driving technology; Point cloud
"The paper presents a methodology for training neural networks for vision tasks on synthesized data on the example of steel defect recognition in automated production control systems. The article describes the process of dataset procedural generation of steel slab defects with a symmetrical distribution. The results of training two neural networks Unet and Xception on a generated data grid and testing them on real data are presented. The performance of these neural networks was assessed using real data from the Severstal: Steel Defect Detection set. In both cases, the neural networks showed good results in the classification and segmentation of surface defects of steel workpieces in the image. Dice score on synthetic data reaches 0.62, and accuracy-0.81.",computer vision; synthetic data; steel defect detection; machine learning
"Fashion is the way we present ourselves to the world and has become one of the world's largest industries. Fashion, mainly conveyed by vision, has thus attracted much attention from computer vision researchers in recent years. Given the rapid development, this article provides a comprehensive survey of more than 200 major fashion-related works covering four main aspects for enabling intelligent fashion: (1) Fashion detection includes landmark detection, fashion parsing, and item retrieval; (2) Fashion analysis contains attribute recognition, style learning, and popularity prediction; (3) Fashion synthesis involves style transfer, pose transformation, and physical simulation; and (4) Fashion recommendation comprises fashion compatibility, outfit matching, and hairstyle suggestion. For each task, the benchmark datasets and the evaluation protocols are summarized. Furthermore, we highlight promising directions for future research.",Intelligent fashion; fashion detection; fashion analysis; fashion synthesis; fashion recommendation
"Human activity recognition aims to classify the user activity in various applications like healthcare, gesture recognition and indoor navigation. In the latter, smartphone location recognition is gaining more attention as it enhances indoor positioning accuracy. Commonly the smartphone's inertial sensor readings are used as input to a machine learning algorithm which performs the classification. There are several approaches to tackle such a task: feature based approaches, one dimensional deep learning algorithms, and two dimensional deep learning architectures. When using deep learning approaches, feature engineering is redundant. In addition, while utilizing two-dimensional deep learning approaches enables to utilize methods from the well-established computer vision domain. In this paper, a framework for smartphone location and human activity recognition, based on the smartphone's inertial sensors, is proposed. The contributions of this work are a novel time series encoding approach, from inertial signals to inertial images, and transfer learning from computer vision domain to the inertial sensors classification problem. Four different datasets are employed to show the benefits of using the proposed approach. In addition, as the proposed framework performs classification on inertial sensors readings, it can be applied for other classification tasks using inertial data. It can also be adopted to handle other types of sensory data collected for a classification task.",activity recognition; two dimensional convolutional neural network; accelerometers; gyroscopes
"Deep regression trackers are among the fastest tracking algorithms available, and therefore suitable for real-time robotic applications. However, their accuracy is inadequate in many domains due to distribution shift and overfitting. In this letter we overcome such limitations by presenting the first methodology for domain adaption of such a class of trackers. To reduce the labeling effort we propose a weakly-supervised adaptation strategy, in which reinforcement learning is used to express weak supervision as a scalar application-dependent and temporally-delayed feedback. At the same time, knowledge distillation is employed to guarantee learning stability and to compress and transfer knowledge from more powerful but slower trackers. Extensive experiments on five different robotic vision domains demonstrate the relevance of our methodology. Real-time speed is achieved on embedded devices and on machines without GPUs, while accuracy reaches significant results.",Visual tracking; computer vision for automation; deep learning for visual perception
"While there is a significant body of research on crack detection by computer vision methods in concrete and asphalt, less attention has been given to masonry. We train a convolutional neural network (CNN) on images of brick walls built in a laboratory environment and test its ability to detect cracks in images of brick-and-mortar structures both in the laboratory and on real-world images taken from the internet. We also compare the performance of the CNN to a variety of simpler classifiers operating on handcrafted features. We find that the CNN performed better on the domain adaptation from laboratory to real-world images than these simple models. However, we also find that performance is significantly better in performing the reverse domain adaptation task, where the simple classifiers are trained on real-world images and tested on the laboratory images. This work demonstrates the ability to detect cracks in images of masonry using a variety of machine learning methods and provides guidance for improving the reliability of such models when performing domain adaptation for crack detection in masonry.",computer vision; crack detection; structural health monitoring; masonry; machine learning; convolutional neural network
"Benchmark datasets used for testing computer vision (CV) methods often contain little variation in illumination. The methods that perform well on these datasets have been observed to fail under challenging illumination conditions encountered in the real world, in particular, when the dynamic range of a scene is high. The authors present a new dataset for evaluating CV methods in challenging illumination conditions such as low light, high dynamic range, and glare. The main feature of the dataset is that each scene has been captured in all the adversarial illuminations. Moreover, each scene includes an additional reference condition with uniform illumination, which can be used to automatically generate labels for the tested CV methods. We demonstrate the usefulness of the dataset in a preliminary study by evaluating the performance of popular face detection, optical flow, and object detection methods under adversarial illumination conditions. We further assess whether the performance of these applications can be improved if a different transfer function is used. (C) 2021 Society for Imaging Science and Technology.",
"In the area of computer vision (CV), action recognition is a hot topic of research nowadays due to famous applications, which include human-machine interaction, robotics, visual surveillance, video analysis, etc. Many techniques are presented in the literature by researchers of CV, but still they faced a lot of challenges such as complexity in the background, variation in the camera view point and movement of humans. A new method is proposed in this work for action recognition. The proposed method is based on the shape and deep learning features fusion. Two-steps-based method is executed- human extraction to action recognition. In the first step, first, humans are extracted by simple learning process. In this process, HOG features are extracted from few selected datasets such as INRIA, CAVIAR, Weizmann and KTH. Then, we need to select the robust features using entropy-controlled LSVM maximization and performed detection. Second, geometric features are extracted from detected regions and parallel deep learning features are extracted from original video frame. However, the extracted deep learning features are high in dimension and some are not relevant, so it is essential to remove irrelevant features before fusion. For this purpose, a new feature reduction technique is presented named as entropy-controlled geometric mean . Through this technique, we can select the robust deep learning features and remove the irrelevant of them. Finally, both types of features (selected deep learning and original geometric) are fused by proposed parallel conditional entropy approach. The obtained feature vector is classified by a cubic multi-class SVM. Six datasets (i.e., IXMAS, KTH, Weizmann, UCF Sports, UT Interaction and WVU) are used for the experimental process and achieved an average accuracy of above 98.00%. The detailed statistical analysis and comparison with existing techniques show the the effectiveness of proposed method .",Action recognition; Silhouette extraction; Shape features; Deep features; Feature selection; Feature fusion
"Deep learning has been broadly leveraged by major cloud providers, such as Google, AWS and Baidu, to offer various computer vision related services including image classification, object identification, illegal image detection, etc. While recent works extensively demonstrated that deep learning classification models are vulnerable to adversarial examples, cloud-based image detection models, which are more complicated than classifiers, may also have similar security concern but not get enough attention yet. In this paper, we mainly focus on the security issues of real-world cloud-based image detectors. Specifically, (1) based on effective semantic segmentation, we propose four attacks to generate semantics-aware adversarial examples via only interacting with black-box APIs; and (2) we make the first attempt to conduct an extensive empirical study of black-box attacks against real-world cloud-based image detectors. Through the comprehensive evaluations on five major cloud platforms: AWS, Azure, Google Cloud, Baidu Cloud, and Alibaba Cloud, we demonstrate that our image processing based attacks can reach a success rate of approximately 100 percent, and the semantic segmentation based attacks have a success rate over 90 percent among different detection services, such as violence, politician, and pornography detection. We also proposed several possible defense strategies for these security challenges in the real-life situation.",Detectors; Image segmentation; Deep learning; Google; Computer vision; Computational modeling; Semantics; Cloud vision API; cloud-based image detection service; deep learning; adversarial examples
"Manual processing of a large volume of video data captured through closed-circuit television is challenging due to various reasons. First, manual analysis is highly time-consuming. Moreover, as surveillance videos are recorded in dynamic conditions such as in the presence of camera motion, varying illumination, or occlusion, conventional supervised learning may not work always. Thus, computer vision-based automatic surveillance scene analysis is carried out in unsupervised ways. Topic modelling is one of the emerging fields used in unsupervised information processing. Topic modelling is used in text analysis, computer vision applications, and other areas involving spatio-temporal data. In this article, we discuss the scope, variations, and applications of topic modelling, particularly focusing on surveillance video analysis. We have provided a methodological survey on existing topic models, their features, underlying representations, characterization, and applications in visual surveillance's perspective. Important research papers related to topic modelling in visual surveillance have been summarized and critically analyzed in this article.",Video analysis; topic model; unsupervised learning
"Nowadays, a goodness digital compactness measure is necessary in computer vision, shape analysis and computer medical diagnosis process where digital picture are used widely. We introduce a compactness measure called Normalized E-Factor which shows as a measure robust to translations, rotations and scale-changes and that it satisfies the set of criteria for a good compactness measure. Through a series of experiments, we show that the Normalized E-Factor is useful for shape description, measuring digital compactness with or without holes and that it overcomes some drawbacks that present several compactness measures over digital space.",Shape description; Shape classification; Compactness measure; Digital region descriptor; Normalized
"Core failure inspection is an important issue in die casting. The inspection process is often carried out by manually examining X-ray images. However, human visual inspection suffers from individual biases and eye fatigues. Computer-vision-based automatic inspection, if it can achieve equal to or better than human performance, is favored to assist the inspectors to achieve better quality control. Most existing works are heavily relied on the supervised methods, which require enormous labeling and cannot be deployed quickly and economically. This is particularly difficult for a die casting plant that has many different types of products. Labeling each type of product before applying automated inspection may not be feasible in practice. It is therefore necessary to investigate unsupervised methods for die casting products. In this research, an inspection framework built on top of convolutional autoencoder (CAE) is designed and developed to inspect core failures from real-world die casting X-ray images in an unsupervised manner. Identification of good and scrap product, and localization of the defect are achieved in a single network. The framework is designed to be easily generalized to other image inspection scenarios. The area of interest for inspection is first extracted automatically through the Hough transformation. Then the preprocessed image is inspected by CAE. The noises of the model are removed using edge detection. It achieved an impressive 97.45% classification accuracy on average, and precisely pinpointed the defect regions with a small training set of 30 images.",Anomaly detection; Deep learning; Automatic inspection; Convolutional autoencoder; Computer vision; Die casting
"The pill manufacturing process accrues substantial financial costs due to quality. Pill quality inspection is laborious, time-consuming and subjective, resulting in poor statistical representation and inconsistent results. In this study, we developed an approach that integrates deep learning algorithms and computer-vision-based processing with an optimization algorithm to fully automate the image analysis of internal crack/contamination detection. This approach exploits the features learned by convolutional neural network using various sub-processing techniques and Adam optimization. It achieves robust quantification of internal pill defects with an average accuracy of 95%.",Quality inspection; deep convolutional network; deep learning; optimization
"With the rapid development of science and technology in today's society, various industries are pursuing information digitization and intelligence, and pattern recognition and computer vision are also constantly carrying out technological innovation. Computer vision is to let computers, cameras, and other machines receive information like human beings, analyze and process their semantic information, and make coping strategies. As an important research direction in the field of computer vision, human motion recognition has new solutions with the gradual rise of deep learning. Human motion recognition technology has a high market value, and it has broad application prospects in the fields of intelligent monitoring, motion analysis, human-computer interaction, and medical monitoring. This paper mainly studies the recognition of sports training action based on deep learning algorithm. Experimental work has been carried out in order to show the validity of the proposed research.",
"Recent increases in computational power and the development of specialized architecture led to the possibility to perform machine learning, especially inference, on the edge. OpenVINO is a toolkit based on convolutional neural networks that facilitates fast-track development of computer vision algorithms and deep learning neural networks into vision applications, and enables their easy heterogeneous execution across hardware platforms. A smart queue management can be the key to the success of any sector. In this paper, we focus on edge deployments to make the smart queuing system (SQS) accessible by all also providing ability to run it on cheap devices. This gives it the ability to run the queuing system deep learning algorithms on pre-existing computers which a retail store, public transportation facility or a factory may already possess, thus considerably reducing the cost of deployment of such a system. SQS demonstrates how to create a video AI solution on the edge. We validate our results by testing it on multiple edge devices, namely CPU, integrated edge graphic processing unit (iGPU), vision processing unit (VPU) and field-programmable gate arrays (FPGAs). Experimental results show that deploying a SQS on edge is very promising.",Smart queuing system; Edge computing; Edge AI; Soft computing; Optimization; Intel OpenVINO
"The three-dimensional (3D) reconstruction technology based on computer vision has greatly facilitated damage inspection and assessment and construction monitoring of civil engineering. However, there are several problems in its implementation. A new method of 3D reconstruction is hereby proposed in this paper regarding the geometrical characteristics of pictures of buildings. An algorithm can normalize the 3D-reconstructed point cloud model so that its length, width, and height are parallel to the coordinate axis of the world coordinate system, and the absolute scale of the point cloud model can be obtained. Compared with the traditional one, this methodology can maintain better accuracy. This paper establishes the theoretical framework of the methodology, and steps for implementation are given by using digital image processing technology. After analyzing the results of a field experiment, it has been proven that this methodology can make the best use of the geometric features and improve the efficiency of the traditional reconstruction algorithm and therefore result in high accuracy.",
"The vial, a bottle known to store the drug, should be controlled to meet the requirements of the standard dimension. Due to problems with a visual inspection, there is a need to develop an automated inspection system. In this paper, a machine vision system for measuring and controlling the dimensional characteristics of medical glass vials has been developed. In this regard, because of the difficulty of taking images of glass vials and reflecting the light that may have these images, some innovative actions have been taken to determine the way for obtaining the appropriate images. Also, the effectiveness of several common segmentation methods has been examined and a heuristic segmentation method is proposed to extract vial borders. Finally, using to integrate heuristic segmentation method and appropriate post-processing methods as well as employing machine learning, an automated approach for measuring different dimensional characteristics of vials is proposed and evaluated by real samples.",Heuristic segmentation method; Medicine glass vial; Image processing; Machine vision for dimensional defect detection
"Cytology is a low-cost and non-invasive diagnostic procedure employed to support the diagnosis of a broad range of pathologies. Cells are harvested from tissues by aspiration or scraping, and it is still predominantly performed manually by medical or laboratory professionals extensively trained for this purpose. It is a time-consuming and repetitive process where many diagnostic criteria are subjective and vulnerable to human interpretation. Computer Vision technologies, by automatically generating quantitative and objective descriptions of examinations' contents, can help minimize the chances of misdiagnoses and shorten the time required for analysis. To identify the state-of-art of computer vision techniques currently applied to cytology, we conducted a Systematic Literature Review, searching for approaches for the segmentation, detection, quantification, and classification of cells and organelles using computer vision on cytology slides. We analyzed papers published in the last 4 years. The initial search was executed in September 2020 and resulted in 431 articles. After applying the inclusion/ exclusion criteria, 157 papers remained, which we analyzed to build a picture of the tendencies and problems present in this research area, highlighting the computer vision methods, staining techniques, evaluation metrics, and the availability of the used datasets and computer code. As a result, we identified that the most used methods in the analyzed works are deep learning-based (70 papers), while fewer works employ classic computer vision only (101 papers). The most recurrent metric used for classification and object detection was the accuracy (33 papers and 5 papers), while for segmentation it was the Dice Similarity Coefficient (38 papers). Regarding staining techniques, Papanicolaou was the most employed one (130 papers), followed by H&E (20 papers) and Feulgen (5 papers). Twelve of the datasets used in the papers are publicly available, with the DTU/Herlev dataset being the most used one. We conclude that there still is a lack of high-quality datasets for many types of stains and most of the works are not mature enough to be applied in a daily clinical diagnostic routine. We also identified a growing tendency towards adopting deep learning-based approaches as the methods of choice.",Cytology; Segmentation; Classification; Deep learning; Computer vision
"Image processing and computer vision on mobile devices have a wide range of applications such as digital image enhancement and augmented reality. While images acquired by cameras on mobile devices can be processed with generic image processing algorithms, there are numerous constraints and external issues that call for customized algorithms for such devices. In this paper, we survey mobile image processing and computer vision applications while highlighting these constraints and explaining how the algorithms have been modified/adapted to meet accuracy and performance demands. We hope that this paper will be a useful resource for researchers who intend to apply image processing and computer vision algorithms to real-world scenarios and applications that involve mobile devices.",Mobile devices; Computer vision; Image processing
"The demand for smart automatic system in postharvest technology, particularly in the postharvest of carrot production is high. In this paper, an automatic carrot grading system was developed based on computer vision and deep learning, which can automatically inspect surface quality of carrots and grade washed carrots. Specifically, based on ShuffleNet and transfer learning, a lightweight deep learning model (CDDNet) was constructed to detect surface defects of carrots. Carrot grading methods were also proposed based on minimum bounding rectangle (MBR) fitting and convex polygon approximation. Experimental results showed that the detection accuracy of the proposed CDDNet was 99.82% for binary classification (normal and defective) and 93.01% for multi-class classification (normal, bad spot, abnormity, fibrous root), and demonstrated good performance both in time efficiency and detection accuracy. The grading accuracy of MBR fitting and convex polygon approximation was 92.8% and 95.1% respectively. This research provides a practical method for online defect detection and carrot grading, and has great application potential in commercial packing lines.",Carrot grading; Defect detection; Deep learning; Computer vision; CDDNet
"Traffic signs detection has become an important feature of Advanced driving assisting systems and even self-driving cars. In this paper, we present an implementation of a traffic signs detection method on Graphics Processing Units (GPU) under real-time conditions. The proposed model is based on deep convolutional neural networks, a deep learning model used in computer vision applications. The deep convolutional neural networks have recently been used to solve many computer vision tasks successfully. Unlike old techniques, the model is used to detect and identify the traffic signs at the same time without the need for any external modules. To achieve real-time inference, we implement the proposed model on the GPU as a natural choice for the implementation of deep learning-based models. Also, we build large traffic signs detection dataset. The dataset contains 10000 images captured from the Chinese roads under real-world factors like lightning, occlusion, complex background, etc. 73 traffic sign classes were considered in this dataset. The evaluation of the proposed model on the proposed dataset shows robust performance in terms of speed and accuracy.",Real-time processing; traffic signs detection; deep learning; high-performance computing application; convolutional neural networks
"Three dimensional (3D) hand pose estimation is the task of estimating the 3D location of hand keypoints. In recent years, this task has received much research attention due to its diverse applications in human-computer interaction and virtual reality. To the best of our knowledge, there has been limited studies that model self-attention in 3D hand pose estimation despite its use in various computer vision tasks. Hence, we propose augmenting convolution with self-attention to capture long-range dependencies in a depth image. In addition, motivated by a recent work which uses anchor points set on a depth image, we extend anchor points to the depth dimension to regress 3D hand joint locations. Validation experiments using the proposed approaches are performed on various hand pose datasets, and we obtain performances that are comparable to other state-of-the-art methods. The results demonstrate the potential of these approaches in a hand-based recognition system.",Attention; Convolution; Hand pose estimation; Neural network
"Convolutional neural network is a prominent innovation in computer vision but is often troubled by problems such as dark light, turbidity, blur and high similarity to the background when applied to underwater object detection. Underwater object detection is one of the basic techniques of underwater grasping automation which plays a very important role in ocean detection and fishery of aquatic products. This paper presented an automatic detection method of underwater sea cucumber based on deep learning, which will provide effective technical support for the automated breeding and harvesting of sea cucumber. The Shortcut Feature Pyramid Network (SFPN) proposed in this paper improves the existing multi-scale feature fusion strategy through shortcut connection. The ablation experimental results show that the mean average precision (mAP) of S-FPN reaches 91.5% which outperforms the baseline Feature Pyramid Network (88.6%), YOLO v3 (83.7%) and SVM-HOG (61.6%). To resolve the problem of complex environmental background interference of ocean floor, we proposed a Piecewise Focal Loss (PFL) function for balancing the positive and negative samples such that the algorithm can focus on the training difficulty of hard (i.e., positive) samples. And the ablation experimental results show that the mAP of PFL reaches 92.3% which outperforms the baseline Cross Entropy (91.5%) and Focal Loss (91.8%). Also, we chose Exponential Linear Unit as the optimization strategy, and Adaptive Moment Estimation as the activation function by ablation research, finally the mAP reached 94%.",Sea cucumber; Deep learning; Computer vision; Underwater object detection
"The research progress in multimodal learning has grown rapidly over the last decade in several areas, especially in computer vision. The growing potential of multimodal data streams and deep learning algorithms has contributed to the increasing universality of deep multimodal learning. This involves the development of models capable of processing and analyzing the multimodal information uniformly. Unstructured real-world data can inherently take many forms, also known as modalities, often including visual and textual content. Extracting relevant patterns from this kind of data is still a motivating goal for researchers in deep learning. In this paper, we seek to improve the understanding of key concepts and algorithms of deep multimodal learning for the computer vision community by exploring how to generate deep models that consider the integration and combination of heterogeneous visual cues across sensory modalities. In particular, we summarize six perspectives from the current literature on deep multimodal learning, namely: multimodal data representation, multimodal fusion (i.e., both traditional and deep learning-based schemes), multitask learning, multimodal alignment, multimodal transfer learning, and zero-shot learning. We also survey current multimodal applications and present a collection of benchmark datasets for solving problems in various vision domains. Finally, we highlight the limitations and challenges of deep multimodal learning and provide insights and directions for future research.",Applications; Computer vision; Datasets; Deep learning; Sensory modalities; Multimodal learning
"The details presented in this article revolve around a sophisticated monitoring framework equipped with knowledge representation and computer vision capabilities, that aims to provide innovative solutions and support services in the healthcare sector, with a focus on clinical and non-clinical rehabilitation and care environments for people with mobility problems. In contemporary pervasive systems most modern virtual agents have specific reactions when interacting with humans and usually lack extended dialogue and cognitive competences. The presented tool aims to provide natural human-computer multi-modal interaction via exploitation of state-of-the-art technologies in computer vision, speech recognition and synthesis, knowledge representation, sensor data analysis, and by leveraging prior clinical knowledge and patient history through an intelligent, ontology-driven, dialogue manager with reasoning capabilities, which can also access a web search and retrieval engine module. The framework's main contribution lies in its versatility to combine different technologies, while its inherent capability to monitor patient behaviour allows doctors and caregivers to spend less time collecting patient-related information and focus on healthcare. Moreover, by capitalising on voice, sensor and camera data, it may bolster patients' confidence levels and encourage them to naturally interact with the virtual agent, drastically improving their moral during a recuperation process.",Human-computer interaction; Sensors; Natural language processing; Pervasive systems; Computer vision; Healthcare
"Anomaly detection in pedestrian walkways is an important research topic, commonly used to improve the safety of pedestrians. Due to the wide utilization of video surveillance systems and the increased quantity of captured videos, the traditional manual examination of labeling abnormal events is a tiresome task. So, an automated surveillance system that detects anomalies becomes essential among computer vision researchers. Presently, the development of deep learning (DL) models has gained significant interest in different computer vision processes namely object classification and object detection, and these applications were depending on supervised learning that required labels. Therefore, this paper develops an automated deep learning based anomaly detection technique in pedestrian walkways (DLADT-PW) for vulnerable road user's safety. The goal of the DLADT-PW model is to detect and classify the various anomalies that exist in the pedestrian walkways such as cars, skating, jeep, etc. The DLADT-PW model involves preprocessing as the primary step, which is applied for removing the noise and raise the quality of the image. In addition, mask region convolutional neural network (Mask-RCNN) with densely connected networks (DenseNet) model is employed for the detection process. To ensure the better anomaly detection performance of the DLADT-PW technique, an extensive set of simulations were performed and the outcomes are investigated under distinct aspects. The obtained experimental values confirmed the superior characteristics of the DLADT-PW technique by achieving a maximum detection accuracy.",Anomaly detection; Pedestrian walkways; Deep learning; Safety; Mask RCNN
,
"Automated dimensional inspection is commonly expensive because of the requirement of high-precision measurement devices. To perform a precision measurement, the technician must be highly skilled and fully understands the operation of the equipment. This study proposes a method for reconstructing the two-dimensional profiles of ring-shaped objects using image processing. At first, an industrial camera captures partial images of the object. After that, through several image processing procedures such as binarization, line detection, and contour recognition, the profiles in the images were detected and grouped. Then, a calibration model was introduced to calibrate and combine the contours from partial images. This process results in a point cloud consisting of every point from the outer and inner contours of the object, which can be directly used for the automatic measurement. To verify the proposed method, the data were compared with those acquired from the ATOS measurement system, revealing a favourable correlation.",Image processing; Camera calibration; Ring-shaped object; 2D contour reconstruction
"A convolutional neural network (CNN) is one of the most significant networks in the deep learning field. Since CNN made impressive achievements in many areas, including but not limited to computer vision and natural language processing, it attracted much attention from both industry and academia in the past few years. The existing reviews mainly focus on CNN's applications in different scenarios without considering CNN from a general perspective, and some novel ideas proposed recently are not covered. In this review, we aim to provide some novel ideas and prospects in this fast-growing field. Besides, not only 2-D convolution but also 1-D and multidimensional ones are involved. First, this review introduces the history of CNN. Second, we provide an overview of various convolutions. Third, some classic and advanced CNN models are introduced; especially those key points making them reach state-of-the-art results. Fourth, through experimental analysis, we draw some conclusions and provide several rules of thumb for functions and hyperparameter selection. Fifth, the applications of 1-D, 2-D, and multidimensional convolution are covered. Finally, some open issues and promising directions for CNN are discussed as guidelines for future work.",Convolution; Kernel; Feature extraction; Neurons; Deep learning; Standards; Computer vision; Computer vision; convolutional neural networks (CNNs); deep learning; deep neural networks
"SARS-CoV-2 drive through screening centers (DTSC) have been implemented worldwide as a fast and secure way of mass screening. We use DTSCs as a platform for the acquisition of multimodal datasets that are needed for the development of remote screening methods. Our acquisition setup consists of an array of thermal, infrared and RGB cameras as well as microphones and we apply methods from computer vision and computer audition for the contactless estimation of physiological parameters. We have recorded a multimodal dataset of DTSC participants in Germany for the development of remote screening methods and symptom identification. Acquisition in the early stages of a pandemic and in regions with high infection rates can facilitate and speed up the identification of infection specific symptoms and large-scale data acquisition at DTSC is possible without disturbing the flow of operation.",computer vision; COVID-19; drive through screening; non-contact medical assessment; SARS-CoV-2
"With the development of Internet technology and the popularity of digital devices, Content-Based Image Retrieval (CBIR) has been quickly developed and applied in various fields related to computer vision and artificial intelligence. Currently, it is possible to retrieve related images effectively and efficiently from a large scale database with an input image. In the past ten years, great efforts have been made for new theories and models of CBIR and many effective CBIR algorithms have been established. In this paper, we present a survey on the fast developments and applications of CBIR theories and algorithms during the period from 2009 to 2019. We mainly review the technological developments from the viewpoint of image representation and database search. We further summarize the practical applications of CBIR in the fields of fashion image retrieval, person re-identification, e-commerce product retrieval, remote sensing image retrieval and trademark image retrieval. Finally, we discuss the future research directions of CBIR with the challenge of big data and the utilization of deep learning techniques.& nbsp; (c) 2020 Elsevier B.V. All rights reserved.",Content-based image retrieval; Image representation; Database search; Computer vision; Big data; Deep learning
"One of the promising methods for early detection of Coronavirus Disease 2019 (COVID-19) among symptomatic patients is to analyze chest Computed Tomography (CT) scans or chest x-rays images of individuals using Deep Learning (DL) techniques. This paper proposes a novel stacked ensemble to detect COVID-19 either from chest CT scans or chest x-ray images of an individual. The proposed model is a stacked ensemble of heterogenous pre-trained computer vision models. Four pre-trained DL models were considered: Visual Geometry Group (VGG 19), Residual Network (ResNet 101), Densely Connected Convolutional Networks (DenseNet 169) and Wide Residual Network (WideResNet 50 2). From each pre-trained model, the potential candidates for base classifiers were obtained by varying the number of additional fully-connected layers. After an exhaustive search, three best-performing diverse models were selected to design a weighted average-based heterogeneous stacked ensemble. Five different chest CT scans and chest x-ray images were used to train and evaluate the proposed model. The performance of the proposed model was compared with two other ensemble models, baseline pre-trained computer vision models and existing models for COVID-19 detection. The proposed model achieved uniformly good performance on five different datasets, consisting of chest CT scans and chest x-rays images. In relevance to COVID-19, as the recall is more important than precision, the trade-offs between recall and precision at different thresholds were explored. Recommended threshold values which yielded a high recall and accuracy were obtained for each dataset.",
"Background: The quantitative analysis of microscope videos often requires instance segmentation and tracking of cellular and subcellular objects. The traditional method consists of two stages: (1) performing instance object segmentation of each frame, and (2) associating objects frame-by-frame. Recently, pixel-embedding-based deep learning approaches these two steps simultaneously as a single stage holistic solution. Pixel-embedding-based learning forces similar feature representation of pixels from the same object, while maximizing the difference of feature representations from different objects. However, such deep learning methods require consistent annotations not only spatially (for segmentation), but also temporally (for tracking). In computer vision, annotated training data with consistent segmentation and tracking is resource intensive, the severity of which is multiplied in microscopy imaging due to (1) dense objects (e.g., overlapping or touching), and (2) high dynamics (e.g., irregular motion and mitosis). Adversarial simulations have provided successful solutions to alleviate the lack of such annotations in dynamics scenes in computer vision, such as using simulated environments (e.g., computer games) to train real-world self-driving systems. Methods: In this paper, we propose an annotation-free synthetic instance segmentation and tracking (ASIST) method with adversarial simulation and single-stage pixel-embedding based learning. Contribution: The contribution of this paper is three-fold: (1) the proposed method aggregates adversarial simulations and single-stage pixel-embedding based deep learning (2) the method is assessed with both the cellular (i.e., HeLa cells); and subcellular (i.e., microvilli) objects; and (3) to the best of our knowledge, this is the first study to explore annotation-free instance segmentation and tracking study for microscope videos. Results: The ASIST method achieved an important step forward, when compared with fully supervised approaches: ASIST shows 7%-11% higher segmentation, detection and tracking performance on microvilli relative to fully supervised methods, and comparable performance on Hela cell videos.",Annotation free; Segmentation; Tracking; Cellular; Subcelluar
"An increasing number of tasks have been developed for autonomous driving and advanced driver assistance systems. However, this gives rise to the problem of incorporating plural functionalities to be ported into a power-constrained computing device. Therefore, the objective of this work is to alleviate the complex learning procedure of the pixel-wise approach for driving scene understanding. In this paper, we go beyond the pixel-wise detection of the semantic segmentation task as a point detection task and implement it to detect free space and lane. Instead of pixel-wise learning, we trained a single deep convolution neural network for point of interest detection in a grid-based level and followed with a computer vision (CV) based post-processing of end branches corresponding to the characteristic of target classes. To achieve the corresponding final result of pixel-wise detection of semantic segmentation and parametric description of lanes, we propose a CV-based post-processing to decode points of output from the neural network. The final results showed that the network could learn the spatial relationship for point of interest, including the representative points on the contour of the free space segmented region and the representative points along the center of the road lane. We verify our method on two publicly available datasets, which achieved 98.2% mIoU on the KITTI dataset for the evaluation of free space and 97.8% accuracy on the TuSimple dataset (with the field of view below the y = 320 axis) for the evaluation of the lane.",Autonomous driving; free space detection; lane detection; multi-task learning; semantic segmentation
"Facial expression recognition (FER) is a significant research task in the computer vision field. In this paper, we present a novel network FaceCaps for facial expression recognition with the following novel characteristics: an embedding structure based on a Capsule network which encodes relative spatial relationships between features; incorporates the feature polymerization property of FaceNet, thus offering a more efficient approach to discriminate complex facial expressions; a target reconstruction loss as a better regularization term for Capsule networks. Experimental results on both lab-controlled datasets (CK+) and real-world databases (RAF-DB and SFEW 2.0) demonstrate that the method significantly outperforms the state-of-the-art.",capsule network; facial expression recognition; feature embedding
"Tunnel structural health inspections are predominantly done through periodic visual observations, requiring humans to be physically present on-site, possibly exposing them to hazardous environments. These surveys are subjective (relying on the surveyor experience), time-consuming, and may demand operation shutdown. These issues can be mitigated through accurate automatic monitoring and inspection systems. In this work, we propose a remotely operated machine vision change detection application to improve the structural health monitoring of tunnels. The vision-based sensing system acquires the data from a rig of cameras hosted on a robotic platform that is driven parallel to the tunnel walls. These data are then pre-processed using image processing and deep learning techniques to reduce nuisance changes caused by light variations. Image fusion techniques are then applied to identify the changes occurring in the tunnel structure. Different pixel-based change detection approaches are used to generate temporal change maps. Decision-level fusion methods are then used to combine these change maps to obtain a more reliable detection of the changes that occur between surveys. A quantitative analysis of the results achieved shows that the proposed change detection system achieved a recall value of 81%, a precision value of 93% and an F1-score of 86.7%.",computer vision; data fusion; tunnel lining inspections
"In this article, we present a very lightweight neural network architecture, trained on stereo data pairs, which performs view synthesis from one single image. With the growing success of multi-view formats, this problem is indeed increasingly relevant. The network returns a prediction built from disparity estimation, which fills in wrongly predicted regions using a occlusion handling technique. To do so, during training, the network learns to estimate the left-right consistency structural constraint on the pair of stereo input images, to be able to replicate it at test time from one single image. The method is built upon the idea of blending two predictions: a prediction based on disparity estimation and a prediction based on direct minimization in occluded regions. The network is also able to identify these occluded areas at training and at test time by checking the pixelwise left-right consistency of the produced disparity maps. At test time, the approach can thus generate a left-side and a right-side view from one input image, as well as a depth map and a pixelwise confidence measure in the prediction. The work outperforms visually and metric-wise state-of-the-art approaches on the challenging KITTI dataset, all while reducing by a very significant order of magnitude (5 or 10 times) the required number of parameters (6.5 M).",Training; Estimation; Three-dimensional displays; Neural networks; Image resolution; Computer vision; Tools; Computer vision; monocular; deep learning; stereo; view synthesis
"Real-time visual object tracking provides every object of interest with a unique identity and a trajectory across video frames. This is a fundamental task of many video analytics applications, such as traffic monitoring or video surveillance in general. The development of real-time multiple object tracking systems on low-power edge devices as IoT nodes, without compromising accuracy, is a challenge due to the limited computing capacity of said devices. This might rule out the best in-class computer vision solutions, which, nowadays, are based on deep learning, and thus, they are very hardware demanding. This article meets this challenge with a multiple object detection and tracking system that employs cutting-edge deep learning architectures on an embedded GPU while operating in real time. For this purpose, a system has been designed that extends a joint architecture of tracking and detection by adding a module comprised of appearance-based and movement-based trackers that allow to maintain the identity of the objects of interest for longer periods of time while alleviating the burden of the detector. Our system is mapped onto an embedded GPU platform, cutting down power consumption significantly with respect to a server GPU. Tracking performance metrics show a 51.1% in multiple object tracking accuracy (MOTA) on the MOT16 data set. This, in conjunction with a real-time processing speed of 25.2 FPS for up to 45 simultaneous objects and low-power consumption of 15 W, make our system an ideal solution for a wide range of video analytics applications.",Real-time systems; Feature extraction; Detectors; Deep learning; Hardware; Computer vision; Computer architecture; Deep learning; edge computing; multiple object tracking
"Advances in deep neural networks (DNN) and computer vision (CV) algorithms have made it feasible to extract meaningful insights from large-scale deployments of urban cameras. Tracking an object of interest across the camera network in near real-time is a canonical problem. However, current tracking platforms have two key limitations: 1) They are monolithic, proprietary and lack the ability to rapidly incorporate sophisticated tracking models, and 2) They are less responsive to dynamism across wide-area computing resources that include edge, fog, and cloud abstractions. We address these gaps using Anveshak, a runtime platform for composing and coordinating distributed tracking applications. It provides a domain-specific dataflow programming model to intuitively compose a tracking application, supporting contemporary CV advances like query fusion and re-identification, and enabling dynamic scoping of the camera network's search space to avoid wasted computation. We also offer tunable batching and data-dropping strategies for dataflow blocks deployed on distributed resources to respond to network and compute variability. These balance the tracking accuracy, its real-time performance, and the active camera-set size. We illustrate the concise expressiveness of the programming model for four tracking applications. Our detailed experiments for a network of 1000 camera-feeds on modest resources exhibit the tunable scalability, performance, and quality trade-offs enabled by our dynamic tracking, batching, and dropping strategies.",Cameras; Streaming media; Urban areas; Tracking; Cloud computing; Target tracking; Scalability; Big data platform; edge and fog computing; video analytics; distributed stream processing; Internet of Things
"One of the well-known challenges in computer vision tasks is the visual diversity of images, which could result in an agreement or disagreement between the learned knowledge and the visual content exhibited by the current observation. In this work, we first define such an agreement in a concepts learning process as congruency. Formally, given a particular task and sufficiently large dataset, the congruency issue occurs in the learning process whereby the task-specific semantics in the training data are highly varying. We propose a Direction Concentration Learning (DCL) method to improve congruency in the learning process, where enhancing congruency influences the convergence path to be less circuitous. The experimental results show that the proposed DCL method generalizes to state-of-the-art models and optimizers, as well as improves the performances of saliency prediction task, continual learning task, and classification task. Moreover, it helps mitigate the catastrophic forgetting problem in the continual learning task. The code is publicly available at https://github.com/luoyan407/congruency.",Task analysis; Visualization; Computational modeling; Training; Convergence; Predictive models; Machine learning; Optimization; machine learning; computer vision; accumulated gradient; congruency
"Deep learning has greatly increased the capabilities of intelligent technical systems over the last years [1]. This includes the industrial automation sector [1]-[4], where new data-driven approaches to, for example, predictive maintenance [2], computer vision [3], or anomaly detection [4], have resulted in systems more easily and robustly automated than ever before.",Deep learning; Training data; Machine learning algorithms; Automation; Feature extraction; Data models
"Crop type mapping currently represents an important problem in remote sensing. Accurate information on the extent and types of crops derived from remote sensing can help managing and improving agriculture especially for developing countries where such information is scarce. In this paper, high-resolution RGB drone images are the input data for the classification performed using a transfer learning (TL) approach. VGG16 and GoogLeNet, which are pre-trained convolutional neural networks (CNNs) used for classification tasks coming from computer vision, are considered for the mapping of the crop types. Thanks to the transferred knowledge, the proposed models can successfully classify the studied crop types with high overall accuracy for two considered cases, achieving up to almost 83% for the Malawi dataset and up to 90% for the Mozambique dataset. Notably, these results are comparable to the ones achieved by the same deep CNN architectures in many computer vision tasks. With regard to drone data analysis, application of deep CNN is very limited so far due to high requirements on the number of samples needed to train such complicated architectures. Our results demonstrate that the transfer learning is an efficient way to overcome this problem and take full advantage of the benefits of deep CNN architectures for drone-based crop type mapping. Moreover, based on experiments with different TL approaches we show that the number of frozen layers is an important parameter of TL and a fine-tuning of all the CNN weights results in significantly better performance than the approaches that apply fine-tuning only on some numbers of last layers.",Crop detection; Transfer learning; Convolutional neural networks; Drone images
"Computer Vision is a cross-research field with the main purpose of understanding the surrounding environment as closely as possible to human perception. The image processing systems is continuously growing and expanding into more complex systems, usually tailored to the certain needs or applications it may serve. To better serve this purpose, research on the architecture and design of such systems is also important. We present the End-to-End Computer Vision Framework, an open-source solution that aims to support researchers and teachers within the image processing vast field. The framework has incorporated Computer Vision features and Machine Learning models that researchers can use. In the continuous need to add new Computer Vision algorithms for a day-to-day research activity, our proposed framework has an advantage given by the configurable and scalar architecture. Even if the main focus of the framework is on the Computer Vision processing pipeline, the framework offers solutions to incorporate even more complex activities, such as training Machine Learning models. EECVF aims to become a useful tool for learning activities in the Computer Vision field, as it allows the learner and the teacher to handle only the topics at hand, and not the interconnection necessary for visual processing flow.",Computer Vision Framework; Computer Vision; pipeline architecture; benchmarking; deep learning; neural networks; reproducible research; machine learning
,
"Technology has been promoting a great transformation in farming. The introduction of robotics; the use of sensors in the field; and the advances in computer vision; allow new systems to be developed to assist processes, such as phenotyping, of crop's life cycle monitoring. This work presents, which we believe to be the first time, a system capable of generating 3D models of non-rigid corn plants, which can be used as a tool in the phenotyping process. The system is composed by two modules: an terrestrial acquisition module and a processing module. The terrestrial acquisition module is composed by a robot, equipped with an RGB-D camera and three sets of temperature, humidity, and luminosity sensors, that collects data in the field. The processing module conducts the non-rigid 3D plants reconstruction and merges the sensor data into these models. The work presented here also shows a novel technique for background removal in depth images, as well as efficient techniques for processing these images and the sensor data. Experiments have shown that from the models generated and the data collected, plant structural measurements can be performed accurately and the plant's environment can be mapped, allowing the plant's health to be evaluated and providing greater crop efficiency.",3D reconstruction; computer vision; sensor data fusion; robotics; agriculture
"Classification of human actions is an ongoing research problem in computer vision. This review is aimed to scope current literature on data fusion and action recognition techniques and to identify gaps and future research direction. Success in producing cost-effective and portable vision-based sensors has dramatically increased the number and size of datasets. The increase in the number of action recognition datasets intersects with advances in deep learning architectures and computational support, both of which offer significant research opportunities. Naturally, each action-data modality-such as RGB, depth, skeleton, and infrared (IR)-has distinct characteristics; therefore, it is important to exploit the value of each modality for better action recognition. In this paper, we focus solely on data fusion and recognition techniques in the context of vision with an RGB-D perspective. We conclude by discussing research challenges, emerging trends, and possible future research directions.",action recognition; deep learning; data fusion; RGB-D
"We consider the problem of vision-based detection and ranging of a target UAV using the video feed from a monocular camera onboard a pursuer UAV. Our previously published work in this area employed a cascade classifier algorithm to locate the target UAV, which was found to perform poorly in complex background scenes. We thus study the replacement of the cascade classifier algorithm with newer machine learning-based object detection algorithms. Five candidate algorithms are implemented and quantitatively tested in terms of their efficiency (measured as frames per second processing rate), accuracy (measured as the root mean squared error between ground truth and detected location), and consistency (measured as mean average precision) in a variety of flight patterns, backgrounds, and test conditions. Assigning relative weights of 20%, 40% and 40% to these three criteria, we find that when flying over a white background, the top three performers are YOLO v2 (76.73 out of 100), Faster RCNN v2 (63.65 out of 100), and Tiny YOLO (59.50 out of 100), while over a realistic background, the top three performers are Faster RCNN v2 (54.35 out of 100, SSD MobileNet v1 (51.68 out of 100) and SSD Inception v2 (50.72 out of 100), leading us to recommend Faster RCNN v2 as the recommended solution. We then provide a roadmap for further work in integrating the object detector into our vision-based UAV tracking system.",UAVs; computer vision; detection; machine learning; neural networks; CNN; TensorFlow; darknet
"Seeking consistent point-to-point correspondences between 3D rigid data (point clouds, meshes, or depth maps) is a fundamental problem in 3D computer vision. While a number of correspondence selection methods have been proposed in recent years, their advantages and shortcomings remain unclear regarding different applications and perturbations. To fill this gap, this paper gives a comprehensive evaluation of nine state-of-the-art 3D correspondence grouping methods. A good correspondence grouping algorithm is expected to retrieve as many as inliers from initial feature matches, giving a rise in both precision and recall as well as facilitating accurate transformation estimation. Toward this rule, we deploy experiments on three benchmarks with different application contexts, including shape retrieval, 3D object recognition, and point cloud registration. We also investigate various perturbations such as noise, point density variation, clutter, occlusion, partial overlap, different scales of initial correspondences, and different combinations of keypoint detectors and descriptors. The rich variety of application scenarios and nuisances result in different spatial distributions and inlier ratios of initial feature correspondences, thus enabling a thorough evaluation. Based on the outcomes, we give a summary of the traits, merits, and demerits of evaluated approaches and indicate some potential future research directions.",Three-dimensional displays; Shape; Measurement; Detectors; Feature extraction; Object recognition; Clutter; Performance evaluation; correspondence grouping; 3D computer vision; 3D rigid data; shape matching
"Pavement crack detection on pixel-levels is a high-profile application of computer vision and semantic segmentation. In this paper, a two-step convolutional neural network (CNN) method is proposed to detect crack-pixels from pavement pictures and to reduce time consumption. The method contains two main parts: CNN-1 for patch classification and CNN-2 for semantic segmentation. The first part chooses regions with a high probability to contain cracks and sends them to CNN-2 to get pixel-wise detection results. The CNN-2 cancels down-sampling to ensure the size of a feature map is fixed, so it is an end-to-end network. The proposed method and CrackNet-II are trained and tested on the same datasets, and the results show that compared with the pure-segmentation network, the two-step CNN method reduces the processing-time dramatically while the loss of accuracy is small.",Pavement crack detection; Computer vision; Convolutional neural network (CNN); Semantic segmentation
"3D shape editing is widely used in a range of applications such as movie production, computer games and computer aided design. It is also a popular research topic in computer graphics and computer vision. In past decades, researchers have developed a series of editing methods to make the editing process faster, more robust, and more reliable. Traditionally, the deformed shape is determined by the optimal transformation and weights for an energy formulation. With increasing availability of 3D shapes on the Internet, data-driven methods were proposed to improve the editing results. More recently as the deep neural networks became popular, many deep learning based editing methods have been developed in this field, which are naturally data-driven. We mainly survey recent research studies from the geometric viewpoint to those emerging neural deformation techniques and categorize them into organic shape editing methods and man-made model editing methods. Both traditional methods and recent neural network based methods are reviewed.",mesh deformation; man-made model editing; deformation representation; optimization; deep learning
"Natural image matting is an important problem that widely applied in computer vision and graphics. Recent deep learning matting approaches have made an impressive process in both accuracy and efficiency. However, there are still two fundamental problems remain largely unsolved: 1) accurately separating an object from the image with similar foreground and background color or lots of details; 2) exactly extracting an object with fine structures from complex background. In this paper, we propose an attention transfer network (ATNet) to overcome these challenges. Specifically, we firstly design a feature attention block to effectively distinguish the foreground object from the color-similar regions by activating foreground-related features as well as suppressing others. Then, we introduce a scale transfer block to magnify the feature maps without adding extra information. By integrating the above blocks into an attention transfer module, we effectively reduce the artificial content in results and decrease the computational complexity. Besides, we use a perceptual loss to measure the difference between the feature representations of the predictions and the ground-truths. It can further capture the high-frequency details of the image, and consequently, optimize the fine structures of the object. Extensive experiments on two publicly common datasets (i.e., Composition-1k matting dataset, and www.alphamatting.com dataset) show that the proposed ATNet obtains significant improvements over the previous methods. The source code and compiled models have been made publicly available at https://github.com/ailsaim/ATNet.",Image color analysis; Semantics; Loss measurement; Task analysis; Machine learning; Feature extraction; Computer vision; Natural image matting; feature attention; scale transfer; perceptual loss
"This work presents a gated non-local deep residual learning framework for image deraining. It can avoid the over-deraining or under-deraining caused by the global residual learning in existing deraining networks, since the learned soft gate in our method adaptively adjusts the amount of global residual to be passed for generating the final derained result. To generate feature maps for global residual prediction, we develop a non-local guided attention module (NLAM), which first obtains non-local features by exploiting spatial inter-dependencies among all the feature positions of local features produced by convolutional neural network (CNN), and then leverages the attention mechanism to merge the local and non-local features based on their complementary relation. Moreover, we develop a channel-wise gated prediction module to learn a soft gate on the global residual by explicitly modelling channel inter-dependencies of the feature maps obtained from NLAM. Experiments on four deraining benchmark datasets and real-world rainy images show that our network has a quantitative and qualitative improvement over state-of-the-arts.",Rain; Logic gates; Computer science; Convolutional neural networks; Benchmark testing; Computer vision; Computer architecture; Single-image rain streak removal; non-local residual learning; gated learning
"Human pose estimation is fundamental to many computer vision tasks and has made significant progress in recent years. However, the problem of unbalanced performance among joints has not been paid enough attention. Basing on simple baseline Xiao et al. (Proceedings of the European conference on computer vision, 2018), we propose a weighted summation method of local keypoint, selective receptive field (SRF) unit and use the feature fuse method to tackle this problem. Initially, the weighted summation method of local keypoint is designed to make the network explicitly address keypoints with large loss value. This method calculation weights according to the loss value of each joint. Subsequently, the SRF unit was proposed to adaptively select receptive field size for keypoints. Firstly, multiple branches with different kernel sizes are compared using softmax attention. Secondly, the Select operator chooses one of these branches to yield effective receptive fields. Then, the features coming from the encoder are merged in the decoder using concatenation to solve the occlusion joint. This method enhances communication between spatial information and semantic information. The experimental results show that as a model-agnostic approach, our method promotes SimpleBaseline-50 - 256 x 192 by 4.3 AP on COCO validation set. Extensive experiments demonstrate that the proposed approach is superior to several state-of-the-art methods in terms of accuracy and robustness.",Human pose estimation; Selective receptive field network; Selective receptive field unit; Weighted summation method of local keypoint
"Image inpainting aims to restore the pixel features of damaged parts in incomplete image and plays a key role in many computer vision tasks. Image inpainting technology based on deep learning is a major current research hotspot. To deeply understand related methods and technologies, this article combs and summarizes the latest research status in this field. Firstly, we summarize inpainting methods of different types of neural network structure based on deep learning, then analyze and study important technical improvement mechanisms. In addition, various algorithms are comprehensively reviewed from the aspects of model network structure and restoration methods. And we select some representative image inpainting methods for comparison and analysis. Finally, the current problems of image inpainting are summarized, and the future development trend and research direction are prospected.",Computer vision; Image inpainting; Variational autoencoder (VAE); Generative adversarial networks (GAN)
"Identifying individual animals is crucial for many biological investigations. In response to some of the limitations of current identification methods, new automated computer vision approaches have emerged with strong performance. Here, we review current advances of computer vision identification techniques to provide both computer scientists and biologists with an overview of the available tools and discuss their applications. We conclude by offering recommendations for starting an animal identification project, illustrate current limitations, and propose how they might be addressed in the future.",
"Single-object tracking is regarded as a challenging task in computer vision, especially in complex spatiotemporal contexts. The changes in the environment and object deformation make it difficult to track. In the last 10 years, the application of correlation filters and deep learning enhances the performance of trackers to a large extent. This paper summarizes single-object tracking algorithms based on correlation filters and deep learning. Firstly, we explain the definition of single-object tracking and analyze the components of general object tracking algorithms. Secondly, the single-object tracking algorithms proposed in the past decade are summarized according to different categories. Finally, this paper summarizes the achievements and problems of existing algorithms by analyzing experimental results and discusses the development trends. (c) 2021 Elsevier B.V. All rights reserved.",Single-object tracking; Correlation filters; Deep learning; Computer vision
"With the rapidly increase of population every day, it has become a major issue to fulfill everyone's need for food products (i.e., vegetables, fruits, milk, wheat, etc.) due to limited production of food products. Moreover, healthy food utilization among people is the foremost requirement. The major factors that affect the food system includes increasing food shortage, decreasing quality, wastage, and loss of food products, limited natural resources, etc. This article addresses the various computer vision and machine learning based techniques, used to minimize the aforementioned issues. Image processing has become an effective technique for the analysis of many research applications. This study intends to focus on analysis of image processing based applications in food products and agriculture field. Such applications help in decision making , disease prediction, classification, fruit sorting, soil quality measurement, etc. Moreover, a comprehensive review has been accomplished for various computer vision and statistical approaches used in food production and agricultural field and concludes that Deep Learning (DL) based approaches produce better results, specifically for image processing applications. Additionally, an effort has been made to provide a list of publicly available datasets for the related study.",Food Ssecurity; Deep learning; Convolutional neural network; Smart farming
"In recent years deep neural networks have become the workhorse of computer vision. In this paper, we employ a deep learning approach to classify footwear impression's features known as descriptors for forensic use cases. Within this process, we develop and evaluate an effective technique for feeding downsampled greyscale impressions to a neural network pre-trained on data from a different domain. Our approach relies on learnable preprocessing layer paired with multiple interpolation methods used in parallel. We empirically show that this technique outperforms using a single type of interpolated image without learnable preprocessing, and can help to avoid the computational penalty related to using high resolution inputs, by making more efficient use of the low resolution inputs. We also investigate the effect of preserving the aspect ratio of the inputs, which leads to considerable boost in accuracy without increasing the computational budget with respect to squished rectangular images. Finally, we formulate a set of best practices for transfer learning with greyscale inputs, potentially widely applicable in computer vision tasks ranging from footwear impression classification to medical imaging. (C) 2021 The Author(s). Published by Elsevier B.V.",Deep learning; Forensics; Footwear impressions
"Recently, the development of deep learning has facilitated continuous progress in the field of computer vision. Pixel-level semantic segmentation serves as a fundamental task in computer vision. It achieves significant results by connecting wider and deeper backbone networks and building fine-grained segmentation heads. However, applications such as self-driving cars are more critical to the computational speed of the algorithms. The trade-off between accuracy and real-time performance of existing algorithms is still a challenging task. To address this challenge, this article proposes an adaptive multiscale segmentation fusion network to fuse multiscale contextual, which designs an adaptive multiscale segmentation fusion module based on an attention mechanism. Using segmentation fusion instead of feature fusion, the multiscale segmentation results are aggregated to obtain more precise segmentation results. The final results achieved 70.9% mIoU of accuracy in the Cityspace test set, processing images at 61 FPS when the input is 1024 x 2048. In addition, when adjusting the input size to 512 x 1024, the images are processed at 185 FPS.",computer vision; multiscale fusion; real-time semantic segmentation; segmentation fusion
"Convolutional neural network is widely used to perform the task of image classification, including pretraining, followed by fine-tuning whereby features are adapted to perform the target task, on ImageNet. ImageNet is a large database consisting of 15 million images belonging to 22,000 categories. Images collected from the Web are labeled using Amazon Mechanical Turk crowd-sourcing tool by human labelers. ImageNet is useful for transfer learning because of the sheer volume of its dataset and the number of object classes available. Transfer learning using pretrained models is useful because it helps to build computer vision models in an accurate and inexpensive manner. Models that have been pretrained on substantial datasets are used and repurposed for our requirements. Scene recognition is a widely used application of computer vision in many communities and industries, such as tourism. This study aims to show multilabel scene classification using five architectures, namely, VGG16, VGG19, ResNet50, InceptionV3, and Xception using ImageNet weights available in the Keras library. The performance of different architectures is comprehensively compared in the study. Finally, EnsemV3X is presented in this study. The proposed model with reduced number of parameters is superior to state-of-of-the-art models Inception and Xception because it demonstrates an accuracy of 91%.",EnsemV3X; Multi-label scene classification; Convolutional Neural Network (CNN); Image classification; Deep learning
"Vehicle detection from unmanned aerial vehicle (UAV) imagery is one of the most important tasks in a large number of computer vision-based applications. This crucial task needed to be done with high accuracy and speed. However, it is a very challenging task due to many characteristics related to the aerial images and the used hardware, such as different vehicle sizes, orientations, types, density, limited datasets, and inference speed. In recent years, many classical and deep-learning-based methods have been proposed in the literature to address these problems. Handed engineering- and shallow learning-based techniques suffer from poor accuracy and generalization to other complex cases. Deep-learning-based vehicle detection algorithms achieved better results due to their powerful learning ability. In this article, we provide a review on vehicle detection from UAV imagery using deep learning techniques. We start by presenting the different types of deep learning architectures, such as convolutional neural networks, recurrent neural networks, autoencoders, generative adversarial networks, and their contribution to improve the vehicle detection task. Then, we focus on investigating the different vehicle detection methods, datasets, and the encountered challenges all along with the suggested solutions. Finally, we summarize and compare the techniques used to improve vehicle detection from UAV-based images, which could be a useful aid to researchers and developers to select the most adequate method for their needs.",Computer architecture; Vehicle detection; Deep learning; Task analysis; Computer vision; Object detection; Neural networks; Autoencoders; computer vision; convolutional neural networks (CNNs); deep learning; generative adversarial networks (GANs); recurrent neural networks (RNNs); unmanned aerial vehicle (UAV); vehicle detection